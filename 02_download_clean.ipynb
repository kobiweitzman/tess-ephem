{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0272ff-9a39-4039-a18a-cd40a6da5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: TOI 1801.01 (TIC 119584412)\n",
      " PDCSAP S22: ok (N=16102)\n",
      " PDCSAP S49: ok (N=13272)\n",
      "  FFI S22: skip (already have PDCSAP)\n",
      "  FFI S49: skip (already have PDCSAP)\n",
      "\n",
      "== DONE ==\n",
      "Saved figure: figures/TIC119584412_download_clean.png\n",
      "Saved JSON  : results/TIC119584412_download_clean_summary.json\n",
      "Summary: {'target_toi': 'TOI 1801.01', 'target_tic': 119584412, 'n_sectors_pdcsap': 2, 'n_sectors_ffi': 0, 'sectors_pdcsap': [22, 49], 'sectors_ffi': [], 'n_points_raw': 29374, 'n_points_flat': 29374, 'rms_raw_ppm': 1359.3999901786447, 'rms_flat_ppm': 1099.746714683546, 'cdpp1h_flat_ppm': 251.16073602817906, 'flatten_window_days': 1.0, 'notes': 'PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.'}\n"
     ]
    }
   ],
   "source": [
    "# 02_download_clean — Target A (PDCSAP-first, TESSCut-FFI fallback) — robust & batchable\n",
    "import json, warnings, pathlib, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config (single-target defaults) ---\n",
    "TARGET_TOI = \"TOI 1801.01\"\n",
    "TARGET_TIC = 119584412\n",
    "QUALITY    = 175\n",
    "CUTOUT_SZ  = 15\n",
    "WIN_DAYS   = 1.0\n",
    "\n",
    "# --- Folders ---\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"data_raw_fresh\").mkdir(exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def window_len_for(t_days, window_days=1.0):\n",
    "    dt = np.nanmedian(np.diff(t_days))\n",
    "    wl = 401 if (not np.isfinite(dt) or dt<=0) else max(5, int(round(window_days/dt)))\n",
    "    return wl if wl % 2 == 1 else wl + 1\n",
    "\n",
    "def savgol_trend(t, y, window_len, polyorder=2):\n",
    "    half = window_len//2\n",
    "    trend = np.full_like(y, np.nan, dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        lo, hi = max(0, i-half), min(len(y), i+half+1)\n",
    "        tt = t[lo:hi] - t[i]; yy = y[lo:hi]\n",
    "        m  = np.isfinite(tt) & np.isfinite(yy)\n",
    "        if np.count_nonzero(m) >= polyorder+1:\n",
    "            c = np.polyfit(tt[m], yy[m], polyorder)\n",
    "            trend[i] = np.polyval(c, 0.0)\n",
    "    trend[~np.isfinite(trend)] = np.nanmedian(y)\n",
    "    return trend\n",
    "\n",
    "def custom_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Return a *new* LightCurve with flattened flux (no .replace() calls).\"\"\"\n",
    "    t = lc.time.value\n",
    "    f = lc.flux.value.astype(float)\n",
    "    wl = window_len_for(t, window_days)\n",
    "    tr = savgol_trend(t, f, wl, polyorder=polyorder)\n",
    "    flat = f/np.where(np.isfinite(tr), tr, np.nanmedian(f))\n",
    "    flat /= np.nanmedian(flat)\n",
    "    # carry flux_err if present\n",
    "    try:\n",
    "        ferr = lc.flux_err.value if getattr(lc, \"flux_err\", None) is not None else None\n",
    "    except Exception:\n",
    "        ferr = None\n",
    "    from lightkurve import LightCurve\n",
    "    return LightCurve(time=lc.time, flux=flat, flux_err=ferr, meta=dict(getattr(lc, \"meta\", {})))\n",
    "\n",
    "def approx_cdpp_ppm(lc, hours=1.0):\n",
    "    t = lc.time.value; f = lc.flux.value\n",
    "    dt_hr = np.nanmedian(np.diff(t))*24.0\n",
    "    if not np.isfinite(dt_hr) or dt_hr<=0: return float(\"nan\")\n",
    "    n = max(1, int(round(hours/dt_hr)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run)*1e6)\n",
    "\n",
    "def get_header_sector(header):\n",
    "    # Try common sector keys; then regex fallback\n",
    "    for k in (\"SECTOR\",\"sector\",\"SEC1\"):\n",
    "        try:\n",
    "            v = header.get(k)\n",
    "            if v is not None:\n",
    "                return int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    txt = \" \".join([f\"{k}={header.get(k)}\" for k in header.keys()])\n",
    "    m = re.search(r\"[Ss]ector[^0-9]*([0-9]{1,3})\", txt)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# --- Lightkurve ---\n",
    "import lightkurve as lk\n",
    "from lightkurve import LightCurveCollection\n",
    "\n",
    "def run_download_clean(target_toi: str, target_tic: int, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS):\n",
    "    TARGET = f\"TIC {target_tic}\"\n",
    "    out_fig  = f\"figures/TIC{target_tic}_download_clean.png\"\n",
    "    out_json = f\"results/TIC{target_tic}_download_clean_summary.json\"\n",
    "\n",
    "    print(f\"Target: {target_toi} ({TARGET})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Download SPOC PDCSAP first; read sector from files (robust to missing 'sector' column)\n",
    "    # -------------------------\n",
    "    pdcsap_list, got_spoc_sectors = [], set()\n",
    "    sr_spoc = lk.search_lightcurvefile(TARGET, mission=\"TESS\", author=\"SPOC\")\n",
    "\n",
    "    if len(sr_spoc) == 0:\n",
    "        print(\"No SPOC PDCSAP products found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_spoc)):\n",
    "            try:\n",
    "                lcf = sr_spoc[i].download(download_dir=\"data_raw_fresh\")\n",
    "                lc  = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "                # sector from object or header\n",
    "                s = getattr(lcf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = lcf.header() if hasattr(lcf, \"header\") else lcf.get_header()\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                lc.meta[\"source\"] = \"PDCSAP\"; lc.meta[\"sector\"] = int(s)\n",
    "                pdcsap_list.append(lc)\n",
    "                if s != -1: got_spoc_sectors.add(int(s))\n",
    "                print(f\" PDCSAP S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\" PDCSAP entry {i}: failed -> {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Query TESSCut and add FFI for sectors not covered by PDCSAP\n",
    "    # -------------------------\n",
    "    ffi_list = []\n",
    "    try:\n",
    "        sr_cut = lk.search_tesscut(TARGET)\n",
    "    except Exception as e:\n",
    "        sr_cut = []\n",
    "        print(\"TESSCut query failed:\", e)\n",
    "\n",
    "    if len(sr_cut) == 0:\n",
    "        print(\"No TESSCut entries found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_cut)):\n",
    "            try:\n",
    "                tpf = sr_cut[i].download(cutout_size=cutout_sz, download_dir=\"data_raw_fresh\")\n",
    "                # sector from object or header\n",
    "                s = getattr(tpf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = tpf.hdu[0].header\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                # skip if PDCSAP already covers\n",
    "                if (s != -1) and (int(s) in got_spoc_sectors):\n",
    "                    print(f\"  FFI S{int(s)}: skip (already have PDCSAP)\")\n",
    "                    continue\n",
    "                # aperture photometry\n",
    "                mask = tpf.create_threshold_mask(threshold=3, reference_pixel=None)\n",
    "                if not np.any(mask):\n",
    "                    m = np.zeros_like(mask, dtype=bool)\n",
    "                    yy, xx = np.unravel_index(np.nanargmax(np.nanmedian(tpf.flux.value, axis=0)), mask.shape)\n",
    "                    m[yy, xx] = True\n",
    "                    mask = m\n",
    "                lc = tpf.to_lightcurve(aperture_mask=mask).remove_nans().normalize()\n",
    "                lc.meta[\"source\"] = \"FFI\"; lc.meta[\"sector\"] = int(s)\n",
    "                ffi_list.append(lc)\n",
    "                print(f\"  FFI S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  FFI entry {i}: failed -> {e}\")\n",
    "\n",
    "    if not (pdcsap_list or ffi_list):\n",
    "        raise RuntimeError(\"No light curves obtained from SPOC or TESSCut.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Stitch, flatten, QC\n",
    "    # -------------------------\n",
    "    all_lcs_raw   = pdcsap_list + ffi_list\n",
    "    stitched_raw  = LightCurveCollection(all_lcs_raw).stitch().remove_nans()\n",
    "    stitched_flat = custom_flatten(stitched_raw, window_days=win_days)\n",
    "\n",
    "    def rms_ppm(x): return float(np.nanstd(x)*1e6)\n",
    "    qc = {\n",
    "        \"target_toi\": target_toi,\n",
    "        \"target_tic\": target_tic,\n",
    "        \"n_sectors_pdcsap\": len(pdcsap_list),\n",
    "        \"n_sectors_ffi\": len(ffi_list),\n",
    "        \"sectors_pdcsap\": [int(lc.meta.get(\"sector\", -1)) for lc in pdcsap_list],\n",
    "        \"sectors_ffi\": [int(lc.meta.get(\"sector\", -1)) for lc in ffi_list],\n",
    "        \"n_points_raw\": int(len(stitched_raw)),\n",
    "        \"n_points_flat\": int(len(stitched_flat)),\n",
    "        \"rms_raw_ppm\": rms_ppm(stitched_raw.flux.value),\n",
    "        \"rms_flat_ppm\": rms_ppm(stitched_flat.flux.value),\n",
    "        \"cdpp1h_flat_ppm\": approx_cdpp_ppm(stitched_flat, hours=1.0),\n",
    "        \"flatten_window_days\": float(win_days),\n",
    "        \"notes\": \"PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.\"\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Figure & JSON\n",
    "    # -------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,6), dpi=140, sharex=True)\n",
    "    stitched_raw.plot(ax=ax1, marker=\".\", lw=0, alpha=0.35)\n",
    "    ax1.set_ylabel(\"Flux (norm)\")\n",
    "    ax1.set_title(f\"{target_toi} (TIC {target_tic}) — Stitched RAW\")\n",
    "    stitched_flat.plot(ax=ax2, marker=\".\", lw=0, alpha=0.45, label=\"flattened\")\n",
    "    ax2.set_xlabel(\"Time [BTJD]\")\n",
    "    ax2.set_ylabel(\"Flux (norm)\")\n",
    "    ax2.set_title(f\"{target_toi} — Stitched FLAT (window={win_days:.2f} d)\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_fig); plt.close(fig)\n",
    "\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(qc, f, indent=2)\n",
    "\n",
    "    print(\"\\n== DONE ==\")\n",
    "    print(f\"Saved figure: {out_fig}\")\n",
    "    print(f\"Saved JSON  : {out_json}\")\n",
    "    print(\"Summary:\", qc)\n",
    "\n",
    "    return qc, out_fig, out_json\n",
    "\n",
    "# ---------- Run on Target A (deliverable) ----------\n",
    "_ = run_download_clean(TARGET_TOI, TARGET_TIC, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "\n",
    "# ---------- OPTIONAL: batch-run first 4 priority targets ----------\n",
    "# Set RUN_BATCH=True to run on the first 4 rows in results/priority_targets.csv\n",
    "RUN_BATCH = False\n",
    "if RUN_BATCH:\n",
    "    import pandas as pd\n",
    "    dfp = pd.read_csv(\"results/priority_targets.csv\")\n",
    "    # Expect columns: TIC_ID_norm, toi (string/number). Adjust if your headers differ.\n",
    "    for _, row in dfp.head(4).iterrows():\n",
    "        tic = int(row[\"TIC_ID_norm\"])\n",
    "        toi = f\"TOI {row['toi']}\"\n",
    "        try:\n",
    "            run_download_clean(toi, tic, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "        except Exception as e:\n",
    "            print(f\"[Batch] {toi} (TIC {tic}) failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df68e38b-8753-409d-b23c-dc914e1dc224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: TOI 260.01 (TIC 37749396)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PDCSAP S3: ok (N=12978)\n",
      " PDCSAP S42: ok (N=11473)\n",
      " PDCSAP S70: ok (N=86180)\n",
      " PDCSAP S70: ok (N=14424)\n",
      "  FFI S3: skip (already have PDCSAP)\n",
      "  FFI S42: skip (already have PDCSAP)\n",
      "  FFI S70: skip (already have PDCSAP)\n",
      "\n",
      "== DONE ==\n",
      "Saved figure: figures/TIC37749396_download_clean.png\n",
      "Saved JSON  : results/TIC37749396_download_clean_summary.json\n",
      "Summary: {'target_toi': 'TOI 260.01', 'target_tic': 37749396, 'n_sectors_pdcsap': 4, 'n_sectors_ffi': 0, 'sectors_pdcsap': [3, 42, 70, 70], 'sectors_ffi': [], 'n_points_raw': 125055, 'n_points_flat': 125055, 'rms_raw_ppm': 1043.9811740070581, 'rms_flat_ppm': 1035.729219446299, 'cdpp1h_flat_ppm': 112.01310543832174, 'flatten_window_days': 1.0, 'notes': 'PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.'}\n"
     ]
    }
   ],
   "source": [
    "# 02_download_clean — Target A (PDCSAP-first, TESSCut-FFI fallback) — robust & batchable\n",
    "import json, warnings, pathlib, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config (single-target defaults) ---\n",
    "TARGET_TOI= \"TOI 260.01\"\n",
    "TARGET_TIC= 37749396\n",
    "QUALITY    = 175\n",
    "CUTOUT_SZ  = 15\n",
    "WIN_DAYS   = 1.0\n",
    "\n",
    "# --- Folders ---\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"data_raw_fresh\").mkdir(exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def window_len_for(t_days, window_days=1.0):\n",
    "    dt = np.nanmedian(np.diff(t_days))\n",
    "    wl = 401 if (not np.isfinite(dt) or dt<=0) else max(5, int(round(window_days/dt)))\n",
    "    return wl if wl % 2 == 1 else wl + 1\n",
    "\n",
    "def savgol_trend(t, y, window_len, polyorder=2):\n",
    "    half = window_len//2\n",
    "    trend = np.full_like(y, np.nan, dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        lo, hi = max(0, i-half), min(len(y), i+half+1)\n",
    "        tt = t[lo:hi] - t[i]; yy = y[lo:hi]\n",
    "        m  = np.isfinite(tt) & np.isfinite(yy)\n",
    "        if np.count_nonzero(m) >= polyorder+1:\n",
    "            c = np.polyfit(tt[m], yy[m], polyorder)\n",
    "            trend[i] = np.polyval(c, 0.0)\n",
    "    trend[~np.isfinite(trend)] = np.nanmedian(y)\n",
    "    return trend\n",
    "\n",
    "def custom_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Return a *new* LightCurve with flattened flux (no .replace() calls).\"\"\"\n",
    "    t = lc.time.value\n",
    "    f = lc.flux.value.astype(float)\n",
    "    wl = window_len_for(t, window_days)\n",
    "    tr = savgol_trend(t, f, wl, polyorder=polyorder)\n",
    "    flat = f/np.where(np.isfinite(tr), tr, np.nanmedian(f))\n",
    "    flat /= np.nanmedian(flat)\n",
    "    # carry flux_err if present\n",
    "    try:\n",
    "        ferr = lc.flux_err.value if getattr(lc, \"flux_err\", None) is not None else None\n",
    "    except Exception:\n",
    "        ferr = None\n",
    "    from lightkurve import LightCurve\n",
    "    return LightCurve(time=lc.time, flux=flat, flux_err=ferr, meta=dict(getattr(lc, \"meta\", {})))\n",
    "\n",
    "def approx_cdpp_ppm(lc, hours=1.0):\n",
    "    t = lc.time.value; f = lc.flux.value\n",
    "    dt_hr = np.nanmedian(np.diff(t))*24.0\n",
    "    if not np.isfinite(dt_hr) or dt_hr<=0: return float(\"nan\")\n",
    "    n = max(1, int(round(hours/dt_hr)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run)*1e6)\n",
    "\n",
    "def get_header_sector(header):\n",
    "    # Try common sector keys; then regex fallback\n",
    "    for k in (\"SECTOR\",\"sector\",\"SEC1\"):\n",
    "        try:\n",
    "            v = header.get(k)\n",
    "            if v is not None:\n",
    "                return int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    txt = \" \".join([f\"{k}={header.get(k)}\" for k in header.keys()])\n",
    "    m = re.search(r\"[Ss]ector[^0-9]*([0-9]{1,3})\", txt)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# --- Lightkurve ---\n",
    "import lightkurve as lk\n",
    "from lightkurve import LightCurveCollection\n",
    "\n",
    "def run_download_clean(target_toi: str, target_tic: int, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS):\n",
    "    TARGET = f\"TIC {target_tic}\"\n",
    "    out_fig  = f\"figures/TIC{target_tic}_download_clean.png\"\n",
    "    out_json = f\"results/TIC{target_tic}_download_clean_summary.json\"\n",
    "\n",
    "    print(f\"Target: {target_toi} ({TARGET})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Download SPOC PDCSAP first; read sector from files (robust to missing 'sector' column)\n",
    "    # -------------------------\n",
    "    pdcsap_list, got_spoc_sectors = [], set()\n",
    "    sr_spoc = lk.search_lightcurvefile(TARGET, mission=\"TESS\", author=\"SPOC\")\n",
    "\n",
    "    if len(sr_spoc) == 0:\n",
    "        print(\"No SPOC PDCSAP products found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_spoc)):\n",
    "            try:\n",
    "                lcf = sr_spoc[i].download(download_dir=\"data_raw_fresh\")\n",
    "                lc  = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "                # sector from object or header\n",
    "                s = getattr(lcf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = lcf.header() if hasattr(lcf, \"header\") else lcf.get_header()\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                lc.meta[\"source\"] = \"PDCSAP\"; lc.meta[\"sector\"] = int(s)\n",
    "                pdcsap_list.append(lc)\n",
    "                if s != -1: got_spoc_sectors.add(int(s))\n",
    "                print(f\" PDCSAP S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\" PDCSAP entry {i}: failed -> {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Query TESSCut and add FFI for sectors not covered by PDCSAP\n",
    "    # -------------------------\n",
    "    ffi_list = []\n",
    "    try:\n",
    "        sr_cut = lk.search_tesscut(TARGET)\n",
    "    except Exception as e:\n",
    "        sr_cut = []\n",
    "        print(\"TESSCut query failed:\", e)\n",
    "\n",
    "    if len(sr_cut) == 0:\n",
    "        print(\"No TESSCut entries found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_cut)):\n",
    "            try:\n",
    "                tpf = sr_cut[i].download(cutout_size=cutout_sz, download_dir=\"data_raw_fresh\")\n",
    "                # sector from object or header\n",
    "                s = getattr(tpf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = tpf.hdu[0].header\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                # skip if PDCSAP already covers\n",
    "                if (s != -1) and (int(s) in got_spoc_sectors):\n",
    "                    print(f\"  FFI S{int(s)}: skip (already have PDCSAP)\")\n",
    "                    continue\n",
    "                # aperture photometry\n",
    "                mask = tpf.create_threshold_mask(threshold=3, reference_pixel=None)\n",
    "                if not np.any(mask):\n",
    "                    m = np.zeros_like(mask, dtype=bool)\n",
    "                    yy, xx = np.unravel_index(np.nanargmax(np.nanmedian(tpf.flux.value, axis=0)), mask.shape)\n",
    "                    m[yy, xx] = True\n",
    "                    mask = m\n",
    "                lc = tpf.to_lightcurve(aperture_mask=mask).remove_nans().normalize()\n",
    "                lc.meta[\"source\"] = \"FFI\"; lc.meta[\"sector\"] = int(s)\n",
    "                ffi_list.append(lc)\n",
    "                print(f\"  FFI S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  FFI entry {i}: failed -> {e}\")\n",
    "\n",
    "    if not (pdcsap_list or ffi_list):\n",
    "        raise RuntimeError(\"No light curves obtained from SPOC or TESSCut.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Stitch, flatten, QC\n",
    "    # -------------------------\n",
    "    all_lcs_raw   = pdcsap_list + ffi_list\n",
    "    stitched_raw  = LightCurveCollection(all_lcs_raw).stitch().remove_nans()\n",
    "    stitched_flat = custom_flatten(stitched_raw, window_days=win_days)\n",
    "\n",
    "    def rms_ppm(x): return float(np.nanstd(x)*1e6)\n",
    "    qc = {\n",
    "        \"target_toi\": target_toi,\n",
    "        \"target_tic\": target_tic,\n",
    "        \"n_sectors_pdcsap\": len(pdcsap_list),\n",
    "        \"n_sectors_ffi\": len(ffi_list),\n",
    "        \"sectors_pdcsap\": [int(lc.meta.get(\"sector\", -1)) for lc in pdcsap_list],\n",
    "        \"sectors_ffi\": [int(lc.meta.get(\"sector\", -1)) for lc in ffi_list],\n",
    "        \"n_points_raw\": int(len(stitched_raw)),\n",
    "        \"n_points_flat\": int(len(stitched_flat)),\n",
    "        \"rms_raw_ppm\": rms_ppm(stitched_raw.flux.value),\n",
    "        \"rms_flat_ppm\": rms_ppm(stitched_flat.flux.value),\n",
    "        \"cdpp1h_flat_ppm\": approx_cdpp_ppm(stitched_flat, hours=1.0),\n",
    "        \"flatten_window_days\": float(win_days),\n",
    "        \"notes\": \"PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.\"\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Figure & JSON\n",
    "    # -------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,6), dpi=140, sharex=True)\n",
    "    stitched_raw.plot(ax=ax1, marker=\".\", lw=0, alpha=0.35)\n",
    "    ax1.set_ylabel(\"Flux (norm)\")\n",
    "    ax1.set_title(f\"{target_toi} (TIC {target_tic}) — Stitched RAW\")\n",
    "    stitched_flat.plot(ax=ax2, marker=\".\", lw=0, alpha=0.45, label=\"flattened\")\n",
    "    ax2.set_xlabel(\"Time [BTJD]\")\n",
    "    ax2.set_ylabel(\"Flux (norm)\")\n",
    "    ax2.set_title(f\"{target_toi} — Stitched FLAT (window={win_days:.2f} d)\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_fig); plt.close(fig)\n",
    "\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(qc, f, indent=2)\n",
    "\n",
    "    print(\"\\n== DONE ==\")\n",
    "    print(f\"Saved figure: {out_fig}\")\n",
    "    print(f\"Saved JSON  : {out_json}\")\n",
    "    print(\"Summary:\", qc)\n",
    "\n",
    "    return qc, out_fig, out_json\n",
    "\n",
    "# ---------- Run on Target A (deliverable) ----------\n",
    "_ = run_download_clean(TARGET_TOI, TARGET_TIC, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "\n",
    "# ---------- OPTIONAL: batch-run first 4 priority targets ----------\n",
    "# Set RUN_BATCH=True to run on the first 4 rows in results/priority_targets.csv\n",
    "RUN_BATCH = False\n",
    "if RUN_BATCH:\n",
    "    import pandas as pd\n",
    "    dfp = pd.read_csv(\"results/priority_targets.csv\")\n",
    "    # Expect columns: TIC_ID_norm, toi (string/number). Adjust if your headers differ.\n",
    "    for _, row in dfp.head(4).iterrows():\n",
    "        tic = int(row[\"TIC_ID_norm\"])\n",
    "        toi = f\"TOI {row['toi']}\"\n",
    "        try:\n",
    "            run_download_clean(toi, tic, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "        except Exception as e:\n",
    "            print(f\"[Batch] {toi} (TIC {tic}) failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4729ef31-7dbd-4be7-a792-ca3ddd4c6647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: TOI 550.02 (TIC 311183180)\n",
      " PDCSAP S5: ok (N=17286)\n",
      " PDCSAP S31: ok (N=16250)\n",
      "  FFI S5: skip (already have PDCSAP)\n",
      "  FFI S31: skip (already have PDCSAP)\n",
      "\n",
      "== DONE ==\n",
      "Saved figure: figures/TIC311183180_download_clean.png\n",
      "Saved JSON  : results/TIC311183180_download_clean_summary.json\n",
      "Summary: {'target_toi': 'TOI 550.02', 'target_tic': 311183180, 'n_sectors_pdcsap': 2, 'n_sectors_ffi': 0, 'sectors_pdcsap': [5, 31], 'sectors_ffi': [], 'n_points_raw': 33536, 'n_points_flat': 33536, 'rms_raw_ppm': 3489.643335342407, 'rms_flat_ppm': 2277.229196109564, 'cdpp1h_flat_ppm': 2077.4277749925795, 'flatten_window_days': 1.0, 'notes': 'PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.'}\n"
     ]
    }
   ],
   "source": [
    "# 02_download_clean — Target A (PDCSAP-first, TESSCut-FFI fallback) — robust & batchable\n",
    "import json, warnings, pathlib, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config (single-target defaults) ---\n",
    "TARGET_TOI=\"TOI 550.02\"\n",
    "TARGET_TIC=311183180\n",
    "QUALITY    = 175\n",
    "CUTOUT_SZ  = 15\n",
    "WIN_DAYS   = 1.0\n",
    "\n",
    "# --- Folders ---\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"data_raw_fresh\").mkdir(exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def window_len_for(t_days, window_days=1.0):\n",
    "    dt = np.nanmedian(np.diff(t_days))\n",
    "    wl = 401 if (not np.isfinite(dt) or dt<=0) else max(5, int(round(window_days/dt)))\n",
    "    return wl if wl % 2 == 1 else wl + 1\n",
    "\n",
    "def savgol_trend(t, y, window_len, polyorder=2):\n",
    "    half = window_len//2\n",
    "    trend = np.full_like(y, np.nan, dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        lo, hi = max(0, i-half), min(len(y), i+half+1)\n",
    "        tt = t[lo:hi] - t[i]; yy = y[lo:hi]\n",
    "        m  = np.isfinite(tt) & np.isfinite(yy)\n",
    "        if np.count_nonzero(m) >= polyorder+1:\n",
    "            c = np.polyfit(tt[m], yy[m], polyorder)\n",
    "            trend[i] = np.polyval(c, 0.0)\n",
    "    trend[~np.isfinite(trend)] = np.nanmedian(y)\n",
    "    return trend\n",
    "\n",
    "def custom_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Return a *new* LightCurve with flattened flux (no .replace() calls).\"\"\"\n",
    "    t = lc.time.value\n",
    "    f = lc.flux.value.astype(float)\n",
    "    wl = window_len_for(t, window_days)\n",
    "    tr = savgol_trend(t, f, wl, polyorder=polyorder)\n",
    "    flat = f/np.where(np.isfinite(tr), tr, np.nanmedian(f))\n",
    "    flat /= np.nanmedian(flat)\n",
    "    # carry flux_err if present\n",
    "    try:\n",
    "        ferr = lc.flux_err.value if getattr(lc, \"flux_err\", None) is not None else None\n",
    "    except Exception:\n",
    "        ferr = None\n",
    "    from lightkurve import LightCurve\n",
    "    return LightCurve(time=lc.time, flux=flat, flux_err=ferr, meta=dict(getattr(lc, \"meta\", {})))\n",
    "\n",
    "def approx_cdpp_ppm(lc, hours=1.0):\n",
    "    t = lc.time.value; f = lc.flux.value\n",
    "    dt_hr = np.nanmedian(np.diff(t))*24.0\n",
    "    if not np.isfinite(dt_hr) or dt_hr<=0: return float(\"nan\")\n",
    "    n = max(1, int(round(hours/dt_hr)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run)*1e6)\n",
    "\n",
    "def get_header_sector(header):\n",
    "    # Try common sector keys; then regex fallback\n",
    "    for k in (\"SECTOR\",\"sector\",\"SEC1\"):\n",
    "        try:\n",
    "            v = header.get(k)\n",
    "            if v is not None:\n",
    "                return int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    txt = \" \".join([f\"{k}={header.get(k)}\" for k in header.keys()])\n",
    "    m = re.search(r\"[Ss]ector[^0-9]*([0-9]{1,3})\", txt)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# --- Lightkurve ---\n",
    "import lightkurve as lk\n",
    "from lightkurve import LightCurveCollection\n",
    "\n",
    "def run_download_clean(target_toi: str, target_tic: int, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS):\n",
    "    TARGET = f\"TIC {target_tic}\"\n",
    "    out_fig  = f\"figures/TIC{target_tic}_download_clean.png\"\n",
    "    out_json = f\"results/TIC{target_tic}_download_clean_summary.json\"\n",
    "\n",
    "    print(f\"Target: {target_toi} ({TARGET})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Download SPOC PDCSAP first; read sector from files (robust to missing 'sector' column)\n",
    "    # -------------------------\n",
    "    pdcsap_list, got_spoc_sectors = [], set()\n",
    "    sr_spoc = lk.search_lightcurvefile(TARGET, mission=\"TESS\", author=\"SPOC\")\n",
    "\n",
    "    if len(sr_spoc) == 0:\n",
    "        print(\"No SPOC PDCSAP products found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_spoc)):\n",
    "            try:\n",
    "                lcf = sr_spoc[i].download(download_dir=\"data_raw_fresh\")\n",
    "                lc  = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "                # sector from object or header\n",
    "                s = getattr(lcf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = lcf.header() if hasattr(lcf, \"header\") else lcf.get_header()\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                lc.meta[\"source\"] = \"PDCSAP\"; lc.meta[\"sector\"] = int(s)\n",
    "                pdcsap_list.append(lc)\n",
    "                if s != -1: got_spoc_sectors.add(int(s))\n",
    "                print(f\" PDCSAP S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\" PDCSAP entry {i}: failed -> {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Query TESSCut and add FFI for sectors not covered by PDCSAP\n",
    "    # -------------------------\n",
    "    ffi_list = []\n",
    "    try:\n",
    "        sr_cut = lk.search_tesscut(TARGET)\n",
    "    except Exception as e:\n",
    "        sr_cut = []\n",
    "        print(\"TESSCut query failed:\", e)\n",
    "\n",
    "    if len(sr_cut) == 0:\n",
    "        print(\"No TESSCut entries found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_cut)):\n",
    "            try:\n",
    "                tpf = sr_cut[i].download(cutout_size=cutout_sz, download_dir=\"data_raw_fresh\")\n",
    "                # sector from object or header\n",
    "                s = getattr(tpf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = tpf.hdu[0].header\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                # skip if PDCSAP already covers\n",
    "                if (s != -1) and (int(s) in got_spoc_sectors):\n",
    "                    print(f\"  FFI S{int(s)}: skip (already have PDCSAP)\")\n",
    "                    continue\n",
    "                # aperture photometry\n",
    "                mask = tpf.create_threshold_mask(threshold=3, reference_pixel=None)\n",
    "                if not np.any(mask):\n",
    "                    m = np.zeros_like(mask, dtype=bool)\n",
    "                    yy, xx = np.unravel_index(np.nanargmax(np.nanmedian(tpf.flux.value, axis=0)), mask.shape)\n",
    "                    m[yy, xx] = True\n",
    "                    mask = m\n",
    "                lc = tpf.to_lightcurve(aperture_mask=mask).remove_nans().normalize()\n",
    "                lc.meta[\"source\"] = \"FFI\"; lc.meta[\"sector\"] = int(s)\n",
    "                ffi_list.append(lc)\n",
    "                print(f\"  FFI S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  FFI entry {i}: failed -> {e}\")\n",
    "\n",
    "    if not (pdcsap_list or ffi_list):\n",
    "        raise RuntimeError(\"No light curves obtained from SPOC or TESSCut.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Stitch, flatten, QC\n",
    "    # -------------------------\n",
    "    all_lcs_raw   = pdcsap_list + ffi_list\n",
    "    stitched_raw  = LightCurveCollection(all_lcs_raw).stitch().remove_nans()\n",
    "    stitched_flat = custom_flatten(stitched_raw, window_days=win_days)\n",
    "\n",
    "    def rms_ppm(x): return float(np.nanstd(x)*1e6)\n",
    "    qc = {\n",
    "        \"target_toi\": target_toi,\n",
    "        \"target_tic\": target_tic,\n",
    "        \"n_sectors_pdcsap\": len(pdcsap_list),\n",
    "        \"n_sectors_ffi\": len(ffi_list),\n",
    "        \"sectors_pdcsap\": [int(lc.meta.get(\"sector\", -1)) for lc in pdcsap_list],\n",
    "        \"sectors_ffi\": [int(lc.meta.get(\"sector\", -1)) for lc in ffi_list],\n",
    "        \"n_points_raw\": int(len(stitched_raw)),\n",
    "        \"n_points_flat\": int(len(stitched_flat)),\n",
    "        \"rms_raw_ppm\": rms_ppm(stitched_raw.flux.value),\n",
    "        \"rms_flat_ppm\": rms_ppm(stitched_flat.flux.value),\n",
    "        \"cdpp1h_flat_ppm\": approx_cdpp_ppm(stitched_flat, hours=1.0),\n",
    "        \"flatten_window_days\": float(win_days),\n",
    "        \"notes\": \"PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.\"\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Figure & JSON\n",
    "    # -------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,6), dpi=140, sharex=True)\n",
    "    stitched_raw.plot(ax=ax1, marker=\".\", lw=0, alpha=0.35)\n",
    "    ax1.set_ylabel(\"Flux (norm)\")\n",
    "    ax1.set_title(f\"{target_toi} (TIC {target_tic}) — Stitched RAW\")\n",
    "    stitched_flat.plot(ax=ax2, marker=\".\", lw=0, alpha=0.45, label=\"flattened\")\n",
    "    ax2.set_xlabel(\"Time [BTJD]\")\n",
    "    ax2.set_ylabel(\"Flux (norm)\")\n",
    "    ax2.set_title(f\"{target_toi} — Stitched FLAT (window={win_days:.2f} d)\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_fig); plt.close(fig)\n",
    "\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(qc, f, indent=2)\n",
    "\n",
    "    print(\"\\n== DONE ==\")\n",
    "    print(f\"Saved figure: {out_fig}\")\n",
    "    print(f\"Saved JSON  : {out_json}\")\n",
    "    print(\"Summary:\", qc)\n",
    "\n",
    "    return qc, out_fig, out_json\n",
    "\n",
    "# ---------- Run on Target A (deliverable) ----------\n",
    "_ = run_download_clean(TARGET_TOI, TARGET_TIC, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "\n",
    "# ---------- OPTIONAL: batch-run first 4 priority targets ----------\n",
    "# Set RUN_BATCH=True to run on the first 4 rows in results/priority_targets.csv\n",
    "RUN_BATCH = False\n",
    "if RUN_BATCH:\n",
    "    import pandas as pd\n",
    "    dfp = pd.read_csv(\"results/priority_targets.csv\")\n",
    "    # Expect columns: TIC_ID_norm, toi (string/number). Adjust if your headers differ.\n",
    "    for _, row in dfp.head(4).iterrows():\n",
    "        tic = int(row[\"TIC_ID_norm\"])\n",
    "        toi = f\"TOI {row['toi']}\"\n",
    "        try:\n",
    "            run_download_clean(toi, tic, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "        except Exception as e:\n",
    "            print(f\"[Batch] {toi} (TIC {tic}) failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7962c1f8-998c-443c-ae97-1c3aa7378a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: TOI 139.01 (TIC 62483237)\n",
      " PDCSAP S1: ok (N=18094)\n",
      " PDCSAP S28: ok (N=77241)\n",
      " PDCSAP S28: ok (N=12584)\n",
      " PDCSAP S68: ok (N=87527)\n",
      " PDCSAP S68: ok (N=14710)\n",
      "  FFI S1: skip (already have PDCSAP)\n",
      "  FFI S28: skip (already have PDCSAP)\n",
      "  FFI S68: skip (already have PDCSAP)\n",
      "  FFI S95: ok (N=10114)\n",
      "\n",
      "== DONE ==\n",
      "Saved figure: figures/TIC62483237_download_clean.png\n",
      "Saved JSON  : results/TIC62483237_download_clean_summary.json\n",
      "Summary: {'target_toi': 'TOI 139.01', 'target_tic': 62483237, 'n_sectors_pdcsap': 5, 'n_sectors_ffi': 1, 'sectors_pdcsap': [1, 28, 28, 68, 68], 'sectors_ffi': [95], 'n_points_raw': 220270, 'n_points_flat': 220270, 'rms_raw_ppm': 368080.16896247864, 'rms_flat_ppm': 167808.4942875445, 'cdpp1h_flat_ppm': 84620.20798996135, 'flatten_window_days': 1.0, 'notes': 'PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.'}\n"
     ]
    }
   ],
   "source": [
    "# 02_download_clean — Target A (PDCSAP-first, TESSCut-FFI fallback) — robust & batchable\n",
    "import json, warnings, pathlib, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config (single-target defaults) ---\n",
    "TARGET_TOI=\"TOI 139.01\"\n",
    "TARGET_TIC=62483237\n",
    "QUALITY    = 175\n",
    "CUTOUT_SZ  = 15\n",
    "WIN_DAYS   = 1.0\n",
    "\n",
    "# --- Folders ---\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"data_raw_fresh\").mkdir(exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def window_len_for(t_days, window_days=1.0):\n",
    "    dt = np.nanmedian(np.diff(t_days))\n",
    "    wl = 401 if (not np.isfinite(dt) or dt<=0) else max(5, int(round(window_days/dt)))\n",
    "    return wl if wl % 2 == 1 else wl + 1\n",
    "\n",
    "def savgol_trend(t, y, window_len, polyorder=2):\n",
    "    half = window_len//2\n",
    "    trend = np.full_like(y, np.nan, dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        lo, hi = max(0, i-half), min(len(y), i+half+1)\n",
    "        tt = t[lo:hi] - t[i]; yy = y[lo:hi]\n",
    "        m  = np.isfinite(tt) & np.isfinite(yy)\n",
    "        if np.count_nonzero(m) >= polyorder+1:\n",
    "            c = np.polyfit(tt[m], yy[m], polyorder)\n",
    "            trend[i] = np.polyval(c, 0.0)\n",
    "    trend[~np.isfinite(trend)] = np.nanmedian(y)\n",
    "    return trend\n",
    "\n",
    "def custom_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Return a *new* LightCurve with flattened flux (no .replace() calls).\"\"\"\n",
    "    t = lc.time.value\n",
    "    f = lc.flux.value.astype(float)\n",
    "    wl = window_len_for(t, window_days)\n",
    "    tr = savgol_trend(t, f, wl, polyorder=polyorder)\n",
    "    flat = f/np.where(np.isfinite(tr), tr, np.nanmedian(f))\n",
    "    flat /= np.nanmedian(flat)\n",
    "    # carry flux_err if present\n",
    "    try:\n",
    "        ferr = lc.flux_err.value if getattr(lc, \"flux_err\", None) is not None else None\n",
    "    except Exception:\n",
    "        ferr = None\n",
    "    from lightkurve import LightCurve\n",
    "    return LightCurve(time=lc.time, flux=flat, flux_err=ferr, meta=dict(getattr(lc, \"meta\", {})))\n",
    "\n",
    "def approx_cdpp_ppm(lc, hours=1.0):\n",
    "    t = lc.time.value; f = lc.flux.value\n",
    "    dt_hr = np.nanmedian(np.diff(t))*24.0\n",
    "    if not np.isfinite(dt_hr) or dt_hr<=0: return float(\"nan\")\n",
    "    n = max(1, int(round(hours/dt_hr)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run)*1e6)\n",
    "\n",
    "def get_header_sector(header):\n",
    "    # Try common sector keys; then regex fallback\n",
    "    for k in (\"SECTOR\",\"sector\",\"SEC1\"):\n",
    "        try:\n",
    "            v = header.get(k)\n",
    "            if v is not None:\n",
    "                return int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    txt = \" \".join([f\"{k}={header.get(k)}\" for k in header.keys()])\n",
    "    m = re.search(r\"[Ss]ector[^0-9]*([0-9]{1,3})\", txt)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "# --- Lightkurve ---\n",
    "import lightkurve as lk\n",
    "from lightkurve import LightCurveCollection\n",
    "\n",
    "def run_download_clean(target_toi: str, target_tic: int, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS):\n",
    "    TARGET = f\"TIC {target_tic}\"\n",
    "    out_fig  = f\"figures/TIC{target_tic}_download_clean.png\"\n",
    "    out_json = f\"results/TIC{target_tic}_download_clean_summary.json\"\n",
    "\n",
    "    print(f\"Target: {target_toi} ({TARGET})\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) Download SPOC PDCSAP first; read sector from files (robust to missing 'sector' column)\n",
    "    # -------------------------\n",
    "    pdcsap_list, got_spoc_sectors = [], set()\n",
    "    sr_spoc = lk.search_lightcurvefile(TARGET, mission=\"TESS\", author=\"SPOC\")\n",
    "\n",
    "    if len(sr_spoc) == 0:\n",
    "        print(\"No SPOC PDCSAP products found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_spoc)):\n",
    "            try:\n",
    "                lcf = sr_spoc[i].download(download_dir=\"data_raw_fresh\")\n",
    "                lc  = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "                # sector from object or header\n",
    "                s = getattr(lcf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = lcf.header() if hasattr(lcf, \"header\") else lcf.get_header()\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                lc.meta[\"source\"] = \"PDCSAP\"; lc.meta[\"sector\"] = int(s)\n",
    "                pdcsap_list.append(lc)\n",
    "                if s != -1: got_spoc_sectors.add(int(s))\n",
    "                print(f\" PDCSAP S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\" PDCSAP entry {i}: failed -> {e}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) Query TESSCut and add FFI for sectors not covered by PDCSAP\n",
    "    # -------------------------\n",
    "    ffi_list = []\n",
    "    try:\n",
    "        sr_cut = lk.search_tesscut(TARGET)\n",
    "    except Exception as e:\n",
    "        sr_cut = []\n",
    "        print(\"TESSCut query failed:\", e)\n",
    "\n",
    "    if len(sr_cut) == 0:\n",
    "        print(\"No TESSCut entries found.\")\n",
    "    else:\n",
    "        for i in range(len(sr_cut)):\n",
    "            try:\n",
    "                tpf = sr_cut[i].download(cutout_size=cutout_sz, download_dir=\"data_raw_fresh\")\n",
    "                # sector from object or header\n",
    "                s = getattr(tpf, \"sector\", None)\n",
    "                if s is None:\n",
    "                    try:\n",
    "                        hdr = tpf.hdu[0].header\n",
    "                    except Exception:\n",
    "                        hdr = None\n",
    "                    if hdr is not None:\n",
    "                        s = get_header_sector(hdr)\n",
    "                if s is None: s = -1\n",
    "                # skip if PDCSAP already covers\n",
    "                if (s != -1) and (int(s) in got_spoc_sectors):\n",
    "                    print(f\"  FFI S{int(s)}: skip (already have PDCSAP)\")\n",
    "                    continue\n",
    "                # aperture photometry\n",
    "                mask = tpf.create_threshold_mask(threshold=3, reference_pixel=None)\n",
    "                if not np.any(mask):\n",
    "                    m = np.zeros_like(mask, dtype=bool)\n",
    "                    yy, xx = np.unravel_index(np.nanargmax(np.nanmedian(tpf.flux.value, axis=0)), mask.shape)\n",
    "                    m[yy, xx] = True\n",
    "                    mask = m\n",
    "                lc = tpf.to_lightcurve(aperture_mask=mask).remove_nans().normalize()\n",
    "                lc.meta[\"source\"] = \"FFI\"; lc.meta[\"sector\"] = int(s)\n",
    "                ffi_list.append(lc)\n",
    "                print(f\"  FFI S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  FFI entry {i}: failed -> {e}\")\n",
    "\n",
    "    if not (pdcsap_list or ffi_list):\n",
    "        raise RuntimeError(\"No light curves obtained from SPOC or TESSCut.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) Stitch, flatten, QC\n",
    "    # -------------------------\n",
    "    all_lcs_raw   = pdcsap_list + ffi_list\n",
    "    stitched_raw  = LightCurveCollection(all_lcs_raw).stitch().remove_nans()\n",
    "    stitched_flat = custom_flatten(stitched_raw, window_days=win_days)\n",
    "\n",
    "    def rms_ppm(x): return float(np.nanstd(x)*1e6)\n",
    "    qc = {\n",
    "        \"target_toi\": target_toi,\n",
    "        \"target_tic\": target_tic,\n",
    "        \"n_sectors_pdcsap\": len(pdcsap_list),\n",
    "        \"n_sectors_ffi\": len(ffi_list),\n",
    "        \"sectors_pdcsap\": [int(lc.meta.get(\"sector\", -1)) for lc in pdcsap_list],\n",
    "        \"sectors_ffi\": [int(lc.meta.get(\"sector\", -1)) for lc in ffi_list],\n",
    "        \"n_points_raw\": int(len(stitched_raw)),\n",
    "        \"n_points_flat\": int(len(stitched_flat)),\n",
    "        \"rms_raw_ppm\": rms_ppm(stitched_raw.flux.value),\n",
    "        \"rms_flat_ppm\": rms_ppm(stitched_flat.flux.value),\n",
    "        \"cdpp1h_flat_ppm\": approx_cdpp_ppm(stitched_flat, hours=1.0),\n",
    "        \"flatten_window_days\": float(win_days),\n",
    "        \"notes\": \"PDCSAP preferred; FFI(TESSCut) used where SPOC PDCSAP missing.\"\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) Figure & JSON\n",
    "    # -------------------------\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,6), dpi=140, sharex=True)\n",
    "    stitched_raw.plot(ax=ax1, marker=\".\", lw=0, alpha=0.35)\n",
    "    ax1.set_ylabel(\"Flux (norm)\")\n",
    "    ax1.set_title(f\"{target_toi} (TIC {target_tic}) — Stitched RAW\")\n",
    "    stitched_flat.plot(ax=ax2, marker=\".\", lw=0, alpha=0.45, label=\"flattened\")\n",
    "    ax2.set_xlabel(\"Time [BTJD]\")\n",
    "    ax2.set_ylabel(\"Flux (norm)\")\n",
    "    ax2.set_title(f\"{target_toi} — Stitched FLAT (window={win_days:.2f} d)\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_fig); plt.close(fig)\n",
    "\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(qc, f, indent=2)\n",
    "\n",
    "    print(\"\\n== DONE ==\")\n",
    "    print(f\"Saved figure: {out_fig}\")\n",
    "    print(f\"Saved JSON  : {out_json}\")\n",
    "    print(\"Summary:\", qc)\n",
    "\n",
    "    return qc, out_fig, out_json\n",
    "\n",
    "# ---------- Run on Target A (deliverable) ----------\n",
    "_ = run_download_clean(TARGET_TOI, TARGET_TIC, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "\n",
    "# ---------- OPTIONAL: batch-run first 4 priority targets ----------\n",
    "# Set RUN_BATCH=True to run on the first 4 rows in results/priority_targets.csv\n",
    "RUN_BATCH = False\n",
    "if RUN_BATCH:\n",
    "    import pandas as pd\n",
    "    dfp = pd.read_csv(\"results/priority_targets.csv\")\n",
    "    # Expect columns: TIC_ID_norm, toi (string/number). Adjust if your headers differ.\n",
    "    for _, row in dfp.head(4).iterrows():\n",
    "        tic = int(row[\"TIC_ID_norm\"])\n",
    "        toi = f\"TOI {row['toi']}\"\n",
    "        try:\n",
    "            run_download_clean(toi, tic, quality=QUALITY, cutout_sz=CUTOUT_SZ, win_days=WIN_DAYS)\n",
    "        except Exception as e:\n",
    "            print(f\"[Batch] {toi} (TIC {tic}) failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab71a61-cda8-418f-b051-f0893c733f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: TOI 1801.01 (TIC 119584412)\n",
      " PDCSAP S22: ok (N=16101)\n",
      " PDCSAP S49: ok (N=13272)\n",
      "  FFI S22: skip (already have PDCSAP)\n",
      "  FFI S49: skip (already have PDCSAP)\n",
      "\n",
      "== DONE ==\n",
      "Saved figure: figures/TIC119584412_download_clean.png\n",
      "Saved JSON  : results/TIC119584412_download_clean_summary.json\n",
      "Summary: {'target_toi': 'TOI 1801.01', 'target_tic': 119584412, 'n_sectors_pdcsap': 2, 'n_sectors_ffi': 0, 'sectors_pdcsap': [22, 49], 'sectors_ffi': [], 'n_points_raw': 29370, 'n_points_flat': 29370, 'rms_raw_ppm': 1356.3033426180482, 'rms_flat_ppm': 1096.2121625978075, 'cdpp1h_flat_ppm': 251.0249996146523, 'flatten_window_days': 1.0, 'notes': 'PDCSAP preferred; FFI(TESSCut) only when SPOC PDCSAP is missing.'}\n"
     ]
    }
   ],
   "source": [
    "# 02_download_clean — PDCSAP-first, TESSCut-FFI fallback (ONE-CELL VERSION)\n",
    "# “PDCSAP first; FFI fallback; stitched + flattened; saves figure + QC.”\n",
    "# ---- EDIT THESE TWO LINES ONLY ----\n",
    "TARGET_TOI = \"TOI 1801.01\"   # label for plots\n",
    "TARGET_TIC = 119584412       # integer TIC ID\n",
    "# -----------------------------------\n",
    "\n",
    "import json, warnings, pathlib, re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Config ---\n",
    "QUALITY    = 175         # standard TESS quality mask (matches Lightkurve’s default)\n",
    "CUTOUT_SZ  = 15          # TESSCut postage-stamp size (pixels)\n",
    "WIN_DAYS   = 1.0         # flattening window (days) – gentle, won’t erase hour-long dips\n",
    "OUT_FIG    = f\"figures/TIC{TARGET_TIC}_download_clean.png\"\n",
    "OUT_JSON   = f\"results/TIC{TARGET_TIC}_download_clean_summary.json\"\n",
    "TARGET     = f\"TIC {TARGET_TIC}\"\n",
    "\n",
    "# --- Make folders ---\n",
    "for d in [\"figures\", \"results\", \"data_raw_fresh\"]:\n",
    "    pathlib.Path(d).mkdir(exist_ok=True)\n",
    "\n",
    "# --- Helpers ---\n",
    "def window_len_for(t_days, window_days=1.0):\n",
    "    dt = np.nanmedian(np.diff(t_days))\n",
    "    wl = 401 if (not np.isfinite(dt) or dt<=0) else max(5, int(round(window_days/dt)))\n",
    "    return wl if wl % 2 == 1 else wl + 1\n",
    "\n",
    "def savgol_trend(t, y, window_len, polyorder=2):\n",
    "    half = window_len//2\n",
    "    trend = np.full_like(y, np.nan, dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        lo, hi = max(0, i-half), min(len(y), i+half+1)\n",
    "        tt = t[lo:hi] - t[i]; yy = y[lo:hi]\n",
    "        m = np.isfinite(tt) & np.isfinite(yy)\n",
    "        if np.count_nonzero(m) >= polyorder+1:\n",
    "            c = np.polyfit(tt[m], yy[m], polyorder)\n",
    "            trend[i] = np.polyval(c, 0.0)\n",
    "    trend[~np.isfinite(trend)] = np.nanmedian(y)\n",
    "    return trend\n",
    "\n",
    "def custom_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Return a NEW LightCurve with same time, flattened flux (median=1).\"\"\"\n",
    "    from lightkurve import LightCurve\n",
    "    t = lc.time.value\n",
    "    f = lc.flux.value.astype(float)\n",
    "    wl = window_len_for(t, window_days)\n",
    "    tr = savgol_trend(t, f, wl, polyorder=polyorder)\n",
    "    flat = f/np.where(np.isfinite(tr), tr, np.nanmedian(f))\n",
    "    flat /= np.nanmedian(flat)\n",
    "    return LightCurve(time=lc.time, flux=flat, flux_err=getattr(lc, \"flux_err\", None), meta=lc.meta)\n",
    "\n",
    "def approx_cdpp_ppm(lc, hours=1.0):\n",
    "    t = lc.time.value; f = lc.flux.value\n",
    "    dt_hr = np.nanmedian(np.diff(t))*24.0\n",
    "    if not np.isfinite(dt_hr) or dt_hr<=0: return float(\"nan\")\n",
    "    n = max(1, int(round(hours/dt_hr)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run)*1e6)\n",
    "\n",
    "def get_header_sector(header):\n",
    "    for k in (\"SECTOR\",\"sector\",\"SEC1\"):\n",
    "        try:\n",
    "            v = header.get(k)\n",
    "            if v is not None:\n",
    "                return int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    txt = \" \".join([f\"{k}={header.get(k)}\" for k in header.keys()])\n",
    "    m = re.search(r\"[Ss]ector[^0-9]*([0-9]{1,3})\", txt)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def robust_ylim(flux, lo=0.2, hi=99.8):\n",
    "    lo_, hi_ = np.nanpercentile(flux, [lo, hi])\n",
    "    pad = 0.03*(hi_ - lo_)\n",
    "    return lo_-pad, hi_+pad\n",
    "\n",
    "def apply_quality_mask(lc, bitmask=QUALITY):\n",
    "    \"\"\"Keep cadences where (quality & ~bitmask)==0, i.e., standard good cadences.\"\"\"\n",
    "    q = getattr(lc, \"quality\", None)\n",
    "    if q is None:\n",
    "        return lc\n",
    "    good = (q & ~bitmask) == 0\n",
    "    return lc[good]\n",
    "\n",
    "# --- Lightkurve imports ---\n",
    "import lightkurve as lk\n",
    "from lightkurve import LightCurveCollection\n",
    "\n",
    "print(f\"Target: {TARGET_TOI} ({TARGET})\")\n",
    "\n",
    "# =========================\n",
    "# 1) PDCSAP (SPOC) download\n",
    "# =========================\n",
    "pdcsap_list, got_spoc_sectors = [], set()\n",
    "sr_spoc = lk.search_lightcurvefile(TARGET, mission=\"TESS\", author=\"SPOC\")\n",
    "\n",
    "if len(sr_spoc) == 0:\n",
    "    print(\"No SPOC PDCSAP products found.\")\n",
    "else:\n",
    "    for i in range(len(sr_spoc)):\n",
    "        try:\n",
    "            lcf = sr_spoc[i].download(download_dir=\"data_raw_fresh\")\n",
    "            lc  = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "            lc  = apply_quality_mask(lc, QUALITY)\n",
    "\n",
    "            # sector from object or header\n",
    "            s = getattr(lcf, \"sector\", None)\n",
    "            if s is None:\n",
    "                hdr = None\n",
    "                try:\n",
    "                    hdr = lcf.header() if hasattr(lcf, \"header\") else lcf.get_header()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if hdr is not None:\n",
    "                    s = get_header_sector(hdr)\n",
    "            if s is None: s = -1\n",
    "\n",
    "            lc.meta[\"source\"] = \"PDCSAP\"; lc.meta[\"sector\"] = int(s)\n",
    "            pdcsap_list.append(lc)\n",
    "            if s != -1: got_spoc_sectors.add(int(s))\n",
    "            print(f\" PDCSAP S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "        except Exception as e:\n",
    "            print(f\" PDCSAP entry {i}: failed -> {e}\")\n",
    "\n",
    "# ==============================\n",
    "# 2) TESSCut (FFI) for gaps only\n",
    "# ==============================\n",
    "ffi_list = []\n",
    "try:\n",
    "    sr_cut = lk.search_tesscut(TARGET)\n",
    "except Exception as e:\n",
    "    sr_cut = []\n",
    "    print(\"TESSCut query failed:\", e)\n",
    "\n",
    "if len(sr_cut) == 0:\n",
    "    print(\"No TESSCut entries found.\")\n",
    "else:\n",
    "    for i in range(len(sr_cut)):\n",
    "        try:\n",
    "            tpf = sr_cut[i].download(cutout_size=CUTOUT_SZ, download_dir=\"data_raw_fresh\")\n",
    "            s = getattr(tpf, \"sector\", None)\n",
    "            if s is None:\n",
    "                hdr = None\n",
    "                try:\n",
    "                    hdr = tpf.hdu[0].header\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if hdr is not None:\n",
    "                    s = get_header_sector(hdr)\n",
    "            if s is None: s = -1\n",
    "\n",
    "            if (s != -1) and (int(s) in got_spoc_sectors):\n",
    "                print(f\"  FFI S{int(s)}: skip (already have PDCSAP)\")\n",
    "                continue\n",
    "\n",
    "            mask = tpf.create_threshold_mask(threshold=3, reference_pixel=None)\n",
    "            if not np.any(mask):\n",
    "                m = np.zeros_like(mask, dtype=bool)\n",
    "                yy, xx = np.unravel_index(np.nanargmax(np.nanmedian(tpf.flux.value, axis=0)), mask.shape)\n",
    "                m[yy, xx] = True\n",
    "                mask = m\n",
    "\n",
    "            lc = tpf.to_lightcurve(aperture_mask=mask).remove_nans().normalize()\n",
    "            lc = apply_quality_mask(lc, QUALITY)\n",
    "            lc.meta[\"source\"] = \"FFI\"; lc.meta[\"sector\"] = int(s)\n",
    "            ffi_list.append(lc)\n",
    "            print(f\"  FFI S{int(s) if s!=-1 else '?'}: ok (N={len(lc)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"  FFI entry {i}: failed -> {e}\")\n",
    "\n",
    "if not (pdcsap_list or ffi_list):\n",
    "    raise RuntimeError(\"No light curves obtained from SPOC or TESSCut.\")\n",
    "\n",
    "# =======================\n",
    "# 3) Stitch, clean, QC\n",
    "# =======================\n",
    "all_lcs_raw   = pdcsap_list + ffi_list\n",
    "stitched_raw  = LightCurveCollection(all_lcs_raw).stitch().remove_nans()\n",
    "\n",
    "# very light outlier clip (protects plots; safe for downloader stage)\n",
    "f = stitched_raw.flux.value\n",
    "med, sig = np.nanmedian(f), np.nanstd(f)\n",
    "m = (f > med - 5*sig) & (f < med + 5*sig)\n",
    "stitched_raw = stitched_raw[m]\n",
    "\n",
    "stitched_flat = custom_flatten(stitched_raw, window_days=WIN_DAYS)\n",
    "\n",
    "def rms_ppm(x): return float(np.nanstd(x)*1e6)\n",
    "qc = {\n",
    "    \"target_toi\": TARGET_TOI,\n",
    "    \"target_tic\": int(TARGET_TIC),\n",
    "    \"n_sectors_pdcsap\": len(pdcsap_list),\n",
    "    \"n_sectors_ffi\": len(ffi_list),\n",
    "    \"sectors_pdcsap\": [int(lc.meta.get(\"sector\", -1)) for lc in pdcsap_list],\n",
    "    \"sectors_ffi\": [int(lc.meta.get(\"sector\", -1)) for lc in ffi_list],\n",
    "    \"n_points_raw\": int(len(stitched_raw)),\n",
    "    \"n_points_flat\": int(len(stitched_flat)),\n",
    "    \"rms_raw_ppm\": rms_ppm(stitched_raw.flux.value),\n",
    "    \"rms_flat_ppm\": rms_ppm(stitched_flat.flux.value),\n",
    "    \"cdpp1h_flat_ppm\": approx_cdpp_ppm(stitched_flat, hours=1.0),\n",
    "    \"flatten_window_days\": WIN_DAYS,\n",
    "    \"notes\": \"PDCSAP preferred; FFI(TESSCut) only when SPOC PDCSAP is missing.\"\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# 4) Figure + JSON output\n",
    "# =======================\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12,6), dpi=140, sharex=True)\n",
    "t0 = 2457000.0\n",
    "\n",
    "ax1.scatter(stitched_raw.time.value, stitched_raw.flux.value, s=2, alpha=0.35)\n",
    "ax1.set_ylabel(\"Flux (norm)\")\n",
    "ax1.set_title(f\"{TARGET_TOI} ({TARGET}) — Stitched RAW\")\n",
    "\n",
    "ax2.scatter(stitched_flat.time.value, stitched_flat.flux.value, s=2, alpha=0.45, label=\"flattened\")\n",
    "ax2.set_xlabel(\"Time [BTJD]\")\n",
    "ax2.set_ylabel(\"Flux (norm)\")\n",
    "ax2.set_title(f\"{TARGET_TOI} — Stitched FLAT (window={WIN_DAYS:.2f} d)\")\n",
    "ax2.legend(loc=\"best\")\n",
    "\n",
    "# keep y-axes sensible even if an FFI chunk is wild\n",
    "y1_lo, y1_hi = robust_ylim(stitched_raw.flux.value)\n",
    "y2_lo, y2_hi = robust_ylim(stitched_flat.flux.value)\n",
    "ax1.set_ylim(y1_lo, y1_hi)\n",
    "ax2.set_ylim(y2_lo, y2_hi)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG); plt.close(fig)\n",
    "\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(qc, f, indent=2)\n",
    "\n",
    "print(\"\\n== DONE ==\")\n",
    "print(f\"Saved figure: {OUT_FIG}\")\n",
    "print(f\"Saved JSON  : {OUT_JSON}\")\n",
    "print(\"Summary:\", qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3b4630-d025-49e7-b31b-8a48b322992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sector QA complete: saved 2 plots to figures/ and summary to results/QA_TIC119584412_summary.json\n",
      " S22: PDCSAP, N_use=16101/16101, frac_bad=0.0, cdpp1h_ppm=740\n",
      " S49: PDCSAP, N_use=13272/13272, frac_bad=0.0, cdpp1h_ppm=937\n"
     ]
    }
   ],
   "source": [
    "# === Per-sector QA (robust: handles Time/Quantity types) ===\n",
    "import numpy as np, matplotlib.pyplot as plt, pathlib, json\n",
    "import lightkurve as lk\n",
    "\n",
    "TIC = TARGET_TIC\n",
    "BASE = f\"TIC{TIC}\"\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "\n",
    "def as_float_array(x):\n",
    "    \"\"\"Return a float numpy array from Lightkurve/astropy containers.\"\"\"\n",
    "    # Try .value (Quantity/Time), else assume it's already an array-like\n",
    "    try:\n",
    "        x = x.value\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.asarray(x, dtype=float)\n",
    "\n",
    "def rms_ppm(y):\n",
    "    y = as_float_array(y)\n",
    "    return float(np.nanstd(y) * 1e6)\n",
    "\n",
    "def cdpp_ppm(time, flux, hours=1.0):\n",
    "    t = as_float_array(time)\n",
    "    f = as_float_array(flux)\n",
    "    dt = np.nanmedian(np.diff(t))\n",
    "    if not np.isfinite(dt) or dt <= 0:\n",
    "        return float(\"nan\")\n",
    "    n = max(1, int(round((hours/24.0)/dt)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run)*1e6)\n",
    "\n",
    "def build_quality_mask(lc, quality_bitmask):\n",
    "    q = getattr(lc, \"quality\", None)\n",
    "    if q is not None and len(q) == len(lc.flux):\n",
    "        try:\n",
    "            return lk.utils.TessQualityFlags.create_quality_mask(q, bitmask=quality_bitmask)\n",
    "        except Exception:\n",
    "            return np.ones(len(lc.flux), dtype=bool)\n",
    "    return np.ones(len(lc.flux), dtype=bool)\n",
    "\n",
    "entries = []\n",
    "for src_list in (pdcsap_list, ffi_list):\n",
    "    for lc in src_list:\n",
    "        # sector & source tags\n",
    "        try:\n",
    "            sector = int(lc.meta.get(\"sector\", -1))\n",
    "        except Exception:\n",
    "            sector = -1\n",
    "        src_lbl = \"PDCSAP\" if lc.meta.get(\"source\") == \"PDCSAP\" else \"FFI\"\n",
    "\n",
    "        # arrays\n",
    "        t_all = as_float_array(lc.time)\n",
    "        f_all = as_float_array(lc.flux)\n",
    "\n",
    "        # masks\n",
    "        qmask  = build_quality_mask(lc, QUALITY)\n",
    "        finite = np.isfinite(t_all) & np.isfinite(f_all)\n",
    "        m = qmask & finite\n",
    "\n",
    "        N_all = int(len(f_all))\n",
    "        N_use = int(np.count_nonzero(m))\n",
    "        frac_bad = float(1 - (N_use / max(N_all, 1)))\n",
    "\n",
    "        # plot with plain arrays\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(11, 3), dpi=140)\n",
    "        ax.plot(t_all[m], f_all[m], \".\", ms=2, alpha=0.7)\n",
    "        ax.set_title(f\"{TARGET_TOI} S{sector if sector!=-1 else '?'} — {src_lbl}\")\n",
    "        ax.set_xlabel(\"Time [BTJD]\"); ax.set_ylabel(\"Flux (norm)\")\n",
    "        out_png = f\"figures/QA_{BASE}_S{sector if sector!=-1 else 'u'}.png\"\n",
    "        fig.tight_layout(); fig.savefig(out_png); plt.close(fig)\n",
    "\n",
    "        # metrics\n",
    "        entries.append({\n",
    "            \"sector\": sector,\n",
    "            \"source\": src_lbl,\n",
    "            \"N_all\": N_all,\n",
    "            \"N_use\": N_use,\n",
    "            \"frac_bad\": round(frac_bad, 4),\n",
    "            \"rms_ppm\": rms_ppm(f_all[m]),\n",
    "            \"cdpp1h_ppm\": cdpp_ppm(t_all[m], f_all[m]),\n",
    "        })\n",
    "\n",
    "if not entries:\n",
    "    raise RuntimeError(\"No sectors could be processed for QA (empty entries).\")\n",
    "\n",
    "# sort & save summary\n",
    "entries = sorted(entries, key=lambda d: (d[\"sector\"], d[\"source\"]))\n",
    "out_json = f\"results/QA_{BASE}_summary.json\"\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(entries, f, indent=2)\n",
    "\n",
    "print(f\"Per-sector QA complete: saved {len(entries)} plots to figures/ and summary to {out_json}\")\n",
    "for e in entries:\n",
    "    cdpp_int = int(e['cdpp1h_ppm']) if np.isfinite(e['cdpp1h_ppm']) else 'nan'\n",
    "    print(f\" S{e['sector']}: {e['source']}, N_use={e['N_use']}/{e['N_all']}, \"\n",
    "          f\"frac_bad={e['frac_bad']}, cdpp1h_ppm={cdpp_int}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f111e26-ed7f-4d71-9c80-37f8673f6fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detrend formalization on: TOI 1801.01 (TIC 119584412)\n",
      "  PDCSAP S22: N=16102\n",
      "  PDCSAP S49: N=13272\n",
      "\n",
      "Detrend formalization complete.\n",
      "Saved: results/TIC119584412_detrend_metrics.csv\n",
      "Saved: results/TIC119584412_detrend_params.json\n",
      "Saved: figures/TIC119584412_detrend_before_after.png\n"
     ]
    }
   ],
   "source": [
    "# ==== Formalize detrend params & save pre/post metrics (Target A) ====\n",
    "import json, pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from lightkurve import LightCurveCollection\n",
    "\n",
    "# robust import for quality flags across LK versions\n",
    "try:\n",
    "    from lightkurve.utils import TessQualityFlags\n",
    "except Exception:\n",
    "    TessQualityFlags = getattr(lk, \"TessQualityFlags\", None)\n",
    "assert TessQualityFlags is not None, \"Could not import TessQualityFlags from lightkurve.\"\n",
    "\n",
    "# Reuse target if already defined; else set Target A\n",
    "try:\n",
    "    TARGET_TOI, TARGET_TIC, TARGET\n",
    "except NameError:\n",
    "    TARGET_TOI = \"TOI 1801.01\"\n",
    "    TARGET_TIC = 119584412\n",
    "    TARGET     = f\"TIC {TARGET_TIC}\"\n",
    "\n",
    "# ---- detrend knobs we are \"locking\" today ----\n",
    "DETRENDS = {\n",
    "    \"window_days\": 1.0,      # gentle, won't eat shallow dips\n",
    "    \"polyorder\": 2,          # quadratic local trend\n",
    "    \"use_transit_mask\": False,\n",
    "    \"mask_width_factor\": 1.7\n",
    "}\n",
    "QUALITY = 175  # TESS bitmask\n",
    "\n",
    "# ---- helpers ----\n",
    "def to_vec(x):\n",
    "    \"\"\"Return a float array from Lightkurve/astropy/Masked inputs; fill masked with NaN.\"\"\"\n",
    "    # Prefer .value if present (astropy Time/Quantity), else the object itself\n",
    "    base = getattr(x, \"value\", x)\n",
    "    arr = np.array(base)\n",
    "    # If still an object array (e.g., astropy Time), try .astype(float)\n",
    "    try:\n",
    "        arr = arr.astype(float)\n",
    "    except Exception:\n",
    "        # final fallback: per-element conversion (slow but robust)\n",
    "        arr = np.array([float(v) if np.isfinite(v) else np.nan for v in np.array(base, dtype=object)], dtype=float)\n",
    "    # If masked, fill with nan\n",
    "    if np.ma.isMaskedArray(arr):\n",
    "        arr = arr.filled(np.nan)\n",
    "    return arr\n",
    "\n",
    "def window_len_for(t_days, window_days):\n",
    "    dt = np.nanmedian(np.diff(t_days))\n",
    "    wl = 401 if (not np.isfinite(dt) or dt <= 0) else max(5, int(round(window_days/dt)))\n",
    "    return wl if wl % 2 == 1 else wl + 1\n",
    "\n",
    "def savgol_trend(t, y, window_len, polyorder=2):\n",
    "    half = window_len // 2\n",
    "    trend = np.full_like(y, np.nan, dtype=float)\n",
    "    for i in range(len(y)):\n",
    "        lo, hi = max(0, i-half), min(len(y), i+half+1)\n",
    "        tt = t[lo:hi] - t[i]; yy = y[lo:hi]\n",
    "        m = np.isfinite(tt) & np.isfinite(yy)\n",
    "        if np.count_nonzero(m) >= polyorder+1:\n",
    "            c = np.polyfit(tt[m], yy[m], polyorder)\n",
    "            trend[i] = np.polyval(c, 0.0)\n",
    "    if not np.isfinite(trend).any():\n",
    "        trend[:] = np.nanmedian(y)\n",
    "    trend[~np.isfinite(trend)] = np.nanmedian(y)\n",
    "    return trend\n",
    "\n",
    "def flatten_simple(lc, window_days, polyorder):\n",
    "    t = to_vec(lc.time)\n",
    "    f = to_vec(lc.flux)\n",
    "    wl = window_len_for(t, window_days)\n",
    "    tr = savgol_trend(t, f, wl, polyorder=polyorder)\n",
    "    flat = f / np.where(np.isfinite(tr), tr, np.nanmedian(f))\n",
    "    flat /= np.nanmedian(flat)\n",
    "    out = lc.copy()\n",
    "    out.flux = flat\n",
    "    return out\n",
    "\n",
    "def approx_cdpp_ppm(lc, hours=1.0):\n",
    "    t = to_vec(lc.time); f = to_vec(lc.flux)\n",
    "    dt_hr = np.nanmedian(np.diff(t))*24.0\n",
    "    if not np.isfinite(dt_hr) or dt_hr <= 0:\n",
    "        return float(\"nan\")\n",
    "    n = max(1, int(round(hours/dt_hr)))\n",
    "    run = np.convolve(f - np.nanmedian(f), np.ones(n)/n, mode=\"valid\")\n",
    "    return float(np.nanstd(run) * 1e6)\n",
    "\n",
    "# ---- ensure folders ----\n",
    "pathlib.Path(\"results\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"figures\").mkdir(exist_ok=True)\n",
    "pathlib.Path(\"data_raw_fresh\").mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Detrend formalization on: {TARGET_TOI} (TIC {TARGET_TIC})\")\n",
    "\n",
    "# ---- fetch PDCSAP for A (PDCSAP-first) ----\n",
    "sr = lk.search_lightcurvefile(TARGET, mission=\"TESS\", author=\"SPOC\")\n",
    "\n",
    "rows = []         # ONLY numbers/strings for CSV\n",
    "raw_list = []     # LightCurve objects (raw/normalized)\n",
    "flat_list = []    # LightCurve objects (flattened)\n",
    "\n",
    "for i in range(len(sr)):\n",
    "    try:\n",
    "        lcf = sr[i].download(download_dir=\"data_raw_fresh\")\n",
    "        sector = int(getattr(lcf, \"sector\", -1) or -1)\n",
    "\n",
    "        lc0 = lcf.PDCSAP_FLUX\n",
    "        qmask = TessQualityFlags.create_quality_mask(lc0.quality, bitmask=QUALITY)\n",
    "        lc = lc0[qmask].remove_nans().normalize()\n",
    "        print(f\"  PDCSAP S{sector}: N={len(lc)}\")\n",
    "\n",
    "        lc_flat = flatten_simple(lc, DETRENDS[\"window_days\"], DETRENDS[\"polyorder\"])\n",
    "\n",
    "        rows.append({\n",
    "            \"tic\": TARGET_TIC,\n",
    "            \"toi\": TARGET_TOI,\n",
    "            \"sector\": sector,\n",
    "            \"source\": \"PDCSAP\",\n",
    "            \"n_raw\": int(len(lc)),\n",
    "            \"n_flat\": int(len(lc_flat)),\n",
    "            \"rms_raw_ppm\": float(np.nanstd(to_vec(lc.flux))*1e6),\n",
    "            \"rms_flat_ppm\": float(np.nanstd(to_vec(lc_flat.flux))*1e6),\n",
    "            \"cdpp1h_raw_ppm\": approx_cdpp_ppm(lc, hours=1.0),\n",
    "            \"cdpp1h_flat_ppm\": approx_cdpp_ppm(lc_flat, hours=1.0),\n",
    "            \"window_days\": DETRENDS[\"window_days\"],\n",
    "            \"polyorder\": DETRENDS[\"polyorder\"],\n",
    "            \"use_transit_mask\": DETRENDS[\"use_transit_mask\"],\n",
    "            \"mask_width_factor\": DETRENDS[\"mask_width_factor\"],\n",
    "            \"quality_bitmask\": QUALITY,\n",
    "        })\n",
    "        raw_list.append(lc)\n",
    "        flat_list.append(lc_flat)\n",
    "    except Exception as e:\n",
    "        print(f\"  PDCSAP entry {i}: failed -> {e}\")\n",
    "\n",
    "if not raw_list:\n",
    "    raise RuntimeError(\"No PDCSAP sectors processed; aborting.\")\n",
    "\n",
    "# ---- stitched summary row ----\n",
    "stitched_raw  = LightCurveCollection(raw_list).stitch().remove_nans().normalize()\n",
    "stitched_flat = LightCurveCollection(flat_list).stitch().remove_nans().normalize()\n",
    "\n",
    "rows.append({\n",
    "    \"tic\": TARGET_TIC,\n",
    "    \"toi\": TARGET_TOI,\n",
    "    \"sector\": -1,  # -1 = stitched\n",
    "    \"source\": \"STITCHED\",\n",
    "    \"n_raw\": int(len(stitched_raw)),\n",
    "    \"n_flat\": int(len(stitched_flat)),\n",
    "    \"rms_raw_ppm\": float(np.nanstd(to_vec(stitched_raw.flux))*1e6),\n",
    "    \"rms_flat_ppm\": float(np.nanstd(to_vec(stitched_flat.flux))*1e6),\n",
    "    \"cdpp1h_raw_ppm\": approx_cdpp_ppm(stitched_raw, hours=1.0),\n",
    "    \"cdpp1h_flat_ppm\": approx_cdpp_ppm(stitched_flat, hours=1.0),\n",
    "    \"window_days\": DETRENDS[\"window_days\"],\n",
    "    \"polyorder\": DETRENDS[\"polyorder\"],\n",
    "    \"use_transit_mask\": DETRENDS[\"use_transit_mask\"],\n",
    "    \"mask_width_factor\": DETRENDS[\"mask_width_factor\"],\n",
    "    \"quality_bitmask\": QUALITY,\n",
    "})\n",
    "\n",
    "# ---- save metrics & params ----\n",
    "metrics_csv = f\"results/TIC{TARGET_TIC}_detrend_metrics.csv\"\n",
    "pd.DataFrame(rows).sort_values([\"source\",\"sector\"]).to_csv(metrics_csv, index=False)\n",
    "\n",
    "params_json = f\"results/TIC{TARGET_TIC}_detrend_params.json\"\n",
    "with open(params_json, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"target_toi\": TARGET_TOI,\n",
    "        \"target_tic\": TARGET_TIC,\n",
    "        \"detrend_params\": DETRENDS,\n",
    "        \"quality_bitmask\": QUALITY,\n",
    "        \"created_by\": \"02_download_clean.ipynb\",\n",
    "    }, f, indent=2)\n",
    "\n",
    "# ---- quick before/after plot ----\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), dpi=140, sharex=True)\n",
    "stitched_raw.plot(ax=ax1, marker=\".\", lw=0, alpha=0.35)\n",
    "ax1.set_ylabel(\"Flux (norm)\")\n",
    "ax1.set_title(f\"{TARGET_TOI} — Stitched RAW (PDCSAP)\")\n",
    "\n",
    "stitched_flat.plot(ax=ax2, marker=\".\", lw=0, alpha=0.45)\n",
    "ax2.set_xlabel(\"Time [BTJD]\")\n",
    "ax2.set_ylabel(\"Flux (norm)\")\n",
    "ax2.set_title(f\"{TARGET_TOI} — Stitched FLAT (window={DETRENDS['window_days']} d, poly={DETRENDS['polyorder']})\")\n",
    "fig.tight_layout()\n",
    "fig_path = f\"figures/TIC{TARGET_TIC}_detrend_before_after.png\"\n",
    "fig.savefig(fig_path); plt.close(fig)\n",
    "\n",
    "print(\"\\nDetrend formalization complete.\")\n",
    "print(\"Saved:\", metrics_csv)\n",
    "print(\"Saved:\", params_json)\n",
    "print(\"Saved:\", fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741bca7b-2250-4d80-87f9-c2bda5d7fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOI 1801.01 — S22: N=16102\n",
      "\n",
      "[S22] points=16102  threads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S22] BLS done in 2.0s (Pmax used=23.92 d)\n",
      "[S22] TLS refine around 14.97339 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 16102 data points, 2414 periods from 0.601 to 13.29 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2414/2414 periods | 00:09<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 13.26054 days\n",
      "[S22]   TLS best P=13.260536 d, SDE=9.34 (took 10.5s)\n",
      "[S22] TLS refine around 15.98076 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 16102 data points, 2414 periods from 0.601 to 13.29 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2414/2414 periods | 00:08<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 13.26054 days\n",
      "[S22]   TLS best P=13.260536 d, SDE=9.34 (took 9.5s)\n",
      "[S22] TLS refine around 14.75786 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 16102 data points, 2414 periods from 0.601 to 13.29 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2414/2414 periods | 00:09<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 13.26054 days\n",
      "[S22]   TLS best P=13.260536 d, SDE=9.34 (took 10.5s)\n",
      "TOI 1801.01 — S49: N=13272\n",
      "\n",
      "[S49] points=13272  threads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S49] BLS done in 1.8s (Pmax used=21.49 d)\n",
      "[S49] TLS refine around 5.37177 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 13272 data points, 2125 periods from 0.601 to 11.942 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2125/2125 periods | 00:06<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 10.93615 days\n",
      "[S49]   TLS best P=10.936152 d, SDE=7.15 (took 7.5s)\n",
      "[S49] TLS refine around 5.43477 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 13272 data points, 2125 periods from 0.601 to 11.942 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2125/2125 periods | 00:06<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 10.93615 days\n",
      "[S49]   TLS best P=10.936152 d, SDE=7.15 (took 7.4s)\n",
      "[S49] TLS refine around 5.31297 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 13272 data points, 2125 periods from 0.601 to 11.942 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 2125/2125 periods | 00:06<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 10.93615 days\n",
      "[S49]   TLS best P=10.936152 d, SDE=7.15 (took 6.9s)\n",
      "\n",
      "[stitched] points=29374  threads=8\n",
      "[stitched] BLS done in 4.0s (Pmax used=50.00 d)\n",
      "[stitched] TLS refine around 16.02631 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 29374 data points, 359 periods from 15.866 to 16.187 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 359/359 periods | 00:05<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 16.02719 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 44 of 48 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=16.027187 d, SDE=2.30 (took 6.4s)\n",
      "[stitched] TLS refine around 15.02621 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 29374 data points, 366 periods from 14.876 to 15.176 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 366/366 periods | 00:04<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 15.04600 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 47 of 51 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=15.046000 d, SDE=1.73 (took 5.6s)\n",
      "[stitched] TLS refine around 15.67974 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 29374 data points, 361 periods from 15.524 to 15.836 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 361/361 periods | 00:04<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 15.80476 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/stats.py:458: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  snr_pink_per_transit[i] = (1 - mean_flux) / pinknoise\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 46 of 48 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=15.804763 d, SDE=1.71 (took 5.6s)\n",
      "\n",
      "Done: fast BLS→TLS complete (per-sector + stitched). Artifacts saved in figures/ and results/.\n"
     ]
    }
   ],
   "source": [
    "# =============================== #\n",
    "# FAST BLS→TLS (Target A)\n",
    "# Per-sector and stitched:\n",
    "# - BLS over a wide range (fast)\n",
    "# - TLS only near top BLS peaks (narrow windows, multi-threaded)\n",
    "# Saves: periodograms, top-3 CSVs, folded plots.\n",
    "# =============================== #\n",
    "\n",
    "import os, csv, time, json, warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "# ---- Silence Lightkurve warnings (version-safe) ----\n",
    "try:\n",
    "    from lightkurve.utils import LightkurveWarning\n",
    "except Exception:\n",
    "    class LightkurveWarning(Warning): pass\n",
    "warnings.filterwarnings(\"ignore\", category=LightkurveWarning)\n",
    "\n",
    "# ---- Config ----\n",
    "TARGET_TOI = \"TOI 1801.01\"\n",
    "TARGET_TIC = 119584412\n",
    "SECTORS    = [22, 49]                  # edit if needed\n",
    "\n",
    "# BLS search window (days)\n",
    "BLS_PERIOD_MIN = 0.5\n",
    "BLS_PERIOD_MAX = 50.0\n",
    "BLS_NPER       = 5000                  # keep reasonable; BLS is fast\n",
    "\n",
    "# Durations to test in hours (BLS)\n",
    "BLS_DURATIONS_HR = np.linspace(0.5, 3.0, 18)\n",
    "\n",
    "# TLS narrow window around each BLS peak\n",
    "TLS_WINDOW_FRAC  = 0.01                # ±1% around each BLS peak\n",
    "TLS_THREADS      = max(1, (os.cpu_count() or 1))   # use all cores\n",
    "TLS_MIN_TRANSITS = 2                   # require ≥ 2 transits (faster/stricter)\n",
    "\n",
    "FIGDIR = \"figures\"; RESDIR = \"results\"\n",
    "os.makedirs(FIGDIR, exist_ok=True)\n",
    "os.makedirs(RESDIR, exist_ok=True)\n",
    "\n",
    "# If you know star params (from TIC): helps TLS grid a bit\n",
    "R_STAR = 0.55   # R_sun\n",
    "M_STAR = 0.55   # M_sun\n",
    "\n",
    "# ---- Helpers ----\n",
    "def load_pdcsap_sector(tic, sector):\n",
    "    \"\"\"Prefer modern search_lightcurve(..., author='SPOC'); fallback to search_lightcurvefile().\"\"\"\n",
    "    try:\n",
    "        sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", sector=sector, author=\"SPOC\")\n",
    "        if len(sr) == 0:\n",
    "            raise RuntimeError(\"No SPOC PDCSAP LC via search_lightcurve\")\n",
    "        lc = sr.download().remove_nans()\n",
    "        return lc\n",
    "    except Exception:\n",
    "        sr2 = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", sector=sector)\n",
    "        lcf = sr2.download()\n",
    "        lc = lcf.PDCSAP_FLUX.remove_nans()\n",
    "        return lc\n",
    "\n",
    "def lc_to_arrays(lc):\n",
    "    \"\"\"Return (t,f) float arrays, normalized by median; robust to masked arrays/NaNs.\"\"\"\n",
    "    t = getattr(lc.time, \"value\", lc.time)\n",
    "    f = getattr(lc.flux, \"value\", lc.flux)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    f = np.asarray(f, dtype=float)\n",
    "    if np.ma.isMaskedArray(t): t = t.filled(np.nan)\n",
    "    if np.ma.isMaskedArray(f): f = f.filled(np.nan)\n",
    "    m = np.isfinite(t) & np.isfinite(f)\n",
    "    f_med = np.nanmedian(f[m]) if np.any(m) else 1.0\n",
    "    if not np.isfinite(f_med) or f_med == 0: f_med = 1.0\n",
    "    return t[m], (f[m]/f_med)\n",
    "\n",
    "def unique_peaks(periods, power, k=3, tol_frac=0.01):\n",
    "    \"\"\"Pick top-k unique periods (avoid near-duplicates within tol_frac).\"\"\"\n",
    "    idx = np.argsort(power)[::-1]\n",
    "    picks = []\n",
    "    for i in idx:\n",
    "        p = periods[i]\n",
    "        if all(abs(p - q)/q > tol_frac for q in picks):\n",
    "            picks.append(p)\n",
    "        if len(picks) == k:\n",
    "            break\n",
    "    return picks\n",
    "\n",
    "def plot_periodogram(x, y, xlabel, title, outpng):\n",
    "    plt.figure(figsize=(8,4), dpi=140)\n",
    "    plt.plot(x, y, lw=1)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Power\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "def fold_and_plot(t, f, period, t0, title, outpng, nbins=200):\n",
    "    phase = ((t - t0 + 0.5*period) % period) / period - 0.5\n",
    "    order = np.argsort(phase); phase, f = phase[order], f[order]\n",
    "    bins = np.linspace(-0.5, 0.5, nbins+1)\n",
    "    which = np.digitize(phase, bins) - 1\n",
    "    yb = np.array([np.nanmean(f[which==i]) if np.any(which==i) else np.nan for i in range(nbins)])\n",
    "    xb = 0.5*(bins[:-1]+bins[1:])\n",
    "    plt.figure(figsize=(8,4), dpi=140)\n",
    "    plt.plot(phase, f, \".\", ms=2, alpha=0.35)\n",
    "    plt.plot(xb, yb, \"-\", lw=1.5)\n",
    "    plt.axvline(0.0, color=\"k\", lw=1, alpha=0.3)\n",
    "    plt.xlabel(\"Phase (cycles)\"); plt.ylabel(\"Relative flux\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "def append_csv(path, rows, header=None):\n",
    "    new = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if new and header: w.writerow(header)\n",
    "        for r in rows: w.writerow(r)\n",
    "\n",
    "def bls_power_safe(t, f, periods, durations):\n",
    "    \"\"\"Run BLS, trying objective='snr' first; if not supported, fall back.\"\"\"\n",
    "    bls = BoxLeastSquares(t, f)\n",
    "    try:\n",
    "        res = bls.power(periods, durations, objective=\"snr\")\n",
    "    except TypeError:\n",
    "        res = bls.power(periods, durations)\n",
    "    return res\n",
    "\n",
    "def tls_narrow(t, f, p_center, frac=TLS_WINDOW_FRAC, nthreads=TLS_THREADS, nmin=TLS_MIN_TRANSITS):\n",
    "    \"\"\"\n",
    "    TLS around a single candidate period (±frac). Returns (period, SDE, T0, res).\n",
    "    NOTE: R_star and M_star must be passed to .power(...), not the constructor.\n",
    "    \"\"\"\n",
    "    tls = transitleastsquares(t, f)\n",
    "    pmin = p_center*(1-frac)\n",
    "    pmax = p_center*(1+frac)\n",
    "    if not np.isfinite(pmin) or not np.isfinite(pmax) or pmin <= 0 or pmax <= pmin:\n",
    "        pmin, pmax = max(0.5, p_center*0.98), p_center*1.02\n",
    "\n",
    "    # Try with all threads; if TLS complains, fall back to 1 thread.\n",
    "    try:\n",
    "        res = tls.power(\n",
    "            period_min=pmin, period_max=pmax,\n",
    "            show_progress_bar=True,\n",
    "            use_threads=int(nthreads),\n",
    "            n_transits_min=int(nmin),\n",
    "            R_star=R_STAR, M_star=M_STAR\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        if \"use_threads\" in str(e):\n",
    "            res = tls.power(\n",
    "                period_min=pmin, period_max=pmax,\n",
    "                show_progress_bar=True,\n",
    "                use_threads=1,\n",
    "                n_transits_min=int(nmin),\n",
    "                R_star=R_STAR, M_star=M_STAR\n",
    "            )\n",
    "        else:\n",
    "            # If window was too tight for a grid, widen it a bit\n",
    "            res = tls.power(\n",
    "                period_min=p_center*(1-2*frac), period_max=p_center*(1+2*frac),\n",
    "                show_progress_bar=True,\n",
    "                use_threads=int(nthreads),\n",
    "                n_transits_min=int(nmin),\n",
    "                R_star=R_STAR, M_star=M_STAR\n",
    "            )\n",
    "\n",
    "    return float(res.period), float(res.SDE), float(res.T0), res\n",
    "\n",
    "def run_block(label, t, f):\n",
    "    \"\"\"Run BLS wide (clamped to data span), then TLS narrow for top-3; save artifacts.\"\"\"\n",
    "    print(f\"\\n[{label}] points={t.size}  threads={TLS_THREADS}\")\n",
    "\n",
    "    # ---- BLS (wide, but clamp to data span for per-sector speed) ----\n",
    "    t0 = time.time()\n",
    "    span = float(np.nanmax(t) - np.nanmin(t))\n",
    "    bls_pmax = min(BLS_PERIOD_MAX, max(BLS_PERIOD_MIN*1.2, 0.90*span))\n",
    "    periods   = np.linspace(BLS_PERIOD_MIN, bls_pmax, BLS_NPER)\n",
    "    durations = BLS_DURATIONS_HR / 24.0\n",
    "    bls_res   = bls_power_safe(t, f, periods, durations)\n",
    "    print(f\"[{label}] BLS done in {time.time()-t0:.1f}s (Pmax used={bls_pmax:.2f} d)\")\n",
    "\n",
    "    plot_periodogram(\n",
    "        bls_res.period, bls_res.power,\n",
    "        \"Period (days)\", f\"{TARGET_TOI} ({label}) — BLS periodogram\",\n",
    "        f\"{FIGDIR}/TIC{TARGET_TIC}_{label}_BLS_periodogram.png\"\n",
    "    )\n",
    "\n",
    "    bls_topP = unique_peaks(bls_res.period, bls_res.power, k=3, tol_frac=0.01)\n",
    "    append_csv(\n",
    "        f\"{RESDIR}/TIC{TARGET_TIC}_{label}_BLS_top3.csv\",\n",
    "        [[TARGET_TIC, TARGET_TOI, label, float(p),\n",
    "          float(bls_res.power[np.argmin(abs(bls_res.period-p))])] for p in bls_topP],\n",
    "        header=[\"tic\",\"toi\",\"label\",\"period_days\",\"power\"]\n",
    "    )\n",
    "\n",
    "    # ---- TLS (narrow around each BLS peak) ----\n",
    "    tls_rows = []\n",
    "    for p in bls_topP:\n",
    "        print(f\"[{label}] TLS refine around {p:.5f} d (±{TLS_WINDOW_FRAC*100:.1f}%) …\")\n",
    "        t1 = time.time()\n",
    "        p_best, sde, t0_best, res = tls_narrow(t, f, p)\n",
    "        print(f\"[{label}]   TLS best P={p_best:.6f} d, SDE={sde:.2f} (took {time.time()-t1:.1f}s)\")\n",
    "        tls_rows.append([TARGET_TIC, TARGET_TOI, label, p_best, sde, t0_best])\n",
    "\n",
    "        # Save TLS periodogram and fold for this candidate\n",
    "        plot_periodogram(\n",
    "            res.periods, res.power,\n",
    "            \"Period (days)\", f\"{TARGET_TOI} ({label}) — TLS @ {p:.5f}±{TLS_WINDOW_FRAC*100:.1f}%\",\n",
    "            f\"{FIGDIR}/TIC{TARGET_TIC}_{label}_TLS_periodogram_around_{p:.5f}.png\"\n",
    "        )\n",
    "        fold_and_plot(\n",
    "            t, f, p_best, t0_best,\n",
    "            f\"{TARGET_TOI} ({label}) — TLS fold @ P={p_best:.5f} d\",\n",
    "            f\"{FIGDIR}/TIC{TARGET_TIC}_{label}_TLS_fold_P{p_best:.5f}.png\"\n",
    "        )\n",
    "\n",
    "    append_csv(\n",
    "        f\"{RESDIR}/TIC{TARGET_TIC}_{label}_TLS_top3.csv\",\n",
    "        tls_rows, header=[\"tic\",\"toi\",\"label\",\"period_days\",\"SDE\",\"T0_BTJD\"]\n",
    "    )\n",
    "\n",
    "# =============================== #\n",
    "# Run per-sector\n",
    "# =============================== #\n",
    "t_all_list, f_all_list = [], []\n",
    "for s in SECTORS:\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, s).normalize()\n",
    "    t, f = lc_to_arrays(lc)\n",
    "    print(f\"{TARGET_TOI} — S{s}: N={t.size}\")\n",
    "    run_block(f\"S{s}\", t, f)\n",
    "    t_all_list.append(t); f_all_list.append(f)\n",
    "\n",
    "# =============================== #\n",
    "# Run stitched (combined)\n",
    "# =============================== #\n",
    "t_all = np.concatenate(t_all_list); f_all = np.concatenate(f_all_list)\n",
    "order = np.argsort(t_all); t_all, f_all = t_all[order], f_all[order]\n",
    "run_block(\"stitched\", t_all, f_all)\n",
    "\n",
    "print(\"\\nDone: fast BLS→TLS complete (per-sector + stitched). Artifacts saved in figures/ and results/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3c99e2-95e3-4a44-b8c5-42b41af8a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "# ------------ Config ------------\n",
    "TARGET_TIC = 119584412\n",
    "TLS_THREADS = max(1, os.cpu_count() or 1)   # use all available cores\n",
    "TLS_WINDOW_FRAC = 0.01                      # ±1% around a candidate period\n",
    "TLS_MIN_TRANSITS = 2\n",
    "\n",
    "# Rough stellar params (OK for TLS grid spacing)\n",
    "R_STAR = 0.55    # ~Rsun (from your Target A sheet)\n",
    "M_STAR = 0.55    # ~Msun\n",
    "\n",
    "# Quiet some noisy warnings\n",
    "warnings.filterwarnings(\"ignore\", category=lk.LightkurveWarning)\n",
    "\n",
    "# ------------ Helpers ------------\n",
    "def load_pdcsap_sector(tic, sector):\n",
    "    \"\"\"Download PDCSAP for one sector and return a LightCurve (NaNs removed, normalized).\"\"\"\n",
    "    sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", sector=sector, exptime=120)\n",
    "    if len(sr) == 0:\n",
    "        raise RuntimeError(f\"No TESS LC found for TIC {tic} sector {sector}\")\n",
    "    lcf = sr.download(quality_bitmask=\"hard\")\n",
    "    lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "    return lc\n",
    "\n",
    "def lc_to_arrays(lc):\n",
    "    \"\"\"Convert LightCurve to clean float arrays (t, f), handling masked arrays.\"\"\"\n",
    "    t = np.asarray(getattr(lc.time, \"value\", lc.time), dtype=float)\n",
    "    f = np.asarray(getattr(lc.flux, \"value\", lc.flux))\n",
    "    if np.ma.isMaskedArray(f):\n",
    "        f = f.filled(np.nan)\n",
    "    good = np.isfinite(t) & np.isfinite(f)\n",
    "    return t[good], f[good]\n",
    "\n",
    "def tls_narrow(t, f, p_center, frac=TLS_WINDOW_FRAC,\n",
    "               nthreads=TLS_THREADS, nmin=TLS_MIN_TRANSITS):\n",
    "    \"\"\"Run TLS in a narrow window around p_center (±frac). Returns best (P, SDE, T0, res).\"\"\"\n",
    "    tls = transitleastsquares(t, f)\n",
    "    res = tls.power(\n",
    "        period_min = p_center*(1 - frac),\n",
    "        period_max = p_center*(1 + frac),\n",
    "        use_threads = int(max(1, nthreads)),   # TLS requires >=1\n",
    "        show_progress_bar = False,\n",
    "        R_star = R_STAR,\n",
    "        M_star = M_STAR,\n",
    "        n_transits_min = int(nmin),\n",
    "    )\n",
    "    return float(res.period), float(res.SDE), float(res.T0), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a213b63-3b01-4203-9311-73ca34134751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 16102 data points, 2414 periods from 0.601 to 13.29 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 13.26054 days\n",
      "TLS best: P=13.26054 d | SDE=9.34 | T0=1910.10076\n"
     ]
    }
   ],
   "source": [
    "lc = load_pdcsap_sector(TARGET_TIC, 22)  # Sector 22\n",
    "t, f = lc_to_arrays(lc)\n",
    "\n",
    "p_best, sde, t0, _ = tls_narrow(t, f, p_center=16.05)  # try around ~16 d\n",
    "print(f\"TLS best: P={p_best:.5f} d | SDE={sde:.2f} | T0={t0:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8d135d2-93a1-4954-bd55-dd3423cf3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 38 durations\n",
      "Searching 16102 data points, 2414 periods from 0.601 to 13.29 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 13.26054 days\n",
      "S22 (nmin=1): P=13.26054 d, SDE=9.34\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 13272 data points, 2125 periods from 0.601 to 11.942 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 10.93615 days\n",
      "S49 (nmin=1): P=10.93615 d, SDE=7.15\n"
     ]
    }
   ],
   "source": [
    "# S22\n",
    "lc = load_pdcsap_sector(TARGET_TIC, 22); t22,f22 = lc_to_arrays(lc)\n",
    "p22, sde22, t0_22, _ = tls_narrow(t22, f22, p_center=16.06, nmin=1)\n",
    "print(f\"S22 (nmin=1): P={p22:.5f} d, SDE={sde22:.2f}\")\n",
    "\n",
    "# S49\n",
    "lc = load_pdcsap_sector(TARGET_TIC, 49); t49,f49 = lc_to_arrays(lc)\n",
    "p49, sde49, t0_49, _ = tls_narrow(t49, f49, p_center=16.06, nmin=1)\n",
    "print(f\"S49 (nmin=1): P={p49:.5f} d, SDE={sde49:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34f70c33-0701-4ce9-b8f9-658dc98492ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned TIC119584412_S22_BLS_top3.csv: kept 3\n",
      "Cleaned TIC119584412_S22_TLS_top3.csv: kept 1\n",
      "Cleaned TIC119584412_S49_BLS_top3.csv: kept 3\n",
      "Cleaned TIC119584412_S49_TLS_top3.csv: kept 1\n",
      "Cleaned TIC119584412_stitched_BLS_top3.csv: kept 3\n",
      "Cleaned TIC119584412_stitched_TLS_top3.csv: kept 3\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def clean_top3_csv(p: Path):\n",
    "    with p.open() as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "    if not rows:\n",
    "        print(f\"Skipped {p.name}: empty\")\n",
    "        return\n",
    "\n",
    "    # Pick which metric to sort by\n",
    "    metric = \"power\" if \"_BLS_\" in p.name else \"SDE\"\n",
    "\n",
    "    # Keep the best row per unique period (rounded to 5 decimals)\n",
    "    best_by_period = {}\n",
    "    for r in rows:\n",
    "        try:\n",
    "            per = round(float(r[\"period_days\"]), 5)\n",
    "            val = float(r[metric])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if per not in best_by_period or val > float(best_by_period[per][metric]):\n",
    "            best_by_period[per] = r\n",
    "\n",
    "    # Take the top-3 by metric\n",
    "    cleaned = sorted(best_by_period.values(),\n",
    "                     key=lambda r: float(r[metric]),\n",
    "                     reverse=True)[:3]\n",
    "\n",
    "    # Overwrite the CSV with the cleaned rows\n",
    "    with p.open(\"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cleaned[0].keys())\n",
    "        w.writeheader()\n",
    "        w.writerows(cleaned)\n",
    "    print(f\"Cleaned {p.name}: kept {len(cleaned)}\")\n",
    "\n",
    "# Brace expansion doesn’t work in pathlib; glob each pattern separately\n",
    "paths = []\n",
    "paths += list(Path(\"results\").glob(\"TIC119584412_*_BLS_top3.csv\"))\n",
    "paths += list(Path(\"results\").glob(\"TIC119584412_*_TLS_top3.csv\"))\n",
    "\n",
    "if not paths:\n",
    "    print(\"No matching files found in results/.\")\n",
    "else:\n",
    "    for p in sorted(paths):\n",
    "        clean_top3_csv(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8b6865-9a8f-4dfc-8fbe-f6ade2ec65cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append_csv is now overwrite-mode for this session.\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "\n",
    "def append_csv(path, rows, header=None):\n",
    "    \"\"\"Overwrite version: replaces any existing file instead of appending.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if header:\n",
    "            w.writerow(header)\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "    print(f\"[overwrite] wrote {path} ({len(rows)} rows)\")\n",
    "\n",
    "print(\"append_csv is now overwrite-mode for this session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4170fb-98b0-4734-be75-4a4372015458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tess-ephem)",
   "language": "python",
   "name": "tess-ephem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
