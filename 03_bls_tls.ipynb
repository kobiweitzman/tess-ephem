{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26da43f7-c5a2-4890-87dd-11dc1f2ca4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/kobi.weitzman/Documents/tess-ephem\n",
      "summary path: results/TIC37749396_download_clean_summary.json | exists: True\n",
      "JSON keys: ['cdpp1h_flat_ppm', 'flatten_window_days', 'n_points_flat', 'n_points_raw', 'n_sectors_ffi', 'n_sectors_pdcsap', 'notes', 'rms_flat_ppm', 'rms_raw_ppm', 'sectors_ffi', 'sectors_pdcsap', 'target_tic', 'target_toi']\n"
     ]
    }
   ],
   "source": [
    "# --- Quick sanity check for local summary + keys ---\n",
    "from pathlib import Path\n",
    "import os, json, re\n",
    "\n",
    "tic = 37749396\n",
    "print(\"cwd:\", os.getcwd())\n",
    "p = Path(f\"results/TIC{tic}_download_clean_summary.json\")\n",
    "print(\"summary path:\", p, \"| exists:\", p.exists())\n",
    "\n",
    "if p.exists():\n",
    "    data = json.loads(p.read_text())\n",
    "    print(\"JSON keys:\", sorted(data.keys()))\n",
    "    for key in (\"sectors_used\",\"pdcsap_sectors\",\"sectors\",\"sector_list\"):\n",
    "        if key in data:\n",
    "            print(f\"{key} =\", data[key])\n",
    "else:\n",
    "    print(\"No local summary JSON found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7469730e-6963-4010-8dc8-d37695cb2891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIC 37749396 — PDCSAP sectors: [3, 42, 70] | FFI sectors: []\n",
      "Using SECTORS = [3, 42, 70]\n"
     ]
    }
   ],
   "source": [
    "# --- Target B: pull sectors from existing download summary ---\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "TARGET_TIC = 37749396\n",
    "TARGET_TOI = \"Target B\"  # replace with the real TOI label if you have it\n",
    "\n",
    "data = json.loads(Path(f\"results/TIC{TARGET_TIC}_download_clean_summary.json\").read_text())\n",
    "\n",
    "SECTORS_PDCSAP = sorted({int(s) for s in (data.get(\"sectors_pdcsap\") or [])})\n",
    "SECTORS_FFI    = sorted({int(s) for s in (data.get(\"sectors_ffi\") or [])})\n",
    "SECTORS        = SECTORS_PDCSAP if SECTORS_PDCSAP else SECTORS_FFI\n",
    "\n",
    "print(f\"TIC {TARGET_TIC} — PDCSAP sectors: {SECTORS_PDCSAP} | FFI sectors: {SECTORS_FFI}\")\n",
    "print(f\"Using SECTORS = {SECTORS}\")\n",
    "\n",
    "# Fallback: if still empty, try a quick MAST query (only if needed)\n",
    "if not SECTORS:\n",
    "    import lightkurve as lk\n",
    "    print(\"No sectors in summary; trying MAST quickly …\")\n",
    "    sr = lk.search_lightcurve(f\"TIC {TARGET_TIC}\", mission=\"TESS\")\n",
    "    SECTORS = sorted({int(r.sector) for r in sr if getattr(r, \"sector\", None) is not None})\n",
    "    print(\"MAST sectors:\", SECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11137b01-3cbd-4556-ad56-81b741f43864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lightkurve loader for PDCSAP by sector (with a safe fallback) ---\n",
    "import warnings\n",
    "import lightkurve as lk\n",
    "\n",
    "# Quiet some noisy warnings across versions\n",
    "try:\n",
    "    from lightkurve.utils import LightkurveWarning\n",
    "except Exception:\n",
    "    class LightkurveWarning(Warning): ...\n",
    "warnings.filterwarnings(\"ignore\", category=LightkurveWarning)\n",
    "\n",
    "def load_pdcsap_sector(tic, sector):\n",
    "    \"\"\"\n",
    "    Return a LightCurve with PDCSAP flux for a given TIC and TESS sector.\n",
    "    Tries the modern search_lightcurve path first; falls back to LightCurveFile.\n",
    "    \"\"\"\n",
    "    # 1) Preferred path: search_lightcurve(..., author='SPOC')\n",
    "    try:\n",
    "        sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "        if len(sr) == 0:\n",
    "            raise RuntimeError(\"No SPOC LC via search_lightcurve\")\n",
    "        # flux_column works on newer Lightkurve; if not, .download() defaults to PDCSAP for SPOC\n",
    "        try:\n",
    "            lc = sr.download(flux_column=\"pdcsap_flux\")\n",
    "        except Exception:\n",
    "            lc = sr.download()\n",
    "        return lc.remove_nans()\n",
    "    except Exception:\n",
    "        # 2) Fallback: LightCurveFile -> PDCSAP_FLUX attribute\n",
    "        sr2 = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", sector=sector)\n",
    "        lcf = sr2.download()\n",
    "        try:\n",
    "            lc = lcf.PDCSAP_FLUX\n",
    "        except Exception:\n",
    "            # last resort (shouldn't happen for SPOC sectors): use SAP\n",
    "            lc = lcf.SAP_FLUX\n",
    "        return lc.remove_nans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e92f496-3092-460b-b8ed-0af0df18c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIC 37749396 S3: loaded PDCSAP, N=12978\n",
      "TIC 37749396 S42: loaded PDCSAP, N=11473\n",
      "TIC 37749396 S70: loaded PDCSAP, N=86180\n"
     ]
    }
   ],
   "source": [
    "for s in SECTORS:\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, s)\n",
    "    n = len(getattr(lc.time, \"value\", lc.time))\n",
    "    print(f\"TIC {TARGET_TIC} S{s}: loaded PDCSAP, N={n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3675b366-d4d6-4b57-899d-058dfa96e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BLS→TLS helpers & config (paste once per fresh notebook) =================\n",
    "import os, csv, time, warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "# Quiet Lightkurve warnings across versions\n",
    "try:\n",
    "    from lightkurve.utils import LightkurveWarning\n",
    "except Exception:\n",
    "    class LightkurveWarning(Warning): ...\n",
    "warnings.filterwarnings(\"ignore\", category=LightkurveWarning)\n",
    "\n",
    "# ---- Config (keep modest to stay fast) ----\n",
    "BLS_PERIOD_MIN = 0.5\n",
    "BLS_PERIOD_MAX = 50.0\n",
    "BLS_NPER       = 5000                 # BLS grid size\n",
    "BLS_DURATIONS_HR = np.linspace(0.5, 3.0, 18)  # candidate durations for BLS\n",
    "\n",
    "TLS_WINDOW_FRAC  = 0.01               # ±1% TLS window around each BLS peak\n",
    "TLS_THREADS      = max(1, (os.cpu_count() or 1))  # use all cores\n",
    "TLS_MIN_TRANSITS = 2                  # be a bit strict to speed up\n",
    "\n",
    "FIGDIR = \"figures\"; RESDIR = \"results\"\n",
    "os.makedirs(FIGDIR, exist_ok=True)\n",
    "os.makedirs(RESDIR, exist_ok=True)\n",
    "\n",
    "# If you know star params, set them here; 1.0/1.0 is fine if unknown.\n",
    "R_STAR = 1.0   # R_sun\n",
    "M_STAR = 1.0   # M_sun\n",
    "\n",
    "# ---- Small utility helpers ----\n",
    "def lc_to_arrays(lc):\n",
    "    \"\"\"Return (t,f) float arrays, normalized by median; robust to masked arrays/NaNs.\"\"\"\n",
    "    t = getattr(lc.time, \"value\", lc.time)\n",
    "    f = getattr(lc.flux, \"value\", lc.flux)\n",
    "    t = np.asarray(t, dtype=float)\n",
    "    f = np.asarray(f, dtype=float)\n",
    "    if np.ma.isMaskedArray(t): t = t.filled(np.nan)\n",
    "    if np.ma.isMaskedArray(f): f = f.filled(np.nan)\n",
    "    m = np.isfinite(t) & np.isfinite(f)\n",
    "    f_med = np.nanmedian(f[m]) if np.any(m) else 1.0\n",
    "    if not np.isfinite(f_med) or f_med == 0: f_med = 1.0\n",
    "    return t[m], (f[m]/f_med)\n",
    "\n",
    "def unique_peaks(periods, power, k=3, tol_frac=0.01):\n",
    "    \"\"\"Pick top-k unique periods (avoid near-duplicates within tol_frac).\"\"\"\n",
    "    idx = np.argsort(power)[::-1]\n",
    "    picks = []\n",
    "    for i in idx:\n",
    "        p = float(periods[i])\n",
    "        if all(abs(p - q)/q > tol_frac for q in picks if q != 0):\n",
    "            picks.append(p)\n",
    "        if len(picks) == k:\n",
    "            break\n",
    "    return picks\n",
    "\n",
    "def plot_periodogram(x, y, xlabel, title, outpng):\n",
    "    plt.figure(figsize=(8,4), dpi=140)\n",
    "    plt.plot(x, y, lw=1)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Power\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "def fold_and_plot(t, f, period, t0, title, outpng, nbins=200):\n",
    "    phase = ((t - t0 + 0.5*period) % period) / period - 0.5\n",
    "    order = np.argsort(phase); phase, f = phase[order], f[order]\n",
    "    bins = np.linspace(-0.5, 0.5, nbins+1)\n",
    "    which = np.digitize(phase, bins) - 1\n",
    "    yb = np.array([np.nanmean(f[which==i]) if np.any(which==i) else np.nan for i in range(nbins)])\n",
    "    xb = 0.5*(bins[:-1]+bins[1:])\n",
    "    plt.figure(figsize=(8,4), dpi=140)\n",
    "    plt.plot(phase, f, \".\", ms=2, alpha=0.35)\n",
    "    plt.plot(xb, yb, \"-\", lw=1.5)\n",
    "    plt.axvline(0.0, color=\"k\", lw=1, alpha=0.3)\n",
    "    plt.xlabel(\"Phase (cycles)\"); plt.ylabel(\"Relative flux\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "def append_csv(path, rows, header=None):\n",
    "    new = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if new and header: w.writerow(header)\n",
    "        for r in rows: w.writerow(r)\n",
    "\n",
    "def bls_power_safe(t, f, periods, durations):\n",
    "    \"\"\"Run BLS, trying objective='snr' first; if not supported, fall back.\"\"\"\n",
    "    bls = BoxLeastSquares(t, f)\n",
    "    try:\n",
    "        res = bls.power(periods, durations, objective=\"snr\")\n",
    "    except TypeError:\n",
    "        res = bls.power(periods, durations)\n",
    "    return res\n",
    "\n",
    "def tls_narrow(t, f, p_center, frac=TLS_WINDOW_FRAC, nthreads=TLS_THREADS, nmin=TLS_MIN_TRANSITS):\n",
    "    \"\"\"\n",
    "    TLS around a single candidate period (±frac). Returns (period, SDE, T0, res).\n",
    "    NOTE: R_star and M_star must be passed to .power(...), not the constructor.\n",
    "    \"\"\"\n",
    "    tls = transitleastsquares(t, f)\n",
    "    pmin = p_center*(1-frac)\n",
    "    pmax = p_center*(1+frac)\n",
    "    if not np.isfinite(pmin) or not np.isfinite(pmax) or pmin <= 0 or pmax <= pmin:\n",
    "        pmin, pmax = max(0.5, p_center*0.98), p_center*1.02\n",
    "\n",
    "    # Try with all threads; if TLS complains, fall back to 1 thread.\n",
    "    try:\n",
    "        res = tls.power(\n",
    "            period_min=pmin, period_max=pmax,\n",
    "            show_progress_bar=False,\n",
    "            use_threads=int(nthreads),\n",
    "            n_transits_min=int(nmin),\n",
    "            R_star=R_STAR, M_star=M_STAR\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        if \"use_threads\" in str(e):\n",
    "            res = tls.power(\n",
    "                period_min=pmin, period_max=pmax,\n",
    "                show_progress_bar=False,\n",
    "                use_threads=1,\n",
    "                n_transits_min=int(nmin),\n",
    "                R_star=R_STAR, M_star=M_STAR\n",
    "            )\n",
    "        else:\n",
    "            res = tls.power(\n",
    "                period_min=p_center*(1-2*frac), period_max=p_center*(1+2*frac),\n",
    "                show_progress_bar=False,\n",
    "                use_threads=int(nthreads),\n",
    "                n_transits_min=int(nmin),\n",
    "                R_star=R_STAR, M_star=M_STAR\n",
    "            )\n",
    "    return float(res.period), float(res.SDE), float(res.T0), res\n",
    "\n",
    "def run_block(label, t, f, target_tic, target_toi):\n",
    "    \"\"\"Run BLS wide (clamped to data span), then TLS narrow for top-3; save artifacts.\"\"\"\n",
    "    print(f\"\\n[{label}] points={t.size}  threads={TLS_THREADS}\")\n",
    "\n",
    "    # ---- BLS (wide, but clamp to data span for per-sector speed) ----\n",
    "    t0 = time.time()\n",
    "    span = float(np.nanmax(t) - np.nanmin(t))\n",
    "    bls_pmax = min(BLS_PERIOD_MAX, max(BLS_PERIOD_MIN*1.2, 0.90*span))\n",
    "    periods   = np.linspace(BLS_PERIOD_MIN, bls_pmax, BLS_NPER)\n",
    "    durations = BLS_DURATIONS_HR / 24.0\n",
    "    bls_res   = bls_power_safe(t, f, periods, durations)\n",
    "    print(f\"[{label}] BLS done in {time.time()-t0:.1f}s (Pmax used={bls_pmax:.2f} d)\")\n",
    "\n",
    "    plot_periodogram(\n",
    "        bls_res.period, bls_res.power,\n",
    "        \"Period (days)\", f\"{target_toi} ({label}) — BLS periodogram\",\n",
    "        f\"{FIGDIR}/TIC{target_tic}_{label}_BLS_periodogram.png\"\n",
    "    )\n",
    "\n",
    "    bls_topP = unique_peaks(bls_res.period, bls_res.power, k=3, tol_frac=0.01)\n",
    "    append_csv(\n",
    "        f\"{RESDIR}/TIC{target_tic}_{label}_BLS_top3.csv\",\n",
    "        [[target_tic, target_toi, label, float(p),\n",
    "          float(bls_res.power[np.argmin(abs(bls_res.period-p))])] for p in bls_topP],\n",
    "        header=[\"tic\",\"toi\",\"label\",\"period_days\",\"power\"]\n",
    "    )\n",
    "\n",
    "    # ---- TLS (narrow around each BLS peak) ----\n",
    "    tls_rows = []\n",
    "    for p in bls_topP:\n",
    "        print(f\"[{label}] TLS refine around {p:.5f} d (±{TLS_WINDOW_FRAC*100:.1f}%) …\")\n",
    "        t1 = time.time()\n",
    "        p_best, sde, t0_best, res = tls_narrow(t, f, p)\n",
    "        print(f\"[{label}]   TLS best P={p_best:.6f} d, SDE={sde:.2f} (took {time.time()-t1:.1f}s)\")\n",
    "        tls_rows.append([target_tic, target_toi, label, p_best, sde, t0_best])\n",
    "\n",
    "        # Save TLS periodogram and fold for this candidate\n",
    "        plot_periodogram(\n",
    "            res.periods, res.power,\n",
    "            \"Period (days)\", f\"{target_toi} ({label}) — TLS @ {p:.5f}±{TLS_WINDOW_FRAC*100:.1f}%\",\n",
    "            f\"{FIGDIR}/TIC{target_tic}_{label}_TLS_periodogram_around_{p:.5f}.png\"\n",
    "        )\n",
    "        fold_and_plot(\n",
    "            t, f, p_best, t0_best,\n",
    "            f\"{target_toi} ({label}) — TLS fold @ P={p_best:.5f} d\",\n",
    "            f\"{FIGDIR}/TIC{target_tic}_{label}_TLS_fold_P{p_best:.5f}.png\"\n",
    "        )\n",
    "\n",
    "    append_csv(\n",
    "        f\"{RESDIR}/TIC{target_tic}_{label}_TLS_top3.csv\",\n",
    "        tls_rows, header=[\"tic\",\"toi\",\"label\",\"period_days\",\"SDE\",\"T0_BTJD\"]\n",
    "    )\n",
    "# ============================================================================ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ebc0502-4a99-429f-98ed-788a4f8b4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target B — S3: N=12978\n",
      "\n",
      "[S3] points=12978  threads=8\n",
      "[S3] BLS done in 1.7s (Pmax used=18.25 d)\n",
      "[S3] TLS refine around 6.73492 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12978 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 2.62263 days\n",
      "[S3]   TLS best P=2.622627 d, SDE=7.68 (took 7.7s)\n",
      "[S3] TLS refine around 6.56093 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12978 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 2.62263 days\n",
      "[S3]   TLS best P=2.622627 d, SDE=7.68 (took 6.8s)\n",
      "[S3] TLS refine around 18.24963 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12978 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 2.62263 days\n",
      "[S3]   TLS best P=2.622627 d, SDE=7.68 (took 6.3s)\n",
      "Target B — S42: N=11473\n",
      "\n",
      "[S42] points=11473  threads=8\n",
      "[S42] BLS done in 1.8s (Pmax used=21.03 d)\n",
      "[S42] TLS refine around 16.55722 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 11473 data points, 2070 periods from 0.601 to 11.683 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 4.28951 days\n",
      "[S42]   TLS best P=4.289513 d, SDE=5.40 (took 7.4s)\n",
      "[S42] TLS refine around 7.01734 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 11473 data points, 2070 periods from 0.601 to 11.683 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 4.28951 days\n",
      "[S42]   TLS best P=4.289513 d, SDE=5.40 (took 6.9s)\n",
      "[S42] TLS refine around 14.03160 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 11473 data points, 2070 periods from 0.601 to 11.683 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 4.28951 days\n",
      "[S42]   TLS best P=4.289513 d, SDE=5.40 (took 6.9s)\n",
      "Target B — S70: N=86180\n",
      "\n",
      "[S70] points=86180  threads=8\n",
      "[S70] BLS done in 3.4s (Pmax used=21.99 d)\n",
      "[S70] TLS refine around 13.47196 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 86180 data points, 2183 periods from 0.602 to 12.215 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 6.45607 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 1 of 3 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S70]   TLS best P=6.456068 d, SDE=9.62 (took 36.8s)\n",
      "[S70] TLS refine around 10.69103 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 86180 data points, 2183 periods from 0.602 to 12.215 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 6.45607 days\n",
      "[S70]   TLS best P=6.456068 d, SDE=9.62 (took 49.2s)\n",
      "[S70] TLS refine around 10.82427 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 86180 data points, 2183 periods from 0.602 to 12.215 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 6.45607 days\n",
      "[S70]   TLS best P=6.456068 d, SDE=9.62 (took 40.4s)\n",
      "\n",
      "[stitched] points=110631  threads=8\n",
      "[stitched] BLS done in 6.0s (Pmax used=50.00 d)\n",
      "[stitched] TLS refine around 13.47159 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110631 data points, 616 periods from 13.337 to 13.606 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 13.47572 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 132 of 137 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=13.475725 d, SDE=6.63 (took 28.4s)\n",
      "[stitched] TLS refine around 15.67974 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110631 data points, 585 periods from 15.523 to 15.836 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 15.83220 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 115 of 116 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=15.832202 d, SDE=3.72 (took 24.7s)\n",
      "[stitched] TLS refine around 10.68914 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110631 data points, 665 periods from 10.583 to 10.796 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 10.78088 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 168 of 171 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=10.780883 d, SDE=3.33 (took 31.1s)\n",
      "\n",
      "Done for Target B.\n"
     ]
    }
   ],
   "source": [
    "# --- Run per-sector and stitched on Target B (fixed calls) ---\n",
    "import numpy as np\n",
    "\n",
    "assert SECTORS, \"No sectors to run on — SECTORS is empty.\"\n",
    "\n",
    "t_all_list, f_all_list = [], []\n",
    "for s in SECTORS:\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, s).normalize()\n",
    "    t, f = lc_to_arrays(lc)\n",
    "    print(f\"{TARGET_TOI} — S{s}: N={t.size}\")\n",
    "    # pass TIC/TOI into run_block:\n",
    "    run_block(f\"S{s}\", t, f, TARGET_TIC, TARGET_TOI)\n",
    "    t_all_list.append(t); f_all_list.append(f)\n",
    "\n",
    "# stitched\n",
    "t_all = np.concatenate(t_all_list); f_all = np.concatenate(f_all_list)\n",
    "order = np.argsort(t_all); t_all, f_all = t_all[order], f_all[order]\n",
    "run_block(\"stitched\", t_all, f_all, TARGET_TIC, TARGET_TOI)\n",
    "\n",
    "print(\"\\nDone for Target B.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c399631e-cc53-4bf5-9568-9b3afeaf0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_periodogram(x, y, xlabel, title, outpng,\n",
    "                     clip_q=PLOT_CLIP_Q, smooth_win=SMOOTH_WIN, mark_artifacts=True):\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    yclip = np.copy(y)\n",
    "    ymax = np.nanpercentile(y, clip_q) if np.isfinite(y).any() else np.nan\n",
    "    if np.isfinite(ymax):\n",
    "        yclip = np.minimum(yclip, ymax)\n",
    "\n",
    "    plt.figure(figsize=(8, 4), dpi=140)\n",
    "    plt.plot(x, yclip, lw=0.7, color=\"0.6\", label=\"power (clipped)\")\n",
    "    ys = smooth1d(yclip, smooth_win)\n",
    "    if ys is not None:                         # ← fixed condition\n",
    "        plt.plot(x, ys, lw=1.5, label=f\"smoothed (w={smooth_win})\")\n",
    "\n",
    "    if mark_artifacts:\n",
    "        for p in ARTIFACT_PERIODS:\n",
    "            if np.nanmin(x) < p < np.nanmax(x):\n",
    "                plt.axvline(p, color=\"k\", lw=1, alpha=0.15)\n",
    "\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Power\"); plt.title(title)\n",
    "    plt.legend(loc=\"upper right\", fontsize=8, framealpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c629a31-47a4-4186-b442-3db943bdaeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target B — S3: N=12977\n",
      "\n",
      "[S3] points=12977  threads=8\n",
      "[S3] BLS done in 1.1s (Pmax used=9.63 d)\n",
      "[S3] with nmin=3, periods ≲ 9.63 d have ≥3 transits\n",
      "[S3] TLS refine around 6.73640 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12977 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 6.73332 days\n",
      "[S3]   TLS best P=6.733319 d, SDE=7.24 (took 8.9s)\n",
      "[S3] TLS refine around 9.14036 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12977 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 6.73332 days\n",
      "[S3]   TLS best P=6.733319 d, SDE=7.24 (took 7.5s)\n",
      "[S3] TLS refine around 9.63175 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 36 durations\n",
      "Searching 12977 data points, 1745 periods from 0.602 to 10.139 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 6.73332 days\n",
      "[S3]   TLS best P=6.733319 d, SDE=7.24 (took 7.2s)\n",
      "Target B — S42: N=11469\n",
      "\n",
      "[S42] points=11469  threads=8\n",
      "[S42] BLS done in 1.3s (Pmax used=11.10 d)\n",
      "[S42] with nmin=3, periods ≲ 11.10 d have ≥3 transits\n",
      "[S42] TLS refine around 8.58219 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 11469 data points, 2070 periods from 0.601 to 11.683 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 8.57691 days\n",
      "[S42]   TLS best P=8.576914 d, SDE=7.77 (took 7.3s)\n",
      "[S42] TLS refine around 8.26204 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 11469 data points, 2070 periods from 0.601 to 11.683 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 8.57691 days\n",
      "[S42]   TLS best P=8.576914 d, SDE=7.77 (took 7.0s)\n",
      "[S42] TLS refine around 7.01537 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 11469 data points, 2070 periods from 0.601 to 11.683 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 8.57691 days\n",
      "[S42]   TLS best P=8.576914 d, SDE=7.77 (took 7.3s)\n",
      "Target B — S70: N=85945\n",
      "\n",
      "[S70] points=85945  threads=8\n",
      "[S70] BLS done in 2.8s (Pmax used=11.60 d)\n",
      "[S70] with nmin=3, periods ≲ 11.60 d have ≥3 transits\n",
      "[S70] TLS refine around 7.83016 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 85945 data points, 2183 periods from 0.602 to 12.215 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 3.36926 days\n",
      "[S70]   TLS best P=3.369264 d, SDE=7.76 (took 39.9s)\n",
      "[S70] TLS refine around 6.73730 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 85945 data points, 2183 periods from 0.602 to 12.215 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 3.36926 days\n",
      "[S70]   TLS best P=3.369264 d, SDE=7.76 (took 36.5s)\n",
      "[S70] TLS refine around 8.03452 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 85945 data points, 2183 periods from 0.602 to 12.215 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 3.36926 days\n",
      "[S70]   TLS best P=3.369264 d, SDE=7.76 (took 35.1s)\n",
      "\n",
      "[stitched] points=110391  threads=8\n",
      "[stitched] BLS done in 6.4s (Pmax used=50.00 d)\n",
      "[stitched] TLS refine around 13.47159 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110391 data points, 616 periods from 13.337 to 13.606 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 13.47572 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 132 of 137 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=13.475725 d, SDE=10.63 (took 26.0s)\n",
      "[stitched] TLS refine around 15.65993 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110391 data points, 586 periods from 15.504 to 15.816 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 15.52011 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 117 of 119 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=15.520105 d, SDE=5.53 (took 27.3s)\n",
      "[stitched] TLS refine around 36.38478 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110391 data points, 442 periods from 36.022 to 36.747 days\n",
      "Using all 8 CPU threads\n",
      "Searching for best T0 for period 36.38454 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 49 of 51 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=36.384540 d, SDE=3.12 (took 32.0s)\n",
      "\n",
      "Done for Target B with robust prep. Artifacts saved in figures/ and results/.\n"
     ]
    }
   ],
   "source": [
    "# ==== Run Target B (uses the new cleaning + prettier plots) ====\n",
    "import os, numpy as np\n",
    "\n",
    "# Config\n",
    "TARGET_TOI = \"Target B\"\n",
    "TARGET_TIC = 37749396\n",
    "SECTORS    = [3, 42, 70]       # from your earlier discovery\n",
    "\n",
    "# Ensure dirs/constants exist (safe defaults if missing)\n",
    "FIGDIR = \"figures\"; RESDIR = \"results\"\n",
    "os.makedirs(FIGDIR, exist_ok=True); os.makedirs(RESDIR, exist_ok=True)\n",
    "\n",
    "try: TLS_THREADS\n",
    "except NameError:\n",
    "    TLS_THREADS = max(1, (os.cpu_count() or 1))\n",
    "try: TLS_MIN_TRANSITS\n",
    "except NameError:\n",
    "    TLS_MIN_TRANSITS = 3        # stricter for single sectors\n",
    "try: BLS_PERIOD_MIN\n",
    "except NameError:\n",
    "    BLS_PERIOD_MIN = 0.5\n",
    "try: BLS_PERIOD_MAX\n",
    "except NameError:\n",
    "    BLS_PERIOD_MAX = 50.0\n",
    "try: BLS_NPER\n",
    "except NameError:\n",
    "    BLS_NPER = 5000\n",
    "try: BLS_DURATIONS_HR\n",
    "except NameError:\n",
    "    BLS_DURATIONS_HR = np.linspace(0.5, 3.0, 18)\n",
    "try: TLS_WINDOW_FRAC\n",
    "except NameError:\n",
    "    TLS_WINDOW_FRAC = 0.01\n",
    "\n",
    "# Make sure required helpers exist (define minimal loader if needed)\n",
    "try:\n",
    "    load_pdcsap_sector\n",
    "except NameError:\n",
    "    import lightkurve as lk\n",
    "    def load_pdcsap_sector(tic, sector):\n",
    "        sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", sector=sector, author=\"SPOC\")\n",
    "        if len(sr) == 0: raise RuntimeError(\"No SPOC PDCSAP LC\")\n",
    "        return sr.download().remove_nans()\n",
    "\n",
    "# Run per-sector with new cleaner\n",
    "t_all_list, f_all_list = [], []\n",
    "for s in SECTORS:\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, s)           # raw PDCSAP\n",
    "    t, f = prep_arrays_for_search(lc)                # << clean + detrend\n",
    "    print(f\"{TARGET_TOI} — S{s}: N={t.size}\")\n",
    "    run_block(f\"S{s}\", t, f, TARGET_TIC, TARGET_TOI) # saves figs/CSVs\n",
    "    t_all_list.append(t); f_all_list.append(f)\n",
    "\n",
    "# Stitched (combined)\n",
    "t_all = np.concatenate(t_all_list); f_all = np.concatenate(f_all_list)\n",
    "order = np.argsort(t_all); t_all, f_all = t_all[order], f_all[order]\n",
    "run_block(\"stitched\", t_all, f_all, TARGET_TIC, TARGET_TOI)\n",
    "\n",
    "print(\"\\nDone for Target B with robust prep. Artifacts saved in figures/ and results/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c698e1-4a0b-4a22-8f87-3ff6acb6d7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['figures/TIC37749396_S3_BLS_periodogram.png',\n",
       "  'figures/TIC37749396_S3_TLS_fold_P2.62263.png',\n",
       "  'figures/TIC37749396_S3_TLS_fold_P6.73332.png',\n",
       "  'figures/TIC37749396_S3_TLS_periodogram_around_18.24963.png',\n",
       "  'figures/TIC37749396_S3_TLS_periodogram_around_6.56093.png',\n",
       "  'figures/TIC37749396_S3_TLS_periodogram_around_6.73492.png',\n",
       "  'figures/TIC37749396_S3_TLS_periodogram_around_6.73640.png',\n",
       "  'figures/TIC37749396_S3_TLS_periodogram_around_9.14036.png',\n",
       "  'figures/TIC37749396_S3_TLS_periodogram_around_9.63175.png',\n",
       "  'figures/TIC37749396_S42_BLS_periodogram.png'],\n",
       " ['results/TIC37749396_S3_BLS_top3.csv',\n",
       "  'results/TIC37749396_S3_TLS_top3.csv',\n",
       "  'results/TIC37749396_S42_BLS_top3.csv',\n",
       "  'results/TIC37749396_S42_TLS_top3.csv',\n",
       "  'results/TIC37749396_S70_BLS_top3.csv',\n",
       "  'results/TIC37749396_S70_TLS_top3.csv',\n",
       "  'results/TIC37749396_download_clean_summary.json',\n",
       "  'results/TIC37749396_stitched_BLS_top3.csv',\n",
       "  'results/TIC37749396_stitched_TLS_top3.csv'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "sorted(glob.glob(\"figures/TIC37749396*\"))[:10], sorted(glob.glob(\"results/TIC37749396*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e428f3f8-058a-4dde-9080-e2fcd7f175a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Patch 1: star params + stricter TLS ===\n",
    "# rough TIC-based guesses are fine; refine later if you have better values\n",
    "R_STAR = 0.8   # R_sun\n",
    "M_STAR = 0.8   # M_sun\n",
    "\n",
    "# keep stricter to reduce spurious single-sector hits\n",
    "TLS_MIN_TRANSITS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21ab7165-28ca-42aa-a37c-51308c66d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Patch 2: artifact veto + improved TLS + run_block update ===\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# If your notebook already defined these, this safely reuses them.\n",
    "try:\n",
    "    bls_power_safe\n",
    "except NameError:\n",
    "    from astropy.timeseries import BoxLeastSquares\n",
    "    def bls_power_safe(t, f, periods, durations):\n",
    "        bls = BoxLeastSquares(t, f)\n",
    "        try:\n",
    "            return bls.power(periods, durations, objective=\"snr\")\n",
    "        except TypeError:\n",
    "            return bls.power(periods, durations)\n",
    "\n",
    "# obvious spacecraft/system periods to de-emphasize (±2%)\n",
    "ARTIFACT_PERIODS = (0.5, 1.0, 2.0, 6.85, 13.7, 27.4)\n",
    "def is_artifact_period(p, avoid=ARTIFACT_PERIODS, tol_frac=0.02):\n",
    "    return any(abs(p - a)/a < tol_frac for a in avoid)\n",
    "\n",
    "# Re-define TLS narrow window to pass R_star/M_star and handle threads cleanly\n",
    "from transitleastsquares import transitleastsquares\n",
    "def tls_narrow(t, f, p_center, frac=TLS_WINDOW_FRAC, nthreads=TLS_THREADS, nmin=TLS_MIN_TRANSITS):\n",
    "    tls = transitleastsquares(t, f)\n",
    "    pmin = max(0.5, p_center*(1-frac))\n",
    "    pmax = p_center*(1+frac)\n",
    "    try:\n",
    "        res = tls.power(period_min=pmin, period_max=pmax,\n",
    "                        show_progress_bar=True,\n",
    "                        use_threads=int(nthreads),\n",
    "                        n_transits_min=int(nmin),\n",
    "                        R_star=R_STAR, M_star=M_STAR)\n",
    "    except ValueError:\n",
    "        # fallback for TLS builds that reject use_threads<1, etc.\n",
    "        res = tls.power(period_min=pmin, period_max=pmax,\n",
    "                        show_progress_bar=True,\n",
    "                        use_threads=1,\n",
    "                        n_transits_min=int(nmin),\n",
    "                        R_star=R_STAR, M_star=M_STAR)\n",
    "    return float(res.period), float(res.SDE), float(res.T0), res\n",
    "\n",
    "# Re-define run_block to filter artifact periods before TLS\n",
    "def run_block(label, t, f, target_tic, target_toi):\n",
    "    \"\"\"BLS (capped) → TLS around top peaks (artifact-veto). Saves periodograms & folds.\"\"\"\n",
    "    print(f\"\\n[{label}] points={t.size}  threads={TLS_THREADS}\")\n",
    "\n",
    "    # stricter TLS for single sectors\n",
    "    nmin = 3 if label.startswith(\"S\") else TLS_MIN_TRANSITS\n",
    "\n",
    "    # Cap BLS period so single-sector has >= nmin transits\n",
    "    span = float(np.nanmax(t) - np.nanmin(t))\n",
    "    bls_cap = min(BLS_PERIOD_MAX, max(BLS_PERIOD_MIN*1.2, 0.90*span))\n",
    "    if label.startswith(\"S\") and nmin >= 2:\n",
    "        bls_cap = min(bls_cap, 0.95 * span / (nmin - 1))\n",
    "\n",
    "    periods   = np.linspace(BLS_PERIOD_MIN, bls_cap, BLS_NPER)\n",
    "    durations = BLS_DURATIONS_HR / 24.0\n",
    "\n",
    "    t0 = time.time()\n",
    "    bls_res = bls_power_safe(t, f, periods, durations)\n",
    "    print(f\"[{label}] BLS done in {time.time()-t0:.1f}s (Pmax used={bls_cap:.2f} d)\")\n",
    "    if label.startswith(\"S\") and nmin >= 3:\n",
    "        print(f\"[{label}] with nmin={nmin}, periods ≲ {0.95*span/(nmin-1):.2f} d have ≥{nmin} transits\")\n",
    "\n",
    "    # periodogram + save\n",
    "    plot_periodogram(\n",
    "        bls_res.period, bls_res.power,\n",
    "        \"Period (days)\", f\"{target_toi} ({label}) — BLS periodogram\",\n",
    "        f\"{FIGDIR}/TIC{target_tic}_{label}_BLS_periodogram.png\"\n",
    "    )\n",
    "\n",
    "    # top-3 unique, then drop obvious artifact periods\n",
    "    bls_topP = unique_peaks(bls_res.period, bls_res.power, k=3, tol_frac=0.01)\n",
    "    bls_topP = [p for p in bls_topP if not is_artifact_period(p)]\n",
    "    if not bls_topP:\n",
    "        print(f\"[{label}] all top BLS peaks fell on artifact periods; keeping strongest anyway\")\n",
    "        bls_topP = [float(bls_res.period[np.argmax(bls_res.power)])]\n",
    "\n",
    "    append_csv(\n",
    "        f\"{RESDIR}/TIC{target_tic}_{label}_BLS_top3.csv\",\n",
    "        [[target_tic, target_toi, label, float(p),\n",
    "          float(bls_res.power[np.argmin(np.abs(bls_res.period-p))])] for p in bls_topP],\n",
    "        header=[\"tic\",\"toi\",\"label\",\"period_days\",\"power\"]\n",
    "    )\n",
    "\n",
    "    # TLS refine around each remaining BLS peak\n",
    "    tls_rows = []\n",
    "    for p in bls_topP:\n",
    "        print(f\"[{label}] TLS refine around {p:.5f} d (±{TLS_WINDOW_FRAC*100:.1f}%) …\")\n",
    "        t1 = time.time()\n",
    "        p_best, sde, t0_best, res = tls_narrow(\n",
    "            t, f, p, frac=TLS_WINDOW_FRAC, nthreads=TLS_THREADS, nmin=nmin\n",
    "        )\n",
    "        print(f\"[{label}]   TLS best P={p_best:.6f} d, SDE={sde:.2f} (took {time.time()-t1:.1f}s)\")\n",
    "        tls_rows.append([target_tic, target_toi, label, p_best, sde, t0_best])\n",
    "\n",
    "        plot_periodogram(\n",
    "            res.periods, res.power,\n",
    "            \"Period (days)\", f\"{target_toi} ({label}) — TLS @ {p:.5f}±{TLS_WINDOW_FRAC*100:.1f}%\",\n",
    "            f\"{FIGDIR}/TIC{target_tic}_{label}_TLS_periodogram_around_{p:.5f}.png\"\n",
    "        )\n",
    "        fold_and_plot(\n",
    "            t, f, p_best, t0_best,\n",
    "            f\"{target_toi} ({label}) — TLS fold @ P={p_best:.5f} d\",\n",
    "            f\"{FIGDIR}/TIC{target_tic}_{label}_TLS_fold_P{p_best:.5f}.png\"\n",
    "        )\n",
    "\n",
    "    append_csv(\n",
    "        f\"{RESDIR}/TIC{target_tic}_{label}_TLS_top3.csv\",\n",
    "        tls_rows, header=[\"tic\",\"toi\",\"label\",\"period_days\",\"SDE\",\"T0_BTJD\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "211e6dca-e07c-4fb5-ae97-f90d23390bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[stitched] points=110391  threads=8\n",
      "[stitched] BLS done in 6.2s (Pmax used=50.00 d)\n",
      "[stitched] TLS refine around 15.65993 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110391 data points, 679 periods from 15.504 to 15.816 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 679/679 periods | 00:16<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 15.52014 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 13885/13885 [00:11<00:00, 1178.40it/s]\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 117 of 119 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=15.520138 d, SDE=5.93 (took 30.0s)\n",
      "[stitched] TLS refine around 36.38478 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 110391 data points, 514 periods from 36.021 to 36.748 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 514/514 periods | 00:12<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 36.42690 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 26793/26793 [00:19<00:00, 1347.51it/s]\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 49 of 51 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=36.426902 d, SDE=4.06 (took 33.5s)\n",
      "stitched re-run complete. Check figures/TIC37749396_stitched_*\n"
     ]
    }
   ],
   "source": [
    "# === Patch 3: stitched-only re-run (Target B) ===\n",
    "import numpy as np, lightkurve as lk\n",
    "\n",
    "TARGET_TOI = \"Target B\"\n",
    "TARGET_TIC = 37749396\n",
    "SECTORS    = [3, 42, 70]  # from your earlier discovery\n",
    "\n",
    "def load_pdcsap_sector(tic, sector):\n",
    "    sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", sector=sector, author=\"SPOC\")\n",
    "    if len(sr) == 0:\n",
    "        raise RuntimeError(f\"No SPOC PDCSAP for TIC {tic} sector {sector}\")\n",
    "    return sr.download().remove_nans()\n",
    "\n",
    "# minimal fallback in case prep_arrays_for_search isn't in this notebook cell\n",
    "def _to_arrays_basic(lc):\n",
    "    t = getattr(lc.time, \"value\", lc.time)\n",
    "    f = getattr(lc.flux, \"value\", lc.flux)\n",
    "    t = np.asarray(t, float); f = np.asarray(f, float)\n",
    "    m = np.isfinite(t) & np.isfinite(f)\n",
    "    f_med = np.nanmedian(f[m]) if np.any(m) else 1.0\n",
    "    return t[m], (f[m]/(f_med if f_med else 1.0))\n",
    "\n",
    "t_all_list, f_all_list = [], []\n",
    "for s in SECTORS:\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, s)\n",
    "    if \"prep_arrays_for_search\" in globals():\n",
    "        t, f = prep_arrays_for_search(lc)  # uses your robust clean/detrend\n",
    "    else:\n",
    "        t, f = _to_arrays_basic(lc.normalize())\n",
    "    t_all_list.append(t); f_all_list.append(f)\n",
    "\n",
    "t_all = np.concatenate(t_all_list); f_all = np.concatenate(f_all_list)\n",
    "order = np.argsort(t_all); t_all, f_all = t_all[order], f_all[order]\n",
    "\n",
    "run_block(\"stitched\", t_all, f_all, TARGET_TIC, TARGET_TOI)\n",
    "print(\"stitched re-run complete. Check figures/TIC37749396_stitched_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cff8de2b-554b-4c26-a322-43ee87bcdaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def clean_top3_csv(p):\n",
    "    rows = list(csv.DictReader(open(p)))\n",
    "    if not rows: \n",
    "        return\n",
    "    metric = \"power\" if \"BLS\" in p.name else \"SDE\"\n",
    "    best = {}\n",
    "    for r in rows:\n",
    "        try:\n",
    "            per = round(float(r[\"period_days\"]), 5)\n",
    "            val = float(r[metric])\n",
    "        except Exception:\n",
    "            continue\n",
    "        if per not in best or val > float(best[per][metric]):\n",
    "            best[per] = r\n",
    "    cleaned = sorted(best.values(), key=lambda r: float(r[metric]), reverse=True)[:3]\n",
    "    with open(p, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cleaned[0].keys())\n",
    "        w.writeheader(); w.writerows(cleaned)\n",
    "    print(\"Cleaned:\", p.name, \"kept\", len(cleaned))\n",
    "\n",
    "for p in Path(\"results\").glob(\"TIC37749396_*_{BLS,TLS}_top3.csv\"):\n",
    "    clean_top3_csv(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbc1ebc7-6f91-4173-8dad-818bd0194898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Target C → TOI='550.02'  TIC=311183180\n",
      "(Source: priority_targets.csv; skipping used TICs [37749396, 119584412])\n"
     ]
    }
   ],
   "source": [
    "# === Pick Target C from your list (robust column matching) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, re\n",
    "\n",
    "# Mark targets already used (A & B); add more here if needed\n",
    "USED_TICS = {119584412, 37749396}\n",
    "\n",
    "# Prefer priority list, fall back to ranked list\n",
    "candidates = [Path(\"results/priority_targets.csv\"),\n",
    "              Path(\"results/targets_ranked.csv\")]\n",
    "src = next((p for p in candidates if p.exists()), None)\n",
    "assert src is not None, \"No priority/ranked CSV found in results/. Create one first.\"\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", s.lower())\n",
    "\n",
    "# Map normalized -> original column names\n",
    "cols_norm = {norm(c): c for c in df.columns}\n",
    "\n",
    "def find_col(options):\n",
    "    \"\"\"Return the first matching column name (by fuzzy normalized match).\"\"\"\n",
    "    for opt in options:\n",
    "        n = norm(opt)\n",
    "        # exact or substring match in either direction\n",
    "        for key, orig in cols_norm.items():\n",
    "            if n == key or n in key or key in n:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "# Try many reasonable names\n",
    "col_tic = find_col([\n",
    "    \"TIC\", \"tic\", \"tic_id\", \"tic id\", \"target_tic\",\n",
    "    \"ticnumber\", \"ticid\", \"tic8\", \"tic_v8\", \"tic_id_norm\"\n",
    "])\n",
    "if not col_tic:\n",
    "    raise ValueError(f\"Couldn't find a TIC-like column in {src.name}. \"\n",
    "                     f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "col_toi = find_col([\n",
    "    \"TOI\", \"toi\", \"toi_id\", \"toi id\", \"target\", \"target_toi\",\n",
    "    \"name\", \"designation\", \"label\"\n",
    "])\n",
    "if not col_toi:\n",
    "    # If no TOI-like column, fabricate a readable label from TIC\n",
    "    df[\"__TOI\"] = df[col_tic].apply(lambda v: f\"TIC {int(v)}\" if pd.notnull(v) else \"Unknown\")\n",
    "    col_toi = \"__TOI\"\n",
    "\n",
    "# Clean and pick the next unused TIC\n",
    "df = df[pd.to_numeric(df[col_tic], errors=\"coerce\").notnull()].copy()\n",
    "df[\"__tic\"] = df[col_tic].astype(int)\n",
    "pool = df[~df[\"__tic\"].isin(USED_TICS)]\n",
    "\n",
    "if pool.empty:\n",
    "    raise ValueError(\"All candidates in this file are already used. \"\n",
    "                     \"Update USED_TICS or switch to another list.\")\n",
    "\n",
    "next_row = pool.iloc[0]\n",
    "TARGET_TIC = int(next_row[\"__tic\"])\n",
    "TARGET_TOI = str(next_row[col_toi])\n",
    "\n",
    "print(f\"Selected Target C → TOI='{TARGET_TOI}'  TIC={TARGET_TIC}\")\n",
    "print(f\"(Source: {src.name}; skipping used TICs {sorted(USED_TICS)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b0c6326-78e8-41a2-8a67-1c96a7df2c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local] Found PDCSAP sectors in summary: [5, 31]\n",
      "\n",
      "Target C TIC=311183180  PDCSAP sectors: [5, 31]\n"
     ]
    }
   ],
   "source": [
    "# --- Target C setup & sector discovery ---\n",
    "import os, json, time, lightkurve as lk\n",
    "\n",
    "TARGET_TOI = \"TOI 550.02\"\n",
    "TARGET_TIC = 311183180\n",
    "\n",
    "# 1) Try your local downloader summary first\n",
    "sectors_pdcsap = []\n",
    "summary_path = f\"results/TIC{TARGET_TIC}_download_clean_summary.json\"\n",
    "if os.path.exists(summary_path):\n",
    "    try:\n",
    "        with open(summary_path) as f:\n",
    "            summ = json.load(f)\n",
    "        # be tolerant to different key names\n",
    "        sectors_pdcsap = (summ.get(\"sectors_pdcsap\") or\n",
    "                          summ.get(\"pdcsap_sectors\") or\n",
    "                          summ.get(\"sectors_used\") or\n",
    "                          summ.get(\"sectors\") or [])\n",
    "        sectors_pdcsap = sorted(set(int(s) for s in sectors_pdcsap))\n",
    "        print(f\"[local] Found PDCSAP sectors in summary: {sectors_pdcsap}\")\n",
    "    except Exception as e:\n",
    "        print(\"[local] Summary existed but could not be parsed:\", e)\n",
    "\n",
    "# 2) If nothing local, query MAST with retries\n",
    "def mast_search_pdcsap(tic, retries=3, sleep=2.5):\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "            secs = sorted({int(r.sector) for r in sr if getattr(r, \"sector\", None) is not None})\n",
    "            return secs\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"[mast] attempt {i+1}/{retries} failed: {e} — retrying in {sleep}s\")\n",
    "            time.sleep(sleep)\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "\n",
    "if not sectors_pdcsap:\n",
    "    sectors_pdcsap = mast_search_pdcsap(TARGET_TIC)\n",
    "    print(f\"[mast] PDCSAP sectors from MAST: {sectors_pdcsap}\")\n",
    "\n",
    "SECTORS = sectors_pdcsap\n",
    "assert SECTORS, \"No sectors found for this TIC. Check network or TIC ID.\"\n",
    "print(f\"\\nTarget C TIC={TARGET_TIC}  PDCSAP sectors: {SECTORS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "967db591-05f3-401a-9c5e-cb37db85231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strict local match + robust MAST fallback (Sector-correct) ---\n",
    "import time\n",
    "from pathlib import Path\n",
    "import lightkurve as lk\n",
    "\n",
    "def _find_local_lcf_exact(tic: int, sector: int):\n",
    "    \"\"\"Return a local SPOC LCF path that MATCHES the requested sector; else None.\"\"\"\n",
    "    tic16 = f\"{int(tic):016d}\"\n",
    "    # MAST uses 's{sector:04d}-' before the TIC in filenames, e.g. s0031-0000...{tic}..._lc.fits\n",
    "    pat = f\"*s{int(sector):04d}-*{tic16}*lc.fits\"\n",
    "    roots = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd() / \"mastDownload\",\n",
    "        Path.home() / \"Downloads\" / \"mastDownload\",\n",
    "        Path.cwd() / \"data_raw_fresh\" / \"mastDownload\",\n",
    "    ]\n",
    "    for root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        hits = list(root.rglob(pat))\n",
    "        if hits:\n",
    "            # most recent file is usually the correct one if duplicates exist\n",
    "            return sorted(hits, key=lambda p: p.stat().st_mtime)[-1]\n",
    "    return None\n",
    "\n",
    "def load_pdcsap_sector(tic: int, sector: int, qmask: int = 175, retries: int = 3, sleep: float = 2.5):\n",
    "    \"\"\"Load SPOC PDCSAP for a given TIC/sector.\n",
    "       1) Use a sector-matched local file if present; otherwise\n",
    "       2) Download that sector’s LCF from MAST with retries.\n",
    "       Returns a LightCurve already cleaned & normalized.\n",
    "    \"\"\"\n",
    "    # 1) Exact sector-matched local file?\n",
    "    local = _find_local_lcf_exact(tic, sector)\n",
    "    lcf = None\n",
    "    if local:\n",
    "        print(f\"[local] Using {local}\")\n",
    "        lcf = lk.open(local)  # TessLightCurveFile\n",
    "    else:\n",
    "        # 2) MAST fetch using LightCurveFile API (more robust for PDCSAP selection)\n",
    "        last_err = None\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                print(f\"[mast] Fetching TIC {tic} sector {sector} (try {i+1}/{retries}) …\")\n",
    "                sr = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "                if len(sr) == 0:\n",
    "                    raise RuntimeError(f\"No SPOC LightCurveFile for TIC {tic} sector {sector}\")\n",
    "                lcf = sr.download()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                if i < retries - 1:\n",
    "                    print(f\"  → retrying in {sleep}s due to: {e}\")\n",
    "                    time.sleep(sleep)\n",
    "        if lcf is None:\n",
    "            raise last_err if last_err else RuntimeError(\"Unknown MAST download error\")\n",
    "\n",
    "    # Sanity: confirm the LightCurveFile sector\n",
    "    lcfile_sector = getattr(lcf, \"sector\", None)\n",
    "    if lcfile_sector is not None and int(lcfile_sector) != int(sector):\n",
    "        print(f\"[warn] Loaded LCF sector={lcfile_sector} but requested sector={sector}\")\n",
    "\n",
    "    # PDCSAP flux, then clean\n",
    "    lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "    if hasattr(lc, \"quality\"):\n",
    "        try:\n",
    "            # keep cadences that pass the bitmask\n",
    "            mask = (lc.quality & ~qmask) == 0\n",
    "            lc = lc[mask]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9959d3dc-76fc-4879-9e9b-e04aed2817d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local-first PDCSAP loader with retries for MAST ---\n",
    "import time\n",
    "from pathlib import Path\n",
    "import lightkurve as lk\n",
    "\n",
    "def _find_local_lcf(tic: int, sector: int):\n",
    "    tic16 = f\"{int(tic):016d}\"\n",
    "    patterns = [\n",
    "        f\"*{tic16}*s{int(sector):04d}*lc.fits\",\n",
    "        f\"*{tic16}*lc.fits\",\n",
    "    ]\n",
    "    roots = [Path.cwd(), Path.cwd()/ \"mastDownload\", Path.home()/ \"Downloads\"/ \"mastDownload\"]\n",
    "    for root in roots:\n",
    "        if not root.exists(): \n",
    "            continue\n",
    "        for pat in patterns:\n",
    "            hits = list(root.rglob(pat))\n",
    "            if hits:\n",
    "                return sorted(hits, key=lambda p: (p.stat().st_mtime, len(str(p))))[-1]\n",
    "    return None\n",
    "\n",
    "def load_pdcsap_sector(tic: int, sector: int, qmask: int = 175, retries: int = 3, sleep: float = 2.5):\n",
    "    local = _find_local_lcf(tic, sector)\n",
    "    if local:\n",
    "        print(f\"[local] Using {local}\")\n",
    "        lcf = lk.open(local)\n",
    "        lc = lcf.PDCSAP_FLUX\n",
    "    else:\n",
    "        last_err = None\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                print(f\"[mast] Fetching TIC {tic} sector {sector} (try {i+1}/{retries}) …\")\n",
    "                sr = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "                lcf = sr.download()\n",
    "                lc = lcf.PDCSAP_FLUX\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                if i < retries-1:\n",
    "                    print(f\"  → retrying in {sleep}s due to: {e}\")\n",
    "                    time.sleep(sleep)\n",
    "                else:\n",
    "                    raise last_err\n",
    "    lc = lc.remove_nans().normalize()\n",
    "    if hasattr(lc, \"quality\"):\n",
    "        try:\n",
    "            m = (lc.quality & ~qmask) == 0\n",
    "            lc = lc[m]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d98a04de-f291-498e-9fc0-514244392831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[local] Using /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020294194027-s0031-0000000311183180-0198-s/tess2020294194027-s0031-0000000311183180-0198-s_lc.fits\n",
      "TOI 550.02 — S5: N=16057\n",
      "\n",
      "[S5] points=16057  threads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S5] BLS done in 1.2s (Pmax used=12.08 d)\n",
      "[S5] with nmin=3, periods ≲ 12.08 d have ≥3 transits\n",
      "[S5] TLS refine around 12.03045 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 16057 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:03<1:55:00\u001b[A\n",
      "  1%|▏                                           | 12/2290 periods | 00:03<07:10\u001b[A\n",
      "  2%|▋                                           | 37/2290 periods | 00:03<01:52\u001b[A\n",
      "  3%|█▏                                          | 62/2290 periods | 00:03<00:59\u001b[A\n",
      "  4%|█▌                                          | 84/2290 periods | 00:03<00:39\u001b[A\n",
      "  5%|██                                         | 111/2290 periods | 00:03<00:26\u001b[A\n",
      "  6%|██▍                                        | 133/2290 periods | 00:03<00:21\u001b[A\n",
      "  7%|██▉                                        | 155/2290 periods | 00:03<00:17\u001b[A\n",
      "  8%|███▎                                       | 179/2290 periods | 00:03<00:14\u001b[A\n",
      "  9%|███▉                                       | 208/2290 periods | 00:03<00:11\u001b[A\n",
      " 10%|████▍                                      | 235/2290 periods | 00:04<00:10\u001b[A\n",
      " 11%|████▉                                      | 260/2290 periods | 00:04<00:09\u001b[A\n",
      " 13%|█████▍                                     | 287/2290 periods | 00:04<00:09\u001b[A\n",
      " 14%|█████▉                                     | 314/2290 periods | 00:04<00:08\u001b[A\n",
      " 15%|██████▍                                    | 340/2290 periods | 00:04<00:08\u001b[A\n",
      " 16%|██████▊                                    | 365/2290 periods | 00:04<00:08\u001b[A\n",
      " 17%|███████▎                                   | 392/2290 periods | 00:04<00:08\u001b[A\n",
      " 18%|███████▉                                   | 422/2290 periods | 00:04<00:07\u001b[A\n",
      " 20%|████████▍                                  | 448/2290 periods | 00:04<00:07\u001b[A\n",
      " 21%|█████████                                  | 481/2290 periods | 00:05<00:06\u001b[A\n",
      " 22%|█████████▌                                 | 512/2290 periods | 00:05<00:06\u001b[A\n",
      " 24%|██████████▏                                | 545/2290 periods | 00:05<00:06\u001b[A\n",
      " 25%|██████████▊                                | 574/2290 periods | 00:05<00:06\u001b[A\n",
      " 27%|███████████▍                               | 607/2290 periods | 00:05<00:05\u001b[A\n",
      " 28%|████████████                               | 644/2290 periods | 00:05<00:05\u001b[A\n",
      " 30%|████████████▋                              | 679/2290 periods | 00:05<00:05\u001b[A\n",
      " 31%|█████████████▎                             | 711/2290 periods | 00:05<00:05\u001b[A\n",
      " 32%|█████████████▉                             | 744/2290 periods | 00:05<00:04\u001b[A\n",
      " 34%|██████████████▋                            | 779/2290 periods | 00:05<00:04\u001b[A\n",
      " 35%|███████████████▏                           | 812/2290 periods | 00:06<00:04\u001b[A\n",
      " 37%|███████████████▉                           | 846/2290 periods | 00:06<00:04\u001b[A\n",
      " 38%|████████████████▌                          | 880/2290 periods | 00:06<00:04\u001b[A\n",
      " 40%|█████████████████▏                         | 914/2290 periods | 00:06<00:04\u001b[A\n",
      " 41%|█████████████████▊                         | 949/2290 periods | 00:06<00:04\u001b[A\n",
      " 43%|██████████████████▍                        | 983/2290 periods | 00:06<00:03\u001b[A\n",
      " 44%|██████████████████▋                       | 1016/2290 periods | 00:06<00:03\u001b[A\n",
      " 46%|███████████████████▏                      | 1049/2290 periods | 00:06<00:03\u001b[A\n",
      " 47%|███████████████████▉                      | 1084/2290 periods | 00:06<00:03\u001b[A\n",
      " 49%|████████████████████▍                     | 1117/2290 periods | 00:07<00:03\u001b[A\n",
      " 50%|█████████████████████▏                    | 1154/2290 periods | 00:07<00:03\u001b[A\n",
      " 52%|█████████████████████▊                    | 1190/2290 periods | 00:07<00:03\u001b[A\n",
      " 53%|██████████████████████▍                   | 1224/2290 periods | 00:07<00:03\u001b[A\n",
      " 55%|███████████████████████                   | 1257/2290 periods | 00:07<00:03\u001b[A\n",
      " 56%|███████████████████████▋                  | 1290/2290 periods | 00:07<00:03\u001b[A\n",
      " 58%|████████████████████████▏                 | 1322/2290 periods | 00:07<00:03\u001b[A\n",
      " 59%|████████████████████████▊                 | 1353/2290 periods | 00:07<00:03\u001b[A\n",
      " 61%|█████████████████████████▍                | 1387/2290 periods | 00:07<00:02\u001b[A\n",
      " 62%|██████████████████████████                | 1419/2290 periods | 00:08<00:02\u001b[A\n",
      " 63%|██████████████████████████▌               | 1450/2290 periods | 00:08<00:03\u001b[A\n",
      " 65%|███████████████████████████               | 1478/2290 periods | 00:08<00:03\u001b[A\n",
      " 66%|███████████████████████████▋              | 1510/2290 periods | 00:08<00:03\u001b[A\n",
      " 67%|████████████████████████████▏             | 1538/2290 periods | 00:08<00:02\u001b[A\n",
      " 69%|████████████████████████████▊             | 1573/2290 periods | 00:08<00:02\u001b[A\n",
      " 70%|█████████████████████████████▍            | 1608/2290 periods | 00:08<00:02\u001b[A\n",
      " 72%|██████████████████████████████            | 1642/2290 periods | 00:08<00:02\u001b[A\n",
      " 73%|██████████████████████████████▋           | 1675/2290 periods | 00:08<00:01\u001b[A\n",
      " 75%|███████████████████████████████▎          | 1710/2290 periods | 00:09<00:01\u001b[A\n",
      " 76%|███████████████████████████████▉          | 1744/2290 periods | 00:09<00:01\u001b[A\n",
      " 78%|████████████████████████████████▌         | 1777/2290 periods | 00:09<00:01\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 1810/2290 periods | 00:09<00:01\u001b[A\n",
      " 81%|█████████████████████████████████▊        | 1844/2290 periods | 00:09<00:01\u001b[A\n",
      " 82%|██████████████████████████████████▌       | 1882/2290 periods | 00:09<00:01\u001b[A\n",
      " 84%|███████████████████████████████████       | 1915/2290 periods | 00:09<00:01\u001b[A\n",
      " 85%|███████████████████████████████████▊      | 1951/2290 periods | 00:09<00:01\u001b[A\n",
      " 87%|████████████████████████████████████▍     | 1985/2290 periods | 00:09<00:00\u001b[A\n",
      " 88%|█████████████████████████████████████     | 2021/2290 periods | 00:10<00:00\u001b[A\n",
      " 90%|█████████████████████████████████████▋    | 2056/2290 periods | 00:10<00:00\u001b[A\n",
      " 91%|██████████████████████████████████████▎   | 2090/2290 periods | 00:10<00:00\u001b[A\n",
      " 93%|██████████████████████████████████████▉   | 2123/2290 periods | 00:10<00:00\u001b[A\n",
      " 94%|███████████████████████████████████████▌  | 2156/2290 periods | 00:10<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▏ | 2189/2290 periods | 00:10<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 2220/2290 periods | 00:10<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████▎| 2254/2290 periods | 00:10<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:10<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                     | 214/9020 [00:00<00:04, 1849.98it/s]\u001b[A\n",
      "  5%|██                                    | 479/9020 [00:00<00:03, 2291.18it/s]\u001b[A\n",
      " 10%|███▌                                  | 860/9020 [00:00<00:02, 2960.63it/s]\u001b[A\n",
      " 14%|█████▏                               | 1256/9020 [00:00<00:02, 3345.29it/s]\u001b[A\n",
      " 21%|███████▌                             | 1850/9020 [00:00<00:01, 4267.48it/s]\u001b[A\n",
      " 29%|██████████▌                          | 2588/9020 [00:00<00:01, 5313.83it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3347/9020 [00:00<00:00, 6052.15it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4121/9020 [00:00<00:00, 6584.98it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 4912/9020 [00:00<00:00, 6996.70it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 5679/9020 [00:01<00:00, 7203.97it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 6442/9020 [00:01<00:00, 7333.76it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 7230/9020 [00:01<00:00, 7497.88it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 8031/9020 [00:01<00:00, 7650.43it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:01<00:00, 6271.27it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S5]   TLS best P=9.348442 d, SDE=7.80 (took 12.5s)\n",
      "[S5] TLS refine around 9.34124 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 16057 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:02<1:35:18\u001b[A\n",
      "  1%|▍                                           | 23/2290 periods | 00:02<03:05\u001b[A\n",
      "  2%|█                                           | 56/2290 periods | 00:02<01:03\u001b[A\n",
      "  4%|█▊                                          | 93/2290 periods | 00:02<00:33\u001b[A\n",
      "  6%|██▍                                        | 128/2290 periods | 00:02<00:21\u001b[A\n",
      "  7%|██▉                                        | 159/2290 periods | 00:03<00:16\u001b[A\n",
      "  9%|███▋                                       | 195/2290 periods | 00:03<00:12\u001b[A\n",
      " 10%|████▎                                      | 230/2290 periods | 00:03<00:10\u001b[A\n",
      " 11%|████▉                                      | 263/2290 periods | 00:03<00:08\u001b[A\n",
      " 13%|█████▌                                     | 296/2290 periods | 00:03<00:07\u001b[A\n",
      " 14%|██████▏                                    | 330/2290 periods | 00:03<00:07\u001b[A\n",
      " 16%|██████▊                                    | 365/2290 periods | 00:03<00:06\u001b[A\n",
      " 17%|███████▍                                   | 398/2290 periods | 00:03<00:07\u001b[A\n",
      " 19%|████████                                   | 427/2290 periods | 00:03<00:07\u001b[A\n",
      " 20%|████████▋                                  | 463/2290 periods | 00:04<00:06\u001b[A\n",
      " 22%|█████████▍                                 | 500/2290 periods | 00:04<00:06\u001b[A\n",
      " 23%|██████████                                 | 536/2290 periods | 00:04<00:05\u001b[A\n",
      " 25%|██████████▋                                | 571/2290 periods | 00:04<00:05\u001b[A\n",
      " 26%|███████████▎                               | 604/2290 periods | 00:04<00:05\u001b[A\n",
      " 28%|███████████▉                               | 636/2290 periods | 00:04<00:05\u001b[A\n",
      " 29%|████████████▌                              | 671/2290 periods | 00:04<00:05\u001b[A\n",
      " 31%|█████████████▏                             | 704/2290 periods | 00:04<00:05\u001b[A\n",
      " 32%|█████████████▊                             | 736/2290 periods | 00:04<00:04\u001b[A\n",
      " 34%|██████████████▍                            | 772/2290 periods | 00:04<00:04\u001b[A\n",
      " 35%|███████████████▏                           | 807/2290 periods | 00:05<00:04\u001b[A\n",
      " 37%|███████████████▊                           | 841/2290 periods | 00:05<00:04\u001b[A\n",
      " 38%|████████████████▍                          | 876/2290 periods | 00:05<00:04\u001b[A\n",
      " 40%|█████████████████                          | 910/2290 periods | 00:05<00:04\u001b[A\n",
      " 41%|█████████████████▊                         | 946/2290 periods | 00:05<00:03\u001b[A\n",
      " 43%|██████████████████▍                        | 981/2290 periods | 00:05<00:03\u001b[A\n",
      " 44%|██████████████████▋                       | 1016/2290 periods | 00:05<00:03\u001b[A\n",
      " 46%|███████████████████▎                      | 1050/2290 periods | 00:05<00:03\u001b[A\n",
      " 47%|███████████████████▉                      | 1085/2290 periods | 00:05<00:03\u001b[A\n",
      " 49%|████████████████████▌                     | 1120/2290 periods | 00:06<00:03\u001b[A\n",
      " 50%|█████████████████████▏                    | 1155/2290 periods | 00:06<00:03\u001b[A\n",
      " 52%|█████████████████████▊                    | 1189/2290 periods | 00:06<00:03\u001b[A\n",
      " 53%|██████████████████████▍                   | 1220/2290 periods | 00:06<00:03\u001b[A\n",
      " 55%|███████████████████████                   | 1259/2290 periods | 00:06<00:03\u001b[A\n",
      " 56%|███████████████████████▋                  | 1292/2290 periods | 00:06<00:03\u001b[A\n",
      " 58%|████████████████████████▎                 | 1325/2290 periods | 00:06<00:03\u001b[A\n",
      " 59%|████████████████████████▉                 | 1357/2290 periods | 00:06<00:03\u001b[A\n",
      " 61%|█████████████████████████▌                | 1391/2290 periods | 00:06<00:02\u001b[A\n",
      " 62%|██████████████████████████▏               | 1428/2290 periods | 00:06<00:02\u001b[A\n",
      " 64%|██████████████████████████▊               | 1461/2290 periods | 00:07<00:02\u001b[A\n",
      " 65%|███████████████████████████▍              | 1499/2290 periods | 00:07<00:02\u001b[A\n",
      " 67%|████████████████████████████▏             | 1535/2290 periods | 00:07<00:02\u001b[A\n",
      " 69%|████████████████████████████▊             | 1570/2290 periods | 00:07<00:02\u001b[A\n",
      " 70%|█████████████████████████████▍            | 1608/2290 periods | 00:07<00:01\u001b[A\n",
      " 72%|██████████████████████████████▏           | 1644/2290 periods | 00:07<00:01\u001b[A\n",
      " 73%|██████████████████████████████▊           | 1682/2290 periods | 00:07<00:01\u001b[A\n",
      " 75%|███████████████████████████████▌          | 1718/2290 periods | 00:07<00:01\u001b[A\n",
      " 77%|████████████████████████████████▏         | 1753/2290 periods | 00:07<00:01\u001b[A\n",
      " 78%|████████████████████████████████▊         | 1788/2290 periods | 00:08<00:01\u001b[A\n",
      " 80%|█████████████████████████████████▍        | 1825/2290 periods | 00:08<00:01\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 1863/2290 periods | 00:08<00:01\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 1899/2290 periods | 00:08<00:01\u001b[A\n",
      " 84%|███████████████████████████████████▍      | 1935/2290 periods | 00:08<00:01\u001b[A\n",
      " 86%|████████████████████████████████████▏     | 1970/2290 periods | 00:08<00:00\u001b[A\n",
      " 88%|████████████████████████████████████▊     | 2004/2290 periods | 00:08<00:00\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 2038/2290 periods | 00:08<00:00\u001b[A\n",
      " 90%|██████████████████████████████████████    | 2072/2290 periods | 00:08<00:00\u001b[A\n",
      " 92%|██████████████████████████████████████▋   | 2106/2290 periods | 00:08<00:00\u001b[A\n",
      " 93%|███████████████████████████████████████▏  | 2139/2290 periods | 00:09<00:00\u001b[A\n",
      " 95%|███████████████████████████████████████▊  | 2174/2290 periods | 00:09<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▍ | 2208/2290 periods | 00:09<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████▏| 2245/2290 periods | 00:09<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:09<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▊                                     | 207/9020 [00:00<00:04, 1946.58it/s]\u001b[A\n",
      "  6%|██▍                                   | 585/9020 [00:00<00:02, 2996.18it/s]\u001b[A\n",
      " 10%|███▋                                  | 888/9020 [00:00<00:02, 2892.55it/s]\u001b[A\n",
      " 13%|████▊                                | 1180/9020 [00:00<00:02, 2687.94it/s]\u001b[A\n",
      " 21%|███████▉                             | 1927/9020 [00:00<00:01, 4292.28it/s]\u001b[A\n",
      " 30%|███████████                          | 2703/9020 [00:00<00:01, 5414.50it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 3485/9020 [00:00<00:00, 6175.26it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 4267/9020 [00:00<00:00, 6686.67it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5047/9020 [00:00<00:00, 7028.41it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 5808/9020 [00:01<00:00, 7203.19it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 6534/9020 [00:01<00:00, 6683.72it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 7271/9020 [00:01<00:00, 6878.57it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 8064/9020 [00:01<00:00, 7183.71it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:01<00:00, 6071.90it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S5]   TLS best P=9.348442 d, SDE=7.80 (took 11.2s)\n",
      "[S5] TLS refine around 6.22816 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 16057 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:02<1:26:02\u001b[A\n",
      "  1%|▎                                           | 16/2290 periods | 00:02<04:04\u001b[A\n",
      "  2%|▉                                           | 51/2290 periods | 00:02<01:02\u001b[A\n",
      "  4%|█▌                                          | 84/2290 periods | 00:02<00:33\u001b[A\n",
      "  5%|██▎                                        | 121/2290 periods | 00:02<00:20\u001b[A\n",
      "  7%|██▉                                        | 154/2290 periods | 00:02<00:15\u001b[A\n",
      "  8%|███▌                                       | 190/2290 periods | 00:02<00:11\u001b[A\n",
      " 10%|████▏                                      | 223/2290 periods | 00:02<00:10\u001b[A\n",
      " 11%|████▉                                      | 260/2290 periods | 00:03<00:08\u001b[A\n",
      " 13%|█████▌                                     | 294/2290 periods | 00:03<00:07\u001b[A\n",
      " 14%|██████▏                                    | 327/2290 periods | 00:03<00:07\u001b[A\n",
      " 16%|██████▊                                    | 362/2290 periods | 00:03<00:06\u001b[A\n",
      " 17%|███████▍                                   | 396/2290 periods | 00:03<00:06\u001b[A\n",
      " 19%|████████▏                                  | 434/2290 periods | 00:03<00:05\u001b[A\n",
      " 21%|████████▊                                  | 470/2290 periods | 00:03<00:05\u001b[A\n",
      " 22%|█████████▌                                 | 507/2290 periods | 00:03<00:05\u001b[A\n",
      " 24%|██████████▏                                | 543/2290 periods | 00:03<00:05\u001b[A\n",
      " 25%|██████████▊                                | 578/2290 periods | 00:04<00:05\u001b[A\n",
      " 27%|███████████▍                               | 612/2290 periods | 00:04<00:05\u001b[A\n",
      " 28%|████████████▏                              | 646/2290 periods | 00:04<00:05\u001b[A\n",
      " 30%|████████████▋                              | 679/2290 periods | 00:04<00:04\u001b[A\n",
      " 31%|█████████████▎                             | 712/2290 periods | 00:04<00:04\u001b[A\n",
      " 33%|█████████████▉                             | 745/2290 periods | 00:04<00:04\u001b[A\n",
      " 34%|██████████████▋                            | 779/2290 periods | 00:04<00:04\u001b[A\n",
      " 36%|███████████████▎                           | 813/2290 periods | 00:04<00:04\u001b[A\n",
      " 37%|███████████████▉                           | 846/2290 periods | 00:04<00:04\u001b[A\n",
      " 38%|████████████████▍                          | 878/2290 periods | 00:04<00:04\u001b[A\n",
      " 40%|█████████████████▏                         | 915/2290 periods | 00:05<00:04\u001b[A\n",
      " 42%|█████████████████▊                         | 951/2290 periods | 00:05<00:03\u001b[A\n",
      " 43%|██████████████████▍                        | 985/2290 periods | 00:05<00:03\u001b[A\n",
      " 44%|██████████████████▋                       | 1019/2290 periods | 00:05<00:03\u001b[A\n",
      " 46%|███████████████████▎                      | 1053/2290 periods | 00:05<00:03\u001b[A\n",
      " 48%|███████████████████▉                      | 1089/2290 periods | 00:05<00:03\u001b[A\n",
      " 49%|████████████████████▋                     | 1126/2290 periods | 00:05<00:03\u001b[A\n",
      " 51%|█████████████████████▎                    | 1161/2290 periods | 00:05<00:03\u001b[A\n",
      " 52%|█████████████████████▉                    | 1195/2290 periods | 00:05<00:03\u001b[A\n",
      " 54%|██████████████████████▌                   | 1228/2290 periods | 00:06<00:03\u001b[A\n",
      " 55%|███████████████████████▏                  | 1261/2290 periods | 00:06<00:04\u001b[A\n",
      " 56%|███████████████████████▋                  | 1289/2290 periods | 00:06<00:03\u001b[A\n",
      " 58%|████████████████████████▏                 | 1317/2290 periods | 00:06<00:03\u001b[A\n",
      " 59%|████████████████████████▊                 | 1350/2290 periods | 00:06<00:03\u001b[A\n",
      " 60%|█████████████████████████▎                | 1382/2290 periods | 00:06<00:03\u001b[A\n",
      " 62%|█████████████████████████▉                | 1416/2290 periods | 00:06<00:02\u001b[A\n",
      " 63%|██████████████████████████▌               | 1448/2290 periods | 00:06<00:02\u001b[A\n",
      " 65%|███████████████████████████▏              | 1483/2290 periods | 00:06<00:02\u001b[A\n",
      " 66%|███████████████████████████▊              | 1519/2290 periods | 00:07<00:02\u001b[A\n",
      " 68%|████████████████████████████▍             | 1552/2290 periods | 00:07<00:02\u001b[A\n",
      " 69%|█████████████████████████████             | 1585/2290 periods | 00:07<00:02\u001b[A\n",
      " 71%|█████████████████████████████▋            | 1617/2290 periods | 00:07<00:02\u001b[A\n",
      " 72%|██████████████████████████████▎           | 1653/2290 periods | 00:07<00:02\u001b[A\n",
      " 74%|██████████████████████████████▉           | 1688/2290 periods | 00:07<00:01\u001b[A\n",
      " 75%|███████████████████████████████▋          | 1726/2290 periods | 00:07<00:01\u001b[A\n",
      " 77%|████████████████████████████████▎         | 1761/2290 periods | 00:07<00:01\u001b[A\n",
      " 79%|████████████████████████████████▉         | 1798/2290 periods | 00:07<00:01\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 1833/2290 periods | 00:08<00:01\u001b[A\n",
      " 82%|██████████████████████████████████▎       | 1870/2290 periods | 00:08<00:01\u001b[A\n",
      " 83%|██████████████████████████████████▉       | 1907/2290 periods | 00:08<00:01\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 1943/2290 periods | 00:08<00:00\u001b[A\n",
      " 86%|████████████████████████████████████▎     | 1979/2290 periods | 00:08<00:00\u001b[A\n",
      " 88%|████████████████████████████████████▉     | 2015/2290 periods | 00:08<00:00\u001b[A\n",
      " 90%|█████████████████████████████████████▌    | 2051/2290 periods | 00:08<00:00\u001b[A\n",
      " 91%|██████████████████████████████████████▎   | 2086/2290 periods | 00:08<00:00\u001b[A\n",
      " 93%|██████████████████████████████████████▉   | 2123/2290 periods | 00:08<00:00\u001b[A\n",
      " 94%|███████████████████████████████████████▌  | 2158/2290 periods | 00:08<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▏ | 2193/2290 periods | 00:09<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▉ | 2230/2290 periods | 00:09<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:09<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█                                     | 263/9020 [00:00<00:03, 2629.86it/s]\u001b[A\n",
      "  6%|██▎                                   | 542/9020 [00:00<00:03, 2615.27it/s]\u001b[A\n",
      " 10%|███▊                                  | 910/9020 [00:00<00:02, 3088.07it/s]\u001b[A\n",
      " 14%|█████▎                               | 1282/9020 [00:00<00:02, 3331.43it/s]\u001b[A\n",
      " 20%|███████▏                             | 1760/9020 [00:00<00:01, 3847.28it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2521/9020 [00:00<00:01, 5117.87it/s]\u001b[A\n",
      " 36%|█████████████▌                       | 3292/9020 [00:00<00:00, 5961.45it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4071/9020 [00:00<00:00, 6539.18it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 4846/9020 [00:00<00:00, 6916.09it/s]\u001b[A\n",
      " 62%|███████████████████████              | 5615/9020 [00:01<00:00, 7151.94it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 6385/9020 [00:01<00:00, 7317.75it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 7170/9020 [00:01<00:00, 7478.29it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 7958/9020 [00:01<00:00, 7599.85it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:01<00:00, 6252.10it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S5]   TLS best P=9.348442 d, SDE=7.80 (took 11.0s)\n",
      "[local] Using /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020294194027-s0031-0000000311183180-0198-s/tess2020294194027-s0031-0000000311183180-0198-s_lc.fits\n",
      "TOI 550.02 — S31: N=16057\n",
      "\n",
      "[S31] points=16057  threads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/grid.py:149: UserWarning: period_grid defaults to R_star=1 and M_star=1 as given density yielded grid with too few values\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S31] BLS done in 1.2s (Pmax used=12.08 d)\n",
      "[S31] with nmin=3, periods ≲ 12.08 d have ≥3 transits\n",
      "[S31] TLS refine around 12.03045 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 16057 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:02<1:30:37\u001b[A\n",
      "  1%|▍                                           | 20/2290 periods | 00:02<03:23\u001b[A\n",
      "  2%|█                                           | 54/2290 periods | 00:02<01:02\u001b[A\n",
      "  4%|█▋                                          | 89/2290 periods | 00:02<00:33\u001b[A\n",
      "  5%|██▏                                        | 117/2290 periods | 00:02<00:23\u001b[A\n",
      "  6%|██▋                                        | 144/2290 periods | 00:02<00:18\u001b[A\n",
      "  7%|███▏                                       | 170/2290 periods | 00:03<00:15\u001b[A\n",
      "  9%|███▋                                       | 199/2290 periods | 00:03<00:12\u001b[A\n",
      " 10%|████▎                                      | 232/2290 periods | 00:03<00:10\u001b[A\n",
      " 12%|█████                                      | 267/2290 periods | 00:03<00:08\u001b[A\n",
      " 13%|█████▋                                     | 301/2290 periods | 00:03<00:07\u001b[A\n",
      " 15%|██████▎                                    | 336/2290 periods | 00:03<00:07\u001b[A\n",
      " 16%|██████▉                                    | 372/2290 periods | 00:03<00:06\u001b[A\n",
      " 18%|███████▌                                   | 406/2290 periods | 00:03<00:06\u001b[A\n",
      " 19%|████████▏                                  | 439/2290 periods | 00:03<00:05\u001b[A\n",
      " 21%|████████▊                                  | 472/2290 periods | 00:03<00:05\u001b[A\n",
      " 22%|█████████▌                                 | 508/2290 periods | 00:04<00:05\u001b[A\n",
      " 24%|██████████▏                                | 542/2290 periods | 00:04<00:05\u001b[A\n",
      " 25%|██████████▊                                | 578/2290 periods | 00:04<00:05\u001b[A\n",
      " 27%|███████████▌                               | 613/2290 periods | 00:04<00:05\u001b[A\n",
      " 28%|████████████▏                              | 648/2290 periods | 00:04<00:05\u001b[A\n",
      " 30%|████████████▊                              | 683/2290 periods | 00:04<00:04\u001b[A\n",
      " 31%|█████████████▍                             | 717/2290 periods | 00:04<00:04\u001b[A\n",
      " 33%|██████████████                             | 750/2290 periods | 00:04<00:05\u001b[A\n",
      " 34%|██████████████▋                            | 780/2290 periods | 00:05<00:06\u001b[A\n",
      " 36%|███████████████▎                           | 813/2290 periods | 00:05<00:05\u001b[A\n",
      " 37%|███████████████▊                           | 842/2290 periods | 00:05<00:05\u001b[A\n",
      " 38%|████████████████▍                          | 876/2290 periods | 00:05<00:04\u001b[A\n",
      " 40%|█████████████████                          | 906/2290 periods | 00:05<00:04\u001b[A\n",
      " 41%|█████████████████▋                         | 939/2290 periods | 00:05<00:04\u001b[A\n",
      " 42%|██████████████████▏                        | 970/2290 periods | 00:05<00:04\u001b[A\n",
      " 44%|██████████████████▎                       | 1001/2290 periods | 00:05<00:04\u001b[A\n",
      " 45%|██████████████████▉                       | 1035/2290 periods | 00:05<00:04\u001b[A\n",
      " 47%|███████████████████▋                      | 1071/2290 periods | 00:05<00:03\u001b[A\n",
      " 48%|████████████████████▎                     | 1109/2290 periods | 00:06<00:03\u001b[A\n",
      " 50%|████████████████████▉                     | 1143/2290 periods | 00:06<00:03\u001b[A\n",
      " 51%|█████████████████████▌                    | 1177/2290 periods | 00:06<00:03\u001b[A\n",
      " 53%|██████████████████████▏                   | 1213/2290 periods | 00:06<00:03\u001b[A\n",
      " 54%|██████████████████████▊                   | 1247/2290 periods | 00:06<00:03\u001b[A\n",
      " 56%|███████████████████████▍                  | 1281/2290 periods | 00:06<00:03\u001b[A\n",
      " 57%|████████████████████████                  | 1315/2290 periods | 00:06<00:02\u001b[A\n",
      " 59%|████████████████████████▋                 | 1349/2290 periods | 00:06<00:02\u001b[A\n",
      " 60%|█████████████████████████▎                | 1382/2290 periods | 00:06<00:02\u001b[A\n",
      " 62%|█████████████████████████▉                | 1415/2290 periods | 00:06<00:02\u001b[A\n",
      " 63%|██████████████████████████▌               | 1448/2290 periods | 00:07<00:02\u001b[A\n",
      " 65%|███████████████████████████▏              | 1480/2290 periods | 00:07<00:02\u001b[A\n",
      " 66%|███████████████████████████▋              | 1513/2290 periods | 00:07<00:02\u001b[A\n",
      " 68%|████████████████████████████▎             | 1546/2290 periods | 00:07<00:02\u001b[A\n",
      " 69%|████████████████████████████▉             | 1580/2290 periods | 00:07<00:02\u001b[A\n",
      " 70%|█████████████████████████████▌            | 1613/2290 periods | 00:07<00:02\u001b[A\n",
      " 72%|██████████████████████████████▏           | 1649/2290 periods | 00:07<00:01\u001b[A\n",
      " 74%|██████████████████████████████▉           | 1685/2290 periods | 00:07<00:01\u001b[A\n",
      " 75%|███████████████████████████████▌          | 1719/2290 periods | 00:07<00:01\u001b[A\n",
      " 77%|████████████████████████████████▏         | 1753/2290 periods | 00:08<00:01\u001b[A\n",
      " 78%|████████████████████████████████▊         | 1786/2290 periods | 00:08<00:01\u001b[A\n",
      " 79%|█████████████████████████████████▎        | 1818/2290 periods | 00:08<00:01\u001b[A\n",
      " 81%|██████████████████████████████████        | 1854/2290 periods | 00:08<00:01\u001b[A\n",
      " 83%|██████████████████████████████████▋       | 1891/2290 periods | 00:08<00:01\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 1925/2290 periods | 00:08<00:01\u001b[A\n",
      " 86%|███████████████████████████████████▉      | 1959/2290 periods | 00:08<00:00\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 1993/2290 periods | 00:08<00:00\u001b[A\n",
      " 88%|█████████████████████████████████████▏    | 2026/2290 periods | 00:08<00:00\u001b[A\n",
      " 90%|█████████████████████████████████████▊    | 2062/2290 periods | 00:08<00:00\u001b[A\n",
      " 92%|██████████████████████████████████████▍   | 2099/2290 periods | 00:09<00:00\u001b[A\n",
      " 93%|███████████████████████████████████████▏  | 2134/2290 periods | 00:09<00:00\u001b[A\n",
      " 95%|███████████████████████████████████████▊  | 2169/2290 periods | 00:09<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▍ | 2203/2290 periods | 00:09<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████ | 2240/2290 periods | 00:09<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:09<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▍                                    | 340/9020 [00:00<00:02, 3387.24it/s]\u001b[A\n",
      "  8%|██▊                                   | 679/9020 [00:00<00:03, 2737.57it/s]\u001b[A\n",
      " 11%|████▏                                 | 991/9020 [00:00<00:02, 2891.79it/s]\u001b[A\n",
      " 16%|█████▊                               | 1415/9020 [00:00<00:02, 3194.25it/s]\u001b[A\n",
      " 20%|███████▌                             | 1830/9020 [00:00<00:02, 3509.09it/s]\u001b[A\n",
      " 29%|██████████▋                          | 2598/9020 [00:00<00:01, 4844.92it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3282/9020 [00:00<00:01, 5471.31it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 3995/9020 [00:00<00:00, 5980.80it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 4723/9020 [00:00<00:00, 6378.93it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 5424/9020 [00:01<00:00, 6569.85it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 6187/9020 [00:01<00:00, 6888.68it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 6945/9020 [00:01<00:00, 7096.55it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 7658/9020 [00:01<00:00, 7089.94it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:01<00:00, 5861.79it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S31]   TLS best P=9.348442 d, SDE=7.80 (took 11.5s)\n",
      "[S31] TLS refine around 9.34124 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 16057 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:02<1:23:18\u001b[A\n",
      "  1%|▎                                           | 14/2290 periods | 00:02<04:30\u001b[A\n",
      "  2%|▉                                           | 49/2290 periods | 00:02<01:02\u001b[A\n",
      "  4%|█▋                                          | 85/2290 periods | 00:02<00:31\u001b[A\n",
      "  5%|██▎                                        | 124/2290 periods | 00:02<00:19\u001b[A\n",
      "  7%|██▉                                        | 156/2290 periods | 00:02<00:15\u001b[A\n",
      "  8%|███▌                                       | 192/2290 periods | 00:02<00:11\u001b[A\n",
      " 10%|████▎                                      | 228/2290 periods | 00:02<00:09\u001b[A\n",
      " 11%|████▉                                      | 263/2290 periods | 00:03<00:08\u001b[A\n",
      " 13%|█████▌                                     | 297/2290 periods | 00:03<00:07\u001b[A\n",
      " 14%|██████▏                                    | 332/2290 periods | 00:03<00:06\u001b[A\n",
      " 16%|██████▉                                    | 367/2290 periods | 00:03<00:06\u001b[A\n",
      " 18%|███████▌                                   | 401/2290 periods | 00:03<00:06\u001b[A\n",
      " 19%|████████▏                                  | 435/2290 periods | 00:03<00:05\u001b[A\n",
      " 20%|████████▊                                  | 469/2290 periods | 00:03<00:05\u001b[A\n",
      " 22%|█████████▌                                 | 507/2290 periods | 00:03<00:05\u001b[A\n",
      " 24%|██████████▏                                | 542/2290 periods | 00:03<00:05\u001b[A\n",
      " 25%|██████████▊                                | 577/2290 periods | 00:03<00:05\u001b[A\n",
      " 27%|███████████▍                               | 612/2290 periods | 00:04<00:04\u001b[A\n",
      " 28%|████████████▏                              | 647/2290 periods | 00:04<00:04\u001b[A\n",
      " 30%|████████████▊                              | 682/2290 periods | 00:04<00:04\u001b[A\n",
      " 31%|█████████████▍                             | 717/2290 periods | 00:04<00:04\u001b[A\n",
      " 33%|██████████████▏                            | 753/2290 periods | 00:04<00:04\u001b[A\n",
      " 34%|██████████████▊                            | 788/2290 periods | 00:04<00:04\u001b[A\n",
      " 36%|███████████████▍                           | 823/2290 periods | 00:04<00:04\u001b[A\n",
      " 37%|████████████████                           | 857/2290 periods | 00:04<00:04\u001b[A\n",
      " 39%|████████████████▋                          | 891/2290 periods | 00:04<00:04\u001b[A\n",
      " 40%|█████████████████▎                         | 925/2290 periods | 00:04<00:04\u001b[A\n",
      " 42%|██████████████████                         | 959/2290 periods | 00:05<00:04\u001b[A\n",
      " 43%|██████████████████▋                        | 993/2290 periods | 00:05<00:04\u001b[A\n",
      " 45%|██████████████████▉                       | 1031/2290 periods | 00:05<00:03\u001b[A\n",
      " 47%|███████████████████▌                      | 1065/2290 periods | 00:05<00:03\u001b[A\n",
      " 48%|████████████████████▏                     | 1101/2290 periods | 00:05<00:03\u001b[A\n",
      " 50%|████████████████████▊                     | 1135/2290 periods | 00:05<00:03\u001b[A\n",
      " 51%|█████████████████████▍                    | 1171/2290 periods | 00:05<00:03\u001b[A\n",
      " 53%|██████████████████████                    | 1205/2290 periods | 00:05<00:03\u001b[A\n",
      " 54%|██████████████████████▋                   | 1239/2290 periods | 00:05<00:03\u001b[A\n",
      " 56%|███████████████████████▎                  | 1274/2290 periods | 00:06<00:03\u001b[A\n",
      " 57%|███████████████████████▉                  | 1308/2290 periods | 00:06<00:03\u001b[A\n",
      " 59%|████████████████████████▋                 | 1343/2290 periods | 00:06<00:02\u001b[A\n",
      " 60%|█████████████████████████▎                | 1379/2290 periods | 00:06<00:02\u001b[A\n",
      " 62%|█████████████████████████▉                | 1413/2290 periods | 00:06<00:02\u001b[A\n",
      " 63%|██████████████████████████▌               | 1447/2290 periods | 00:06<00:02\u001b[A\n",
      " 65%|███████████████████████████▏              | 1484/2290 periods | 00:06<00:02\u001b[A\n",
      " 66%|███████████████████████████▊              | 1518/2290 periods | 00:06<00:02\u001b[A\n",
      " 68%|████████████████████████████▍             | 1550/2290 periods | 00:06<00:02\u001b[A\n",
      " 69%|█████████████████████████████             | 1583/2290 periods | 00:07<00:02\u001b[A\n",
      " 71%|█████████████████████████████▋            | 1619/2290 periods | 00:07<00:02\u001b[A\n",
      " 72%|██████████████████████████████▎           | 1654/2290 periods | 00:07<00:01\u001b[A\n",
      " 74%|██████████████████████████████▉           | 1687/2290 periods | 00:07<00:01\u001b[A\n",
      " 75%|███████████████████████████████▌          | 1722/2290 periods | 00:07<00:01\u001b[A\n",
      " 77%|████████████████████████████████▎         | 1759/2290 periods | 00:07<00:01\u001b[A\n",
      " 78%|████████████████████████████████▉         | 1793/2290 periods | 00:07<00:01\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 1830/2290 periods | 00:07<00:01\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 1865/2290 periods | 00:07<00:01\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 1899/2290 periods | 00:07<00:01\u001b[A\n",
      " 84%|███████████████████████████████████▍      | 1935/2290 periods | 00:08<00:01\u001b[A\n",
      " 86%|████████████████████████████████████      | 1969/2290 periods | 00:08<00:00\u001b[A\n",
      " 88%|████████████████████████████████████▊     | 2005/2290 periods | 00:08<00:00\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 2040/2290 periods | 00:08<00:00\u001b[A\n",
      " 91%|██████████████████████████████████████    | 2075/2290 periods | 00:08<00:00\u001b[A\n",
      " 92%|██████████████████████████████████████▋   | 2111/2290 periods | 00:08<00:00\u001b[A\n",
      " 94%|███████████████████████████████████████▎  | 2146/2290 periods | 00:08<00:00\u001b[A\n",
      " 95%|████████████████████████████████████████  | 2181/2290 periods | 00:08<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 2217/2290 periods | 00:08<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████▎| 2252/2290 periods | 00:08<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:09<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                     | 212/9020 [00:00<00:04, 2119.83it/s]\u001b[A\n",
      "  6%|██▏                                   | 517/9020 [00:00<00:03, 2661.86it/s]\u001b[A\n",
      " 12%|████▍                                | 1081/9020 [00:00<00:01, 4014.27it/s]\u001b[A\n",
      " 17%|██████▎                              | 1525/9020 [00:00<00:01, 4181.13it/s]\u001b[A\n",
      " 22%|███████▉                             | 1944/9020 [00:00<00:01, 4111.94it/s]\u001b[A\n",
      " 30%|███████████                          | 2709/9020 [00:00<00:01, 5299.73it/s]\u001b[A\n",
      " 39%|██████████████▏                      | 3473/9020 [00:00<00:00, 6057.19it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4249/9020 [00:00<00:00, 6594.12it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5010/9020 [00:00<00:00, 6908.83it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 5775/9020 [00:01<00:00, 7135.28it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 6490/9020 [00:01<00:00, 7044.18it/s]\u001b[A\n",
      " 80%|█████████████████████████████▊       | 7258/9020 [00:01<00:00, 7233.40it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 7986/9020 [00:01<00:00, 7245.25it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:01<00:00, 6271.23it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S31]   TLS best P=9.348442 d, SDE=7.80 (took 10.7s)\n",
      "[S31] TLS refine around 6.22816 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 16057 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:02<1:26:30\u001b[A\n",
      "  0%|                                             | 5/2290 periods | 00:02<13:44\u001b[A\n",
      "  1%|▍                                           | 21/2290 periods | 00:02<02:31\u001b[A\n",
      "  2%|█                                           | 55/2290 periods | 00:02<00:47\u001b[A\n",
      "  4%|█▋                                          | 88/2290 periods | 00:02<00:27\u001b[A\n",
      "  5%|██▎                                        | 125/2290 periods | 00:02<00:17\u001b[A\n",
      "  7%|███                                        | 161/2290 periods | 00:02<00:12\u001b[A\n",
      "  9%|███▋                                       | 196/2290 periods | 00:03<00:10\u001b[A\n",
      " 10%|████▎                                      | 228/2290 periods | 00:03<00:09\u001b[A\n",
      " 11%|████▉                                      | 261/2290 periods | 00:03<00:08\u001b[A\n",
      " 13%|█████▌                                     | 298/2290 periods | 00:03<00:07\u001b[A\n",
      " 15%|██████▎                                    | 333/2290 periods | 00:03<00:06\u001b[A\n",
      " 16%|██████▉                                    | 368/2290 periods | 00:03<00:06\u001b[A\n",
      " 18%|███████▌                                   | 402/2290 periods | 00:03<00:06\u001b[A\n",
      " 19%|████████▏                                  | 439/2290 periods | 00:03<00:05\u001b[A\n",
      " 21%|████████▉                                  | 473/2290 periods | 00:03<00:05\u001b[A\n",
      " 22%|█████████▌                                 | 508/2290 periods | 00:03<00:05\u001b[A\n",
      " 24%|██████████▏                                | 543/2290 periods | 00:04<00:05\u001b[A\n",
      " 25%|██████████▊                                | 578/2290 periods | 00:04<00:05\u001b[A\n",
      " 27%|███████████▌                               | 614/2290 periods | 00:04<00:04\u001b[A\n",
      " 28%|████████████▏                              | 649/2290 periods | 00:04<00:04\u001b[A\n",
      " 30%|████████████▊                              | 683/2290 periods | 00:04<00:04\u001b[A\n",
      " 31%|█████████████▍                             | 717/2290 periods | 00:04<00:05\u001b[A\n",
      " 33%|██████████████                             | 751/2290 periods | 00:04<00:04\u001b[A\n",
      " 34%|██████████████▋                            | 785/2290 periods | 00:04<00:04\u001b[A\n",
      " 36%|███████████████▍                           | 819/2290 periods | 00:04<00:04\u001b[A\n",
      " 37%|████████████████                           | 855/2290 periods | 00:04<00:04\u001b[A\n",
      " 39%|████████████████▊                          | 893/2290 periods | 00:05<00:04\u001b[A\n",
      " 41%|█████████████████▍                         | 928/2290 periods | 00:05<00:03\u001b[A\n",
      " 42%|██████████████████                         | 964/2290 periods | 00:05<00:03\u001b[A\n",
      " 44%|██████████████████▎                       | 1001/2290 periods | 00:05<00:03\u001b[A\n",
      " 45%|███████████████████                       | 1038/2290 periods | 00:05<00:03\u001b[A\n",
      " 47%|███████████████████▋                      | 1074/2290 periods | 00:05<00:03\u001b[A\n",
      " 48%|████████████████████▎                     | 1110/2290 periods | 00:05<00:03\u001b[A\n",
      " 50%|█████████████████████                     | 1146/2290 periods | 00:05<00:03\u001b[A\n",
      " 52%|█████████████████████▋                    | 1181/2290 periods | 00:05<00:03\u001b[A\n",
      " 53%|██████████████████████▎                   | 1216/2290 periods | 00:06<00:03\u001b[A\n",
      " 55%|██████████████████████▉                   | 1251/2290 periods | 00:06<00:03\u001b[A\n",
      " 56%|███████████████████████▌                  | 1286/2290 periods | 00:06<00:03\u001b[A\n",
      " 58%|████████████████████████▏                 | 1322/2290 periods | 00:06<00:02\u001b[A\n",
      " 59%|████████████████████████▉                 | 1357/2290 periods | 00:06<00:02\u001b[A\n",
      " 61%|█████████████████████████▌                | 1391/2290 periods | 00:06<00:02\u001b[A\n",
      " 62%|██████████████████████████▏               | 1427/2290 periods | 00:06<00:02\u001b[A\n",
      " 64%|██████████████████████████▊               | 1462/2290 periods | 00:06<00:02\u001b[A\n",
      " 65%|███████████████████████████▍              | 1499/2290 periods | 00:06<00:02\u001b[A\n",
      " 67%|████████████████████████████▏             | 1535/2290 periods | 00:06<00:02\u001b[A\n",
      " 69%|████████████████████████████▊             | 1570/2290 periods | 00:07<00:02\u001b[A\n",
      " 70%|█████████████████████████████▍            | 1608/2290 periods | 00:07<00:01\u001b[A\n",
      " 72%|██████████████████████████████▏           | 1644/2290 periods | 00:07<00:01\u001b[A\n",
      " 73%|██████████████████████████████▊           | 1681/2290 periods | 00:07<00:01\u001b[A\n",
      " 75%|███████████████████████████████▍          | 1717/2290 periods | 00:07<00:01\u001b[A\n",
      " 77%|████████████████████████████████▏         | 1753/2290 periods | 00:07<00:01\u001b[A\n",
      " 78%|████████████████████████████████▊         | 1789/2290 periods | 00:07<00:01\u001b[A\n",
      " 80%|█████████████████████████████████▍        | 1825/2290 periods | 00:07<00:01\u001b[A\n",
      " 81%|██████████████████████████████████        | 1860/2290 periods | 00:07<00:01\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 1896/2290 periods | 00:08<00:01\u001b[A\n",
      " 84%|███████████████████████████████████▍      | 1931/2290 periods | 00:08<00:01\u001b[A\n",
      " 86%|████████████████████████████████████      | 1965/2290 periods | 00:08<00:00\u001b[A\n",
      " 87%|████████████████████████████████████▋     | 1999/2290 periods | 00:08<00:00\u001b[A\n",
      " 89%|█████████████████████████████████████▎    | 2035/2290 periods | 00:08<00:00\u001b[A\n",
      " 90%|█████████████████████████████████████▉    | 2070/2290 periods | 00:08<00:00\u001b[A\n",
      " 92%|██████████████████████████████████████▌   | 2105/2290 periods | 00:08<00:00\u001b[A\n",
      " 93%|███████████████████████████████████████▏  | 2137/2290 periods | 00:08<00:00\u001b[A\n",
      " 95%|███████████████████████████████████████▋  | 2166/2290 periods | 00:08<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▎ | 2197/2290 periods | 00:09<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▉ | 2231/2290 periods | 00:09<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:09<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█                                     | 240/9020 [00:00<00:03, 2399.72it/s]\u001b[A\n",
      "  5%|██                                    | 480/9020 [00:00<00:04, 1867.46it/s]\u001b[A\n",
      " 10%|███▊                                  | 913/9020 [00:00<00:02, 2839.83it/s]\u001b[A\n",
      " 14%|█████▏                               | 1274/9020 [00:00<00:02, 3116.52it/s]\u001b[A\n",
      " 21%|███████▉                             | 1923/9020 [00:00<00:01, 4263.04it/s]\u001b[A\n",
      " 29%|██████████▋                          | 2618/9020 [00:00<00:01, 5141.38it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3322/9020 [00:00<00:00, 5744.86it/s]\u001b[A\n",
      " 43%|████████████████                     | 3908/9020 [00:00<00:00, 5520.86it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 4470/9020 [00:00<00:00, 5464.74it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5074/9020 [00:01<00:00, 5634.88it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 5643/9020 [00:01<00:00, 5400.17it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 6289/9020 [00:01<00:00, 5704.85it/s]\u001b[A\n",
      " 77%|████████████████████████████▋        | 6988/9020 [00:01<00:00, 6079.30it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 7730/9020 [00:01<00:00, 6473.29it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:01<00:00, 5366.06it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S31]   TLS best P=9.348442 d, SDE=7.80 (took 11.2s)\n",
      "\n",
      "[stitched] points=32114  threads=8\n",
      "[stitched] BLS done in 2.7s (Pmax used=22.89 d)\n",
      "[stitched] TLS refine around 18.68163 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 32114 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:03<2:05:51\u001b[A\n",
      "  1%|▏                                           | 13/2290 periods | 00:03<07:12\u001b[A\n",
      "  1%|▌                                           | 30/2290 periods | 00:03<02:36\u001b[A\n",
      "  2%|▊                                           | 45/2290 periods | 00:03<01:32\u001b[A\n",
      "  3%|█▏                                          | 59/2290 periods | 00:03<01:03\u001b[A\n",
      "  3%|█▍                                          | 75/2290 periods | 00:03<00:44\u001b[A\n",
      "  4%|█▋                                          | 89/2290 periods | 00:03<00:37\u001b[A\n",
      "  4%|█▉                                         | 102/2290 periods | 00:04<00:36\u001b[A\n",
      "  5%|██▏                                        | 118/2290 periods | 00:04<00:28\u001b[A\n",
      "  6%|██▌                                        | 137/2290 periods | 00:04<00:22\u001b[A\n",
      "  7%|██▉                                        | 155/2290 periods | 00:04<00:19\u001b[A\n",
      "  8%|███▎                                       | 175/2290 periods | 00:04<00:16\u001b[A\n",
      "  8%|███▌                                       | 193/2290 periods | 00:04<00:14\u001b[A\n",
      "  9%|███▉                                       | 213/2290 periods | 00:04<00:13\u001b[A\n",
      " 10%|████▎                                      | 232/2290 periods | 00:04<00:13\u001b[A\n",
      " 11%|████▋                                      | 250/2290 periods | 00:05<00:13\u001b[A\n",
      " 12%|█████                                      | 269/2290 periods | 00:05<00:12\u001b[A\n",
      " 13%|█████▍                                     | 287/2290 periods | 00:05<00:12\u001b[A\n",
      " 13%|█████▋                                     | 305/2290 periods | 00:05<00:12\u001b[A\n",
      " 14%|██████                                     | 325/2290 periods | 00:05<00:11\u001b[A\n",
      " 15%|██████▍                                    | 344/2290 periods | 00:05<00:11\u001b[A\n",
      " 16%|██████▊                                    | 362/2290 periods | 00:05<00:10\u001b[A\n",
      " 17%|███████▏                                   | 381/2290 periods | 00:05<00:10\u001b[A\n",
      " 17%|███████▍                                   | 399/2290 periods | 00:05<00:10\u001b[A\n",
      " 18%|███████▊                                   | 417/2290 periods | 00:06<00:10\u001b[A\n",
      " 19%|████████▏                                  | 435/2290 periods | 00:06<00:11\u001b[A\n",
      " 20%|████████▍                                  | 452/2290 periods | 00:06<00:11\u001b[A\n",
      " 21%|████████▊                                  | 470/2290 periods | 00:06<00:10\u001b[A\n",
      " 21%|█████████▏                                 | 489/2290 periods | 00:06<00:10\u001b[A\n",
      " 22%|█████████▌                                 | 510/2290 periods | 00:06<00:10\u001b[A\n",
      " 23%|█████████▉                                 | 528/2290 periods | 00:06<00:10\u001b[A\n",
      " 24%|██████████▎                                | 549/2290 periods | 00:06<00:09\u001b[A\n",
      " 25%|██████████▋                                | 568/2290 periods | 00:06<00:09\u001b[A\n",
      " 26%|███████████                                | 587/2290 periods | 00:06<00:09\u001b[A\n",
      " 26%|███████████▍                               | 606/2290 periods | 00:07<00:10\u001b[A\n",
      " 27%|███████████▋                               | 623/2290 periods | 00:07<00:10\u001b[A\n",
      " 28%|████████████                               | 641/2290 periods | 00:07<00:10\u001b[A\n",
      " 29%|████████████▎                              | 658/2290 periods | 00:07<00:10\u001b[A\n",
      " 29%|████████████▋                              | 674/2290 periods | 00:07<00:10\u001b[A\n",
      " 30%|████████████▉                              | 691/2290 periods | 00:07<00:10\u001b[A\n",
      " 31%|█████████████▎                             | 707/2290 periods | 00:07<00:10\u001b[A\n",
      " 32%|█████████████▌                             | 723/2290 periods | 00:07<00:10\u001b[A\n",
      " 32%|█████████████▉                             | 740/2290 periods | 00:07<00:09\u001b[A\n",
      " 33%|██████████████▏                            | 757/2290 periods | 00:08<00:09\u001b[A\n",
      " 34%|██████████████▌                            | 776/2290 periods | 00:08<00:09\u001b[A\n",
      " 35%|██████████████▉                            | 793/2290 periods | 00:08<00:08\u001b[A\n",
      " 35%|███████████████▏                           | 810/2290 periods | 00:08<00:09\u001b[A\n",
      " 36%|███████████████▌                           | 828/2290 periods | 00:08<00:08\u001b[A\n",
      " 37%|███████████████▊                           | 845/2290 periods | 00:08<00:09\u001b[A\n",
      " 38%|████████████████▏                          | 862/2290 periods | 00:08<00:08\u001b[A\n",
      " 38%|████████████████▌                          | 879/2290 periods | 00:08<00:08\u001b[A\n",
      " 39%|████████████████▊                          | 897/2290 periods | 00:08<00:08\u001b[A\n",
      " 40%|█████████████████▏                         | 914/2290 periods | 00:09<00:08\u001b[A\n",
      " 41%|█████████████████▍                         | 930/2290 periods | 00:09<00:09\u001b[A\n",
      " 41%|█████████████████▋                         | 945/2290 periods | 00:09<00:10\u001b[A\n",
      " 42%|██████████████████                         | 959/2290 periods | 00:09<00:10\u001b[A\n",
      " 42%|██████████████████▎                        | 972/2290 periods | 00:09<00:11\u001b[A\n",
      " 43%|██████████████████▍                        | 984/2290 periods | 00:09<00:12\u001b[A\n",
      " 43%|██████████████████▋                        | 995/2290 periods | 00:09<00:16\u001b[A\n",
      " 44%|██████████████████▌                       | 1010/2290 periods | 00:10<00:13\u001b[A\n",
      " 45%|██████████████████▋                       | 1021/2290 periods | 00:10<00:13\u001b[A\n",
      " 45%|██████████████████▉                       | 1034/2290 periods | 00:10<00:12\u001b[A\n",
      " 46%|███████████████████▎                      | 1051/2290 periods | 00:10<00:10\u001b[A\n",
      " 47%|███████████████████▌                      | 1067/2290 periods | 00:10<00:09\u001b[A\n",
      " 47%|███████████████████▉                      | 1084/2290 periods | 00:10<00:08\u001b[A\n",
      " 48%|████████████████████▏                     | 1101/2290 periods | 00:10<00:08\u001b[A\n",
      " 49%|████████████████████▌                     | 1120/2290 periods | 00:10<00:07\u001b[A\n",
      " 50%|████████████████████▉                     | 1139/2290 periods | 00:10<00:07\u001b[A\n",
      " 51%|█████████████████████▎                    | 1159/2290 periods | 00:11<00:06\u001b[A\n",
      " 52%|█████████████████████▋                    | 1180/2290 periods | 00:11<00:06\u001b[A\n",
      " 52%|█████████████████████▉                    | 1199/2290 periods | 00:11<00:06\u001b[A\n",
      " 53%|██████████████████████▎                   | 1219/2290 periods | 00:11<00:06\u001b[A\n",
      " 54%|██████████████████████▋                   | 1237/2290 periods | 00:11<00:06\u001b[A\n",
      " 55%|██████████████████████▉                   | 1254/2290 periods | 00:11<00:06\u001b[A\n",
      " 56%|███████████████████████▎                  | 1274/2290 periods | 00:11<00:05\u001b[A\n",
      " 56%|███████████████████████▋                  | 1292/2290 periods | 00:11<00:05\u001b[A\n",
      " 57%|████████████████████████                  | 1313/2290 periods | 00:11<00:05\u001b[A\n",
      " 58%|████████████████████████▍                 | 1332/2290 periods | 00:11<00:05\u001b[A\n",
      " 59%|████████████████████████▊                 | 1351/2290 periods | 00:12<00:05\u001b[A\n",
      " 60%|█████████████████████████▏                | 1370/2290 periods | 00:12<00:05\u001b[A\n",
      " 61%|█████████████████████████▍                | 1390/2290 periods | 00:12<00:05\u001b[A\n",
      " 61%|█████████████████████████▊                | 1408/2290 periods | 00:12<00:05\u001b[A\n",
      " 62%|██████████████████████████▏               | 1426/2290 periods | 00:12<00:04\u001b[A\n",
      " 63%|██████████████████████████▌               | 1445/2290 periods | 00:12<00:04\u001b[A\n",
      " 64%|██████████████████████████▊               | 1463/2290 periods | 00:12<00:04\u001b[A\n",
      " 65%|███████████████████████████▏              | 1482/2290 periods | 00:12<00:04\u001b[A\n",
      " 66%|███████████████████████████▌              | 1500/2290 periods | 00:12<00:04\u001b[A\n",
      " 66%|███████████████████████████▊              | 1519/2290 periods | 00:13<00:04\u001b[A\n",
      " 67%|████████████████████████████▏             | 1539/2290 periods | 00:13<00:04\u001b[A\n",
      " 68%|████████████████████████████▌             | 1559/2290 periods | 00:13<00:04\u001b[A\n",
      " 69%|████████████████████████████▉             | 1580/2290 periods | 00:13<00:03\u001b[A\n",
      " 70%|█████████████████████████████▎            | 1599/2290 periods | 00:13<00:03\u001b[A\n",
      " 71%|█████████████████████████████▋            | 1619/2290 periods | 00:13<00:03\u001b[A\n",
      " 72%|██████████████████████████████            | 1638/2290 periods | 00:13<00:03\u001b[A\n",
      " 72%|██████████████████████████████▎           | 1656/2290 periods | 00:13<00:03\u001b[A\n",
      " 73%|██████████████████████████████▋           | 1674/2290 periods | 00:13<00:03\u001b[A\n",
      " 74%|███████████████████████████████           | 1692/2290 periods | 00:14<00:03\u001b[A\n",
      " 75%|███████████████████████████████▍          | 1711/2290 periods | 00:14<00:03\u001b[A\n",
      " 76%|███████████████████████████████▋          | 1731/2290 periods | 00:14<00:03\u001b[A\n",
      " 76%|████████████████████████████████          | 1750/2290 periods | 00:14<00:03\u001b[A\n",
      " 77%|████████████████████████████████▍         | 1769/2290 periods | 00:14<00:03\u001b[A\n",
      " 78%|████████████████████████████████▊         | 1787/2290 periods | 00:14<00:02\u001b[A\n",
      " 79%|█████████████████████████████████         | 1806/2290 periods | 00:14<00:02\u001b[A\n",
      " 80%|█████████████████████████████████▍        | 1824/2290 periods | 00:14<00:02\u001b[A\n",
      " 80%|█████████████████████████████████▊        | 1842/2290 periods | 00:14<00:02\u001b[A\n",
      " 81%|██████████████████████████████████▏       | 1861/2290 periods | 00:15<00:02\u001b[A\n",
      " 82%|██████████████████████████████████▍       | 1881/2290 periods | 00:15<00:02\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 1900/2290 periods | 00:15<00:02\u001b[A\n",
      " 84%|███████████████████████████████████▏      | 1918/2290 periods | 00:15<00:02\u001b[A\n",
      " 85%|███████████████████████████████████▌      | 1936/2290 periods | 00:15<00:02\u001b[A\n",
      " 85%|███████████████████████████████████▊      | 1956/2290 periods | 00:15<00:01\u001b[A\n",
      " 86%|████████████████████████████████████▏     | 1974/2290 periods | 00:15<00:01\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 1992/2290 periods | 00:15<00:01\u001b[A\n",
      " 88%|████████████████████████████████████▊     | 2010/2290 periods | 00:15<00:01\u001b[A\n",
      " 89%|█████████████████████████████████████▏    | 2028/2290 periods | 00:16<00:01\u001b[A\n",
      " 89%|█████████████████████████████████████▌    | 2046/2290 periods | 00:16<00:01\u001b[A\n",
      " 90%|█████████████████████████████████████▊    | 2064/2290 periods | 00:16<00:01\u001b[A\n",
      " 91%|██████████████████████████████████████▎   | 2086/2290 periods | 00:16<00:01\u001b[A\n",
      " 92%|██████████████████████████████████████▌   | 2104/2290 periods | 00:16<00:01\u001b[A\n",
      " 93%|██████████████████████████████████████▉   | 2122/2290 periods | 00:16<00:00\u001b[A\n",
      " 93%|███████████████████████████████████████▎  | 2141/2290 periods | 00:16<00:00\u001b[A\n",
      " 94%|███████████████████████████████████████▌  | 2159/2290 periods | 00:16<00:00\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 2179/2290 periods | 00:16<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▎ | 2198/2290 periods | 00:17<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 2216/2290 periods | 00:17<00:00\u001b[A\n",
      " 98%|████████████████████████████████████████▉ | 2234/2290 periods | 00:17<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████▎| 2251/2290 periods | 00:17<00:00\u001b[A\n",
      " 99%|█████████████████████████████████████████▌| 2268/2290 periods | 00:17<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:17<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 99/9020 [00:00<00:09, 981.30it/s]\u001b[A\n",
      "  2%|▊                                      | 200/9020 [00:00<00:08, 980.31it/s]\u001b[A\n",
      "  3%|█▎                                     | 299/9020 [00:00<00:09, 939.37it/s]\u001b[A\n",
      "  5%|█▉                                    | 458/9020 [00:00<00:07, 1186.30it/s]\u001b[A\n",
      "  7%|██▌                                   | 616/9020 [00:00<00:06, 1323.08it/s]\u001b[A\n",
      "  9%|███▍                                  | 829/9020 [00:00<00:05, 1591.97it/s]\u001b[A\n",
      " 14%|█████                                | 1220/9020 [00:00<00:03, 2340.95it/s]\u001b[A\n",
      " 18%|██████▋                              | 1628/9020 [00:00<00:02, 2889.56it/s]\u001b[A\n",
      " 22%|████████▎                            | 2014/9020 [00:00<00:02, 3189.19it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2425/9020 [00:01<00:01, 3469.90it/s]\u001b[A\n",
      " 31%|███████████▍                         | 2801/9020 [00:01<00:01, 3558.19it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3163/9020 [00:01<00:01, 3576.88it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 3538/9020 [00:01<00:01, 3627.49it/s]\u001b[A\n",
      " 44%|████████████████                     | 3925/9020 [00:01<00:01, 3698.60it/s]\u001b[A\n",
      " 48%|█████████████████▌                   | 4296/9020 [00:01<00:01, 3649.92it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 4671/9020 [00:01<00:01, 3678.70it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5061/9020 [00:01<00:01, 3742.72it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 5436/9020 [00:01<00:00, 3649.88it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 5802/9020 [00:01<00:00, 3456.08it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 6155/9020 [00:02<00:00, 3475.99it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 6505/9020 [00:02<00:00, 3265.56it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 6903/9020 [00:02<00:00, 3463.18it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 7326/9020 [00:02<00:00, 3680.30it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 7747/9020 [00:02<00:00, 3831.68it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 8134/9020 [00:02<00:00, 3841.44it/s]\u001b[A\n",
      " 95%|███████████████████████████████████  | 8551/9020 [00:02<00:00, 3936.58it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:02<00:00, 3226.76it/s]\u001b[A\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/stats.py:296: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  theoretical_cadences = span / average_cadence\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 3 of 3 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=9.348442 d, SDE=7.84 (took 20.8s)\n",
      "[stitched] TLS refine around 13.24953 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 32114 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:02<1:45:42\u001b[A\n",
      "  0%|▏                                            | 9/2290 periods | 00:02<08:56\u001b[A\n",
      "  1%|▍                                           | 24/2290 periods | 00:02<02:45\u001b[A\n",
      "  2%|▊                                           | 40/2290 periods | 00:03<01:27\u001b[A\n",
      "  3%|█                                           | 58/2290 periods | 00:03<00:53\u001b[A\n",
      "  3%|█▍                                          | 75/2290 periods | 00:03<00:38\u001b[A\n",
      "  4%|█▊                                          | 94/2290 periods | 00:03<00:27\u001b[A\n",
      "  5%|██                                         | 113/2290 periods | 00:03<00:22\u001b[A\n",
      "  6%|██▍                                        | 130/2290 periods | 00:03<00:19\u001b[A\n",
      "  6%|██▊                                        | 147/2290 periods | 00:03<00:17\u001b[A\n",
      "  7%|███                                        | 166/2290 periods | 00:03<00:15\u001b[A\n",
      "  8%|███▍                                       | 183/2290 periods | 00:03<00:14\u001b[A\n",
      "  9%|███▊                                       | 203/2290 periods | 00:04<00:13\u001b[A\n",
      " 10%|████▏                                      | 221/2290 periods | 00:04<00:12\u001b[A\n",
      " 10%|████▍                                      | 239/2290 periods | 00:04<00:12\u001b[A\n",
      " 11%|████▊                                      | 258/2290 periods | 00:04<00:12\u001b[A\n",
      " 12%|█████▏                                     | 278/2290 periods | 00:04<00:12\u001b[A\n",
      " 13%|█████▌                                     | 297/2290 periods | 00:04<00:11\u001b[A\n",
      " 14%|█████▉                                     | 317/2290 periods | 00:04<00:11\u001b[A\n",
      " 15%|██████▎                                    | 336/2290 periods | 00:04<00:11\u001b[A\n",
      " 15%|██████▋                                    | 354/2290 periods | 00:04<00:11\u001b[A\n",
      " 16%|███████                                    | 375/2290 periods | 00:05<00:10\u001b[A\n",
      " 17%|███████▍                                   | 393/2290 periods | 00:05<00:10\u001b[A\n",
      " 18%|███████▋                                   | 411/2290 periods | 00:05<00:10\u001b[A\n",
      " 19%|████████                                   | 430/2290 periods | 00:05<00:10\u001b[A\n",
      " 20%|████████▍                                  | 450/2290 periods | 00:05<00:10\u001b[A\n",
      " 21%|████████▊                                  | 471/2290 periods | 00:05<00:10\u001b[A\n",
      " 21%|█████████▏                                 | 490/2290 periods | 00:05<00:09\u001b[A\n",
      " 22%|█████████▌                                 | 509/2290 periods | 00:05<00:09\u001b[A\n",
      " 23%|█████████▉                                 | 527/2290 periods | 00:05<00:09\u001b[A\n",
      " 24%|██████████▏                                | 545/2290 periods | 00:06<00:09\u001b[A\n",
      " 25%|██████████▌                                | 563/2290 periods | 00:06<00:09\u001b[A\n",
      " 25%|██████████▉                                | 581/2290 periods | 00:06<00:10\u001b[A\n",
      " 26%|███████████▎                               | 601/2290 periods | 00:06<00:09\u001b[A\n",
      " 27%|███████████▌                               | 619/2290 periods | 00:06<00:09\u001b[A\n",
      " 28%|███████████▉                               | 637/2290 periods | 00:06<00:09\u001b[A\n",
      " 29%|████████████▎                              | 655/2290 periods | 00:06<00:09\u001b[A\n",
      " 29%|████████████▋                              | 675/2290 periods | 00:06<00:09\u001b[A\n",
      " 30%|█████████████                              | 693/2290 periods | 00:06<00:10\u001b[A\n",
      " 31%|█████████████▎                             | 710/2290 periods | 00:07<00:09\u001b[A\n",
      " 32%|█████████████▋                             | 728/2290 periods | 00:07<00:09\u001b[A\n",
      " 33%|██████████████                             | 747/2290 periods | 00:07<00:09\u001b[A\n",
      " 33%|██████████████▎                            | 765/2290 periods | 00:07<00:09\u001b[A\n",
      " 34%|██████████████▊                            | 786/2290 periods | 00:07<00:08\u001b[A\n",
      " 35%|███████████████                            | 804/2290 periods | 00:07<00:08\u001b[A\n",
      " 36%|███████████████▍                           | 822/2290 periods | 00:07<00:08\u001b[A\n",
      " 37%|███████████████▊                           | 839/2290 periods | 00:07<00:08\u001b[A\n",
      " 38%|████████████████▏                          | 859/2290 periods | 00:07<00:08\u001b[A\n",
      " 38%|████████████████▍                          | 877/2290 periods | 00:07<00:08\u001b[A\n",
      " 39%|████████████████▊                          | 896/2290 periods | 00:08<00:08\u001b[A\n",
      " 40%|█████████████████▏                         | 914/2290 periods | 00:08<00:08\u001b[A\n",
      " 41%|█████████████████▍                         | 931/2290 periods | 00:08<00:08\u001b[A\n",
      " 41%|█████████████████▊                         | 950/2290 periods | 00:08<00:08\u001b[A\n",
      " 42%|██████████████████▏                        | 969/2290 periods | 00:08<00:07\u001b[A\n",
      " 43%|██████████████████▌                        | 988/2290 periods | 00:08<00:07\u001b[A\n",
      " 44%|██████████████████▍                       | 1006/2290 periods | 00:08<00:07\u001b[A\n",
      " 45%|██████████████████▊                       | 1026/2290 periods | 00:08<00:07\u001b[A\n",
      " 46%|███████████████████▏                      | 1045/2290 periods | 00:08<00:06\u001b[A\n",
      " 46%|███████████████████▍                      | 1063/2290 periods | 00:09<00:06\u001b[A\n",
      " 47%|███████████████████▊                      | 1081/2290 periods | 00:09<00:06\u001b[A\n",
      " 48%|████████████████████▏                     | 1100/2290 periods | 00:09<00:06\u001b[A\n",
      " 49%|████████████████████▌                     | 1118/2290 periods | 00:09<00:07\u001b[A\n",
      " 50%|████████████████████▊                     | 1135/2290 periods | 00:09<00:07\u001b[A\n",
      " 50%|█████████████████████▏                    | 1153/2290 periods | 00:09<00:06\u001b[A\n",
      " 51%|█████████████████████▍                    | 1171/2290 periods | 00:09<00:06\u001b[A\n",
      " 52%|█████████████████████▊                    | 1188/2290 periods | 00:09<00:06\u001b[A\n",
      " 53%|██████████████████████                    | 1206/2290 periods | 00:09<00:06\u001b[A\n",
      " 54%|██████████████████████▌                   | 1227/2290 periods | 00:10<00:06\u001b[A\n",
      " 54%|██████████████████████▊                   | 1245/2290 periods | 00:10<00:06\u001b[A\n",
      " 55%|███████████████████████▏                  | 1264/2290 periods | 00:10<00:05\u001b[A\n",
      " 56%|███████████████████████▌                  | 1282/2290 periods | 00:10<00:06\u001b[A\n",
      " 57%|███████████████████████▊                  | 1300/2290 periods | 00:10<00:05\u001b[A\n",
      " 58%|████████████████████████▏                 | 1320/2290 periods | 00:10<00:05\u001b[A\n",
      " 58%|████████████████████████▌                 | 1339/2290 periods | 00:10<00:05\u001b[A\n",
      " 59%|████████████████████████▉                 | 1357/2290 periods | 00:10<00:05\u001b[A\n",
      " 60%|█████████████████████████▏                | 1375/2290 periods | 00:10<00:05\u001b[A\n",
      " 61%|█████████████████████████▌                | 1396/2290 periods | 00:11<00:05\u001b[A\n",
      " 62%|█████████████████████████▉                | 1414/2290 periods | 00:11<00:05\u001b[A\n",
      " 63%|██████████████████████████▎               | 1432/2290 periods | 00:11<00:05\u001b[A\n",
      " 63%|██████████████████████████▌               | 1451/2290 periods | 00:11<00:04\u001b[A\n",
      " 64%|██████████████████████████▉               | 1471/2290 periods | 00:11<00:04\u001b[A\n",
      " 65%|███████████████████████████▎              | 1489/2290 periods | 00:11<00:04\u001b[A\n",
      " 66%|███████████████████████████▋              | 1507/2290 periods | 00:11<00:04\u001b[A\n",
      " 67%|███████████████████████████▉              | 1524/2290 periods | 00:11<00:04\u001b[A\n",
      " 67%|████████████████████████████▏             | 1540/2290 periods | 00:12<00:05\u001b[A\n",
      " 68%|████████████████████████████▌             | 1554/2290 periods | 00:12<00:06\u001b[A\n",
      " 68%|████████████████████████████▊             | 1568/2290 periods | 00:12<00:05\u001b[A\n",
      " 69%|████████████████████████████▉             | 1581/2290 periods | 00:12<00:05\u001b[A\n",
      " 70%|█████████████████████████████▎            | 1595/2290 periods | 00:12<00:05\u001b[A\n",
      " 70%|█████████████████████████████▌            | 1612/2290 periods | 00:12<00:05\u001b[A\n",
      " 71%|█████████████████████████████▉            | 1629/2290 periods | 00:12<00:04\u001b[A\n",
      " 72%|██████████████████████████████▏           | 1646/2290 periods | 00:12<00:04\u001b[A\n",
      " 73%|██████████████████████████████▌           | 1663/2290 periods | 00:12<00:04\u001b[A\n",
      " 73%|██████████████████████████████▊           | 1681/2290 periods | 00:12<00:03\u001b[A\n",
      " 74%|███████████████████████████████▏          | 1698/2290 periods | 00:13<00:03\u001b[A\n",
      " 75%|███████████████████████████████▍          | 1716/2290 periods | 00:13<00:03\u001b[A\n",
      " 76%|███████████████████████████████▊          | 1734/2290 periods | 00:13<00:03\u001b[A\n",
      " 77%|████████████████████████████████▏         | 1752/2290 periods | 00:13<00:03\u001b[A\n",
      " 77%|████████████████████████████████▍         | 1770/2290 periods | 00:13<00:03\u001b[A\n",
      " 78%|████████████████████████████████▊         | 1787/2290 periods | 00:13<00:03\u001b[A\n",
      " 79%|█████████████████████████████████         | 1806/2290 periods | 00:13<00:02\u001b[A\n",
      " 80%|█████████████████████████████████▍        | 1824/2290 periods | 00:13<00:02\u001b[A\n",
      " 80%|█████████████████████████████████▊        | 1841/2290 periods | 00:13<00:02\u001b[A\n",
      " 81%|██████████████████████████████████        | 1858/2290 periods | 00:14<00:02\u001b[A\n",
      " 82%|██████████████████████████████████▍       | 1875/2290 periods | 00:14<00:02\u001b[A\n",
      " 83%|██████████████████████████████████▋       | 1893/2290 periods | 00:14<00:02\u001b[A\n",
      " 83%|███████████████████████████████████       | 1910/2290 periods | 00:14<00:02\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 1927/2290 periods | 00:14<00:02\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 1944/2290 periods | 00:14<00:02\u001b[A\n",
      " 86%|███████████████████████████████████▉      | 1962/2290 periods | 00:14<00:01\u001b[A\n",
      " 86%|████████████████████████████████████▎     | 1979/2290 periods | 00:14<00:01\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 1996/2290 periods | 00:14<00:01\u001b[A\n",
      " 88%|████████████████████████████████████▉     | 2013/2290 periods | 00:15<00:01\u001b[A\n",
      " 89%|█████████████████████████████████████▏    | 2030/2290 periods | 00:15<00:01\u001b[A\n",
      " 90%|█████████████████████████████████████▌    | 2050/2290 periods | 00:15<00:01\u001b[A\n",
      " 90%|█████████████████████████████████████▉    | 2068/2290 periods | 00:15<00:01\u001b[A\n",
      " 91%|██████████████████████████████████████▎   | 2087/2290 periods | 00:15<00:01\u001b[A\n",
      " 92%|██████████████████████████████████████▌   | 2105/2290 periods | 00:15<00:01\u001b[A\n",
      " 93%|██████████████████████████████████████▉   | 2123/2290 periods | 00:15<00:00\u001b[A\n",
      " 93%|███████████████████████████████████████▏  | 2140/2290 periods | 00:15<00:00\u001b[A\n",
      " 94%|███████████████████████████████████████▌  | 2157/2290 periods | 00:15<00:00\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 2175/2290 periods | 00:15<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▏ | 2193/2290 periods | 00:16<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▌ | 2212/2290 periods | 00:16<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▉ | 2230/2290 periods | 00:16<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████▏| 2247/2290 periods | 00:16<00:00\u001b[A\n",
      " 99%|█████████████████████████████████████████▌| 2264/2290 periods | 00:16<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:16<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 94/9020 [00:00<00:09, 937.34it/s]\u001b[A\n",
      "  3%|█                                     | 255/9020 [00:00<00:06, 1277.98it/s]\u001b[A\n",
      "  5%|█▊                                    | 425/9020 [00:00<00:05, 1464.58it/s]\u001b[A\n",
      "  6%|██▍                                   | 580/9020 [00:00<00:05, 1493.46it/s]\u001b[A\n",
      "  8%|███                                   | 730/9020 [00:00<00:05, 1486.75it/s]\u001b[A\n",
      " 12%|████▍                                | 1092/9020 [00:00<00:03, 2202.35it/s]\u001b[A\n",
      " 16%|██████                               | 1465/9020 [00:00<00:02, 2697.52it/s]\u001b[A\n",
      " 20%|███████▍                             | 1825/9020 [00:00<00:02, 2982.30it/s]\u001b[A\n",
      " 25%|█████████                            | 2211/9020 [00:00<00:02, 3251.35it/s]\u001b[A\n",
      " 29%|██████████▌                          | 2589/9020 [00:01<00:01, 3413.61it/s]\u001b[A\n",
      " 33%|████████████▎                        | 2989/9020 [00:01<00:01, 3591.74it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3381/9020 [00:01<00:01, 3689.36it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 3760/9020 [00:01<00:01, 3719.10it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4142/9020 [00:01<00:01, 3747.41it/s]\u001b[A\n",
      " 50%|██████████████████▋                  | 4548/9020 [00:01<00:01, 3839.61it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 4947/9020 [00:01<00:01, 3883.18it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 5336/9020 [00:01<00:00, 3861.01it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 5723/9020 [00:01<00:00, 3828.27it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 6106/9020 [00:01<00:00, 3779.42it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 6485/9020 [00:02<00:00, 3774.89it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 6873/9020 [00:02<00:00, 3805.09it/s]\u001b[A\n",
      " 80%|█████████████████████████████▊       | 7254/9020 [00:02<00:00, 3676.73it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 7658/9020 [00:02<00:00, 3780.78it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 8072/9020 [00:02<00:00, 3885.65it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 8491/9020 [00:02<00:00, 3974.37it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:02<00:00, 3351.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=9.348442 d, SDE=7.84 (took 19.8s)\n",
      "[stitched] TLS refine around 12.10758 d (±1.0%) …\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 37 durations\n",
      "Searching 32114 data points, 2290 periods from 0.601 to 12.715 days\n",
      "Using all 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/2290 periods | 00:00<?\u001b[A\n",
      "  0%|                                           | 1/2290 periods | 00:03<2:06:07\u001b[A\n",
      "  0%|                                             | 5/2290 periods | 00:03<19:36\u001b[A\n",
      "  1%|▍                                           | 20/2290 periods | 00:03<03:42\u001b[A\n",
      "  1%|▋                                           | 34/2290 periods | 00:03<01:54\u001b[A\n",
      "  2%|▉                                           | 51/2290 periods | 00:03<01:06\u001b[A\n",
      "  3%|█▎                                          | 67/2290 periods | 00:03<00:45\u001b[A\n",
      "  4%|█▋                                          | 86/2290 periods | 00:03<00:32\u001b[A\n",
      "  4%|█▉                                         | 101/2290 periods | 00:04<00:27\u001b[A\n",
      "  5%|██▏                                        | 118/2290 periods | 00:04<00:22\u001b[A\n",
      "  6%|██▌                                        | 134/2290 periods | 00:04<00:20\u001b[A\n",
      "  7%|██▊                                        | 149/2290 periods | 00:04<00:19\u001b[A\n",
      "  7%|███                                        | 166/2290 periods | 00:04<00:17\u001b[A\n",
      "  8%|███▍                                       | 181/2290 periods | 00:04<00:17\u001b[A\n",
      "  9%|███▋                                       | 195/2290 periods | 00:04<00:17\u001b[A\n",
      "  9%|███▉                                       | 209/2290 periods | 00:04<00:16\u001b[A\n",
      " 10%|████▏                                      | 224/2290 periods | 00:04<00:15\u001b[A\n",
      " 11%|████▌                                      | 242/2290 periods | 00:05<00:14\u001b[A\n",
      " 11%|████▊                                      | 259/2290 periods | 00:05<00:13\u001b[A\n",
      " 12%|█████▏                                     | 278/2290 periods | 00:05<00:12\u001b[A\n",
      " 13%|█████▌                                     | 295/2290 periods | 00:05<00:12\u001b[A\n",
      " 14%|█████▊                                     | 312/2290 periods | 00:05<00:12\u001b[A\n",
      " 14%|██████▏                                    | 329/2290 periods | 00:05<00:12\u001b[A\n",
      " 15%|██████▍                                    | 346/2290 periods | 00:05<00:12\u001b[A\n",
      " 16%|██████▊                                    | 362/2290 periods | 00:05<00:13\u001b[A\n",
      " 16%|███████                                    | 377/2290 periods | 00:05<00:13\u001b[A\n",
      " 17%|███████▎                                   | 391/2290 periods | 00:06<00:14\u001b[A\n",
      " 18%|███████▋                                   | 409/2290 periods | 00:06<00:13\u001b[A\n",
      " 19%|███████▉                                   | 425/2290 periods | 00:06<00:12\u001b[A\n",
      " 19%|████████▎                                  | 440/2290 periods | 00:06<00:14\u001b[A\n",
      " 20%|████████▌                                  | 454/2290 periods | 00:06<00:15\u001b[A\n",
      " 21%|████████▊                                  | 471/2290 periods | 00:06<00:14\u001b[A\n",
      " 21%|█████████▏                                 | 488/2290 periods | 00:06<00:13\u001b[A\n",
      " 22%|█████████▌                                 | 507/2290 periods | 00:06<00:12\u001b[A\n",
      " 23%|█████████▉                                 | 526/2290 periods | 00:07<00:11\u001b[A\n",
      " 24%|██████████▏                                | 542/2290 periods | 00:07<00:11\u001b[A\n",
      " 25%|██████████▌                                | 562/2290 periods | 00:07<00:10\u001b[A\n",
      " 25%|██████████▉                                | 580/2290 periods | 00:07<00:10\u001b[A\n",
      " 26%|███████████▏                               | 598/2290 periods | 00:07<00:10\u001b[A\n",
      " 27%|███████████▌                               | 615/2290 periods | 00:07<00:10\u001b[A\n",
      " 28%|███████████▊                               | 632/2290 periods | 00:07<00:09\u001b[A\n",
      " 28%|████████████▏                              | 650/2290 periods | 00:07<00:09\u001b[A\n",
      " 29%|████████████▌                              | 670/2290 periods | 00:07<00:09\u001b[A\n",
      " 30%|████████████▉                              | 688/2290 periods | 00:07<00:09\u001b[A\n",
      " 31%|█████████████▎                             | 708/2290 periods | 00:08<00:09\u001b[A\n",
      " 32%|█████████████▋                             | 726/2290 periods | 00:08<00:09\u001b[A\n",
      " 33%|██████████████                             | 746/2290 periods | 00:08<00:08\u001b[A\n",
      " 33%|██████████████▎                            | 765/2290 periods | 00:08<00:08\u001b[A\n",
      " 34%|██████████████▋                            | 784/2290 periods | 00:08<00:08\u001b[A\n",
      " 35%|███████████████                            | 803/2290 periods | 00:08<00:08\u001b[A\n",
      " 36%|███████████████▍                           | 822/2290 periods | 00:08<00:08\u001b[A\n",
      " 37%|███████████████▊                           | 840/2290 periods | 00:08<00:09\u001b[A\n",
      " 37%|████████████████                           | 858/2290 periods | 00:08<00:09\u001b[A\n",
      " 38%|████████████████▍                          | 876/2290 periods | 00:09<00:08\u001b[A\n",
      " 39%|████████████████▊                          | 897/2290 periods | 00:09<00:08\u001b[A\n",
      " 40%|█████████████████▏                         | 915/2290 periods | 00:09<00:07\u001b[A\n",
      " 41%|█████████████████▌                         | 935/2290 periods | 00:09<00:07\u001b[A\n",
      " 42%|█████████████████▉                         | 954/2290 periods | 00:09<00:08\u001b[A\n",
      " 42%|██████████████████▏                        | 971/2290 periods | 00:09<00:08\u001b[A\n",
      " 43%|██████████████████▌                        | 988/2290 periods | 00:09<00:08\u001b[A\n",
      " 44%|██████████████████▍                       | 1008/2290 periods | 00:09<00:07\u001b[A\n",
      " 45%|██████████████████▊                       | 1025/2290 periods | 00:09<00:07\u001b[A\n",
      " 46%|███████████████████▏                      | 1045/2290 periods | 00:10<00:07\u001b[A\n",
      " 47%|███████████████████▌                      | 1065/2290 periods | 00:10<00:07\u001b[A\n",
      " 47%|███████████████████▉                      | 1084/2290 periods | 00:10<00:06\u001b[A\n",
      " 48%|████████████████████▏                     | 1102/2290 periods | 00:10<00:07\u001b[A\n",
      " 49%|████████████████████▌                     | 1119/2290 periods | 00:10<00:08\u001b[A\n",
      " 50%|████████████████████▊                     | 1134/2290 periods | 00:10<00:08\u001b[A\n",
      " 50%|█████████████████████                     | 1148/2290 periods | 00:10<00:09\u001b[A\n",
      " 51%|█████████████████████▎                    | 1161/2290 periods | 00:10<00:09\u001b[A\n",
      " 51%|█████████████████████▌                    | 1174/2290 periods | 00:11<00:11\u001b[A\n",
      " 52%|█████████████████████▋                    | 1185/2290 periods | 00:11<00:11\u001b[A\n",
      " 52%|█████████████████████▉                    | 1197/2290 periods | 00:11<00:10\u001b[A\n",
      " 53%|██████████████████████▎                   | 1214/2290 periods | 00:11<00:09\u001b[A\n",
      " 54%|██████████████████████▌                   | 1228/2290 periods | 00:11<00:08\u001b[A\n",
      " 54%|██████████████████████▊                   | 1241/2290 periods | 00:11<00:08\u001b[A\n",
      " 55%|███████████████████████                   | 1257/2290 periods | 00:11<00:08\u001b[A\n",
      " 55%|███████████████████████▎                  | 1270/2290 periods | 00:11<00:08\u001b[A\n",
      " 56%|███████████████████████▌                  | 1285/2290 periods | 00:12<00:07\u001b[A\n",
      " 57%|███████████████████████▊                  | 1301/2290 periods | 00:12<00:07\u001b[A\n",
      " 58%|████████████████████████▏                 | 1318/2290 periods | 00:12<00:06\u001b[A\n",
      " 58%|████████████████████████▍                 | 1335/2290 periods | 00:12<00:06\u001b[A\n",
      " 59%|████████████████████████▊                 | 1354/2290 periods | 00:12<00:06\u001b[A\n",
      " 60%|█████████████████████████▏                | 1373/2290 periods | 00:12<00:05\u001b[A\n",
      " 61%|█████████████████████████▍                | 1390/2290 periods | 00:12<00:06\u001b[A\n",
      " 61%|█████████████████████████▊                | 1406/2290 periods | 00:12<00:06\u001b[A\n",
      " 62%|██████████████████████████                | 1421/2290 periods | 00:12<00:06\u001b[A\n",
      " 63%|██████████████████████████▎               | 1436/2290 periods | 00:13<00:06\u001b[A\n",
      " 63%|██████████████████████████▌               | 1451/2290 periods | 00:13<00:06\u001b[A\n",
      " 64%|██████████████████████████▉               | 1466/2290 periods | 00:13<00:06\u001b[A\n",
      " 65%|███████████████████████████▏              | 1480/2290 periods | 00:13<00:07\u001b[A\n",
      " 65%|███████████████████████████▎              | 1492/2290 periods | 00:13<00:07\u001b[A\n",
      " 66%|███████████████████████████▋              | 1507/2290 periods | 00:13<00:06\u001b[A\n",
      " 66%|███████████████████████████▉              | 1522/2290 periods | 00:13<00:06\u001b[A\n",
      " 67%|████████████████████████████▏             | 1539/2290 periods | 00:13<00:05\u001b[A\n",
      " 68%|████████████████████████████▌             | 1560/2290 periods | 00:14<00:04\u001b[A\n",
      " 69%|████████████████████████████▉             | 1576/2290 periods | 00:14<00:04\u001b[A\n",
      " 70%|█████████████████████████████▎            | 1596/2290 periods | 00:14<00:04\u001b[A\n",
      " 70%|█████████████████████████████▌            | 1613/2290 periods | 00:14<00:04\u001b[A\n",
      " 71%|█████████████████████████████▉            | 1631/2290 periods | 00:14<00:03\u001b[A\n",
      " 72%|██████████████████████████████▎           | 1650/2290 periods | 00:14<00:03\u001b[A\n",
      " 73%|██████████████████████████████▌           | 1669/2290 periods | 00:14<00:03\u001b[A\n",
      " 74%|██████████████████████████████▉           | 1687/2290 periods | 00:14<00:03\u001b[A\n",
      " 74%|███████████████████████████████▎          | 1706/2290 periods | 00:14<00:03\u001b[A\n",
      " 75%|███████████████████████████████▋          | 1725/2290 periods | 00:15<00:03\u001b[A\n",
      " 76%|███████████████████████████████▉          | 1744/2290 periods | 00:15<00:03\u001b[A\n",
      " 77%|████████████████████████████████▎         | 1762/2290 periods | 00:15<00:03\u001b[A\n",
      " 78%|████████████████████████████████▋         | 1780/2290 periods | 00:15<00:02\u001b[A\n",
      " 79%|████████████████████████████████▉         | 1798/2290 periods | 00:15<00:02\u001b[A\n",
      " 79%|█████████████████████████████████▎        | 1817/2290 periods | 00:15<00:02\u001b[A\n",
      " 80%|█████████████████████████████████▋        | 1835/2290 periods | 00:15<00:02\u001b[A\n",
      " 81%|█████████████████████████████████▉        | 1853/2290 periods | 00:15<00:02\u001b[A\n",
      " 82%|██████████████████████████████████▎       | 1871/2290 periods | 00:15<00:02\u001b[A\n",
      " 82%|██████████████████████████████████▋       | 1889/2290 periods | 00:15<00:02\u001b[A\n",
      " 83%|██████████████████████████████████▉       | 1907/2290 periods | 00:16<00:02\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 1925/2290 periods | 00:16<00:02\u001b[A\n",
      " 85%|███████████████████████████████████▌      | 1942/2290 periods | 00:16<00:02\u001b[A\n",
      " 86%|███████████████████████████████████▉      | 1959/2290 periods | 00:16<00:02\u001b[A\n",
      " 86%|████████████████████████████████████▎     | 1977/2290 periods | 00:16<00:01\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 1996/2290 periods | 00:16<00:01\u001b[A\n",
      " 88%|████████████████████████████████████▉     | 2016/2290 periods | 00:16<00:01\u001b[A\n",
      " 89%|█████████████████████████████████████▎    | 2034/2290 periods | 00:16<00:01\u001b[A\n",
      " 90%|█████████████████████████████████████▌    | 2051/2290 periods | 00:16<00:01\u001b[A\n",
      " 90%|██████████████████████████████████████    | 2072/2290 periods | 00:17<00:01\u001b[A\n",
      " 91%|██████████████████████████████████████▎   | 2090/2290 periods | 00:17<00:01\u001b[A\n",
      " 92%|██████████████████████████████████████▋   | 2107/2290 periods | 00:17<00:01\u001b[A\n",
      " 93%|██████████████████████████████████████▉   | 2124/2290 periods | 00:17<00:01\u001b[A\n",
      " 93%|███████████████████████████████████████▎  | 2141/2290 periods | 00:17<00:00\u001b[A\n",
      " 94%|███████████████████████████████████████▌  | 2160/2290 periods | 00:17<00:00\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 2178/2290 periods | 00:17<00:00\u001b[A\n",
      " 96%|████████████████████████████████████████▎ | 2198/2290 periods | 00:17<00:00\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 2217/2290 periods | 00:17<00:00\u001b[A\n",
      " 98%|████████████████████████████████████████▉ | 2235/2290 periods | 00:18<00:00\u001b[A\n",
      " 98%|█████████████████████████████████████████▎| 2252/2290 periods | 00:18<00:00\u001b[A\n",
      " 99%|█████████████████████████████████████████▋| 2270/2290 periods | 00:18<00:00\u001b[A\n",
      "100%|██████████████████████████████████████████| 2290/2290 periods | 00:18<00:00\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 9.34844 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/9020 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                       | 35/9020 [00:00<00:25, 349.83it/s]\u001b[A\n",
      "  1%|▎                                       | 84/9020 [00:00<00:21, 420.07it/s]\u001b[A\n",
      "  3%|▉                                      | 227/9020 [00:00<00:10, 871.61it/s]\u001b[A\n",
      "  3%|█▎                                     | 315/9020 [00:00<00:10, 831.23it/s]\u001b[A\n",
      "  5%|█▉                                    | 452/9020 [00:00<00:08, 1009.71it/s]\u001b[A\n",
      "  7%|██▊                                   | 658/9020 [00:00<00:06, 1353.78it/s]\u001b[A\n",
      "  9%|███▍                                  | 825/9020 [00:00<00:05, 1454.36it/s]\u001b[A\n",
      " 13%|████▉                                | 1210/9020 [00:00<00:03, 2203.97it/s]\u001b[A\n",
      " 18%|██████▌                              | 1604/9020 [00:00<00:02, 2740.62it/s]\u001b[A\n",
      " 22%|████████▏                            | 1984/9020 [00:01<00:02, 3063.14it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2357/9020 [00:01<00:02, 3263.98it/s]\u001b[A\n",
      " 30%|███████████▎                         | 2748/9020 [00:01<00:01, 3459.31it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3133/9020 [00:01<00:01, 3576.73it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 3531/9020 [00:01<00:01, 3698.22it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 3935/9020 [00:01<00:01, 3800.95it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 4317/9020 [00:01<00:01, 3806.14it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 4730/9020 [00:01<00:01, 3902.25it/s]\u001b[A\n",
      " 57%|█████████████████████                | 5121/9020 [00:01<00:01, 3870.27it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 5522/9020 [00:01<00:00, 3910.47it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 5914/9020 [00:02<00:00, 3880.27it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 6322/9020 [00:02<00:00, 3937.90it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 6716/9020 [00:02<00:00, 3930.34it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 7132/9020 [00:02<00:00, 3997.99it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 7533/9020 [00:02<00:00, 4000.43it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 7963/9020 [00:02<00:00, 4087.76it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 8372/9020 [00:02<00:00, 4077.79it/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 9020/9020 [00:02<00:00, 3235.38it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stitched]   TLS best P=9.348442 d, SDE=7.84 (took 21.7s)\n",
      "\n",
      "Done for Target C — artifacts in figures/ and results/.\n"
     ]
    }
   ],
   "source": [
    "# --- Run per-sector and stitched on Target C (quiet TLS; uses local-first loader) ---\n",
    "import os as _os, numpy as np, time\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "# Disable any tqdm-like bars globally (belt & suspenders)\n",
    "_os.environ.setdefault(\"TQDM_DISABLE\", \"1\")\n",
    "\n",
    "# ===== Helpers (define only if missing) =====\n",
    "try:\n",
    "    clean_and_detrend\n",
    "except NameError:\n",
    "    import numpy as _np\n",
    "    def _mad(a):\n",
    "        med = _np.nanmedian(a); return 1.4826*_np.nanmedian(_np.abs(a-med))\n",
    "    def clean_and_detrend(t, f, window_days=0.75, sigma=5.0):\n",
    "        m = _np.isfinite(t)&_np.isfinite(f); t,f = _np.asarray(t)[m], _np.asarray(f)[m]\n",
    "        med, s = _np.nanmedian(f), _mad(f)\n",
    "        if not _np.isfinite(s) or s==0: s=1e9\n",
    "        keep = _np.abs(f-med) < sigma*s; t,f = t[keep], f[keep]\n",
    "        if t.size < 20:\n",
    "            return t, f/_np.nanmedian(f)\n",
    "        step = max(window_days/2, (t.max()-t.min())/200)\n",
    "        grid = _np.arange(t.min()-step, t.max()+step, step)\n",
    "        trend = []\n",
    "        for g in grid:\n",
    "            sel = (t>=g-window_days/2)&(t<g+window_days/2)\n",
    "            trend.append(_np.nanmedian(f[sel]) if sel.any() else _np.nan)\n",
    "        grid = grid[_np.isfinite(trend)]\n",
    "        trend = _np.interp(t, grid, _np.array(trend)[_np.isfinite(trend)])\n",
    "        trend = _np.where(_np.isfinite(trend)&(trend>0), trend, _np.nanmedian(f))\n",
    "        ff = f/trend; ff = ff/_np.nanmedian(ff)\n",
    "        return t, ff\n",
    "\n",
    "try:\n",
    "    prep_arrays_for_search\n",
    "except NameError:\n",
    "    def prep_arrays_for_search(lc):\n",
    "        t0 = getattr(lc.time, \"value\", lc.time)\n",
    "        f0 = getattr(lc.flux, \"value\", lc.flux)\n",
    "        return clean_and_detrend(np.asarray(t0, float), np.asarray(f0, float))\n",
    "\n",
    "try:\n",
    "    unique_peaks\n",
    "except NameError:\n",
    "    def unique_peaks(periods, power, k=3, tol_frac=0.01):\n",
    "        idx = np.argsort(power)[::-1]; picks=[]\n",
    "        for i in idx:\n",
    "            p = periods[i]\n",
    "            if all(abs(p-q)/q > tol_frac for q in picks): picks.append(p)\n",
    "            if len(picks)==k: break\n",
    "        return picks\n",
    "\n",
    "try:\n",
    "    bls_power_safe\n",
    "except NameError:\n",
    "    def bls_power_safe(t, f, periods, durations):\n",
    "        bls = BoxLeastSquares(t, f)\n",
    "        try: return bls.power(periods, durations, objective=\"snr\")\n",
    "        except TypeError: return bls.power(periods, durations)\n",
    "\n",
    "# Always define a QUIET TLS narrow search (ignores any earlier tls_narrow)\n",
    "R_STAR, M_STAR = 0.55, 0.55\n",
    "def tls_narrow_quiet(t, f, p_center, frac=0.01, nthreads=max(1, (_os.cpu_count() or 1)), nmin=3):\n",
    "    tls = transitleastsquares(t, f)\n",
    "    pmin, pmax = p_center*(1-frac), p_center*(1+frac)\n",
    "    try:\n",
    "        res = tls.power(period_min=pmin, period_max=pmax,\n",
    "                        show_progress_bar=False,  # <- no bar\n",
    "                        use_threads=int(nthreads),\n",
    "                        n_transits_min=int(nmin),\n",
    "                        R_star=R_STAR, M_star=M_STAR)\n",
    "    except ValueError:\n",
    "        res = tls.power(period_min=pmin, period_max=pmax,\n",
    "                        show_progress_bar=False,\n",
    "                        use_threads=1,\n",
    "                        n_transits_min=int(nmin),\n",
    "                        R_star=R_STAR, M_star=M_STAR)\n",
    "    return float(res.period), float(res.SDE), float(res.T0), res\n",
    "\n",
    "try:\n",
    "    plot_periodogram, fold_and_plot, append_csv, run_block\n",
    "except NameError:\n",
    "    # Fallback lightweight plot/save utilities\n",
    "    import csv, matplotlib.pyplot as plt\n",
    "    FIGDIR, RESDIR = \"figures\", \"results\"\n",
    "    os.makedirs(FIGDIR, exist_ok=True); os.makedirs(RESDIR, exist_ok=True)\n",
    "    def append_csv(path, rows, header=None):\n",
    "        new = not os.path.exists(path)\n",
    "        with open(path, \"a\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            if new and header: w.writerow(header)\n",
    "            for r in rows: w.writerow(r)\n",
    "    def plot_periodogram(x, y, xlabel, title, outpng):\n",
    "        plt.figure(figsize=(8,4), dpi=140); plt.plot(x, y, lw=0.7)\n",
    "        plt.xlabel(xlabel); plt.ylabel(\"Power\"); plt.title(title)\n",
    "        plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "    def fold_and_plot(t, f, period, t0, title, outpng, nbins=200):\n",
    "        ph = ((t - t0 + 0.5*period) % period) / period - 0.5\n",
    "        o = np.argsort(ph); ph, f = ph[o], f[o]\n",
    "        bins = np.linspace(-0.5, 0.5, nbins+1); idx = np.digitize(ph, bins)-1\n",
    "        xb = 0.5*(bins[:-1]+bins[1:])\n",
    "        yb = np.array([np.nanmean(f[idx==i]) if np.any(idx==i) else np.nan for i in range(nbins)])\n",
    "        plt.figure(figsize=(8,4), dpi=140)\n",
    "        plt.plot(ph, f, \".\", ms=1.5, alpha=0.25); plt.plot(xb, yb, \"-\", lw=1.5)\n",
    "        plt.axvline(0, color=\"k\", lw=1, alpha=0.3)\n",
    "        plt.xlabel(\"Phase\"); plt.ylabel(\"Rel. flux\"); plt.title(title)\n",
    "        plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "    # Search settings + runner\n",
    "    TLS_THREADS = max(1, (_os.cpu_count() or 1))\n",
    "    BLS_PERIOD_MIN, BLS_PERIOD_MAX, BLS_NPER = 0.5, 50.0, 5000\n",
    "    BLS_DURATIONS_HR = np.linspace(0.5, 3.0, 18)\n",
    "    TLS_WINDOW_FRAC, TLS_MIN_TRANSITS = 0.01, 3\n",
    "\n",
    "    def run_block(label, t, f, target_tic, target_toi):\n",
    "        print(f\"\\n[{label}] points={t.size}  threads={TLS_THREADS}\")\n",
    "        # cap per-sector BLS so ≥ n transits\n",
    "        span = float(np.nanmax(t) - np.nanmin(t))\n",
    "        bls_cap = min(BLS_PERIOD_MAX, max(BLS_PERIOD_MIN*1.2, 0.90*span))\n",
    "        if label.startswith(\"S\") and TLS_MIN_TRANSITS >= 2:\n",
    "            bls_cap = min(bls_cap, 0.95 * span / (TLS_MIN_TRANSITS - 1))\n",
    "            print(f\"[{label}] with nmin={TLS_MIN_TRANSITS}, periods ≲ {0.95*span/(TLS_MIN_TRANSITS-1):.2f} d have ≥{TLS_MIN_TRANSITS} transits\")\n",
    "        periods = np.linspace(BLS_PERIOD_MIN, bls_cap, BLS_NPER)\n",
    "        durations = BLS_DURATIONS_HR / 24.0\n",
    "\n",
    "        t0 = time.time()\n",
    "        bls_res = bls_power_safe(t, f, periods, durations)\n",
    "        print(f\"[{label}] BLS done in {time.time()-t0:.1f}s (Pmax used={bls_cap:.2f} d)\")\n",
    "\n",
    "        plot_periodogram(\n",
    "            bls_res.period, bls_res.power, \"Period (days)\",\n",
    "            f\"{target_toi} ({label}) — BLS periodogram\",\n",
    "            f\"{FIGDIR}/TIC{target_tic}_{label}_BLS_periodogram.png\"\n",
    "        )\n",
    "\n",
    "        bls_topP = unique_peaks(bls_res.period, bls_res.power, k=3, tol_frac=0.01)\n",
    "        append_csv(\n",
    "            f\"{RESDIR}/TIC{target_tic}_{label}_BLS_top3.csv\",\n",
    "            [[target_tic, target_toi, label, float(p),\n",
    "              float(bls_res.power[np.argmin(np.abs(bls_res.period-p))])] for p in bls_topP],\n",
    "            header=[\"tic\",\"toi\",\"label\",\"period_days\",\"power\"]\n",
    "        )\n",
    "\n",
    "        # TLS around BLS peaks (QUIET)\n",
    "        tls_rows = []\n",
    "        for p in bls_topP:\n",
    "            print(f\"[{label}] TLS refine around {p:.5f} d (±{TLS_WINDOW_FRAC*100:.1f}%) …\")\n",
    "            p_best, sde, t0_best, res = tls_narrow_quiet(\n",
    "                t, f, p, frac=TLS_WINDOW_FRAC,\n",
    "                nthreads=TLS_THREADS,\n",
    "                nmin=(3 if label.startswith(\"S\") else TLS_MIN_TRANSITS)\n",
    "            )\n",
    "            print(f\"[{label}]   TLS best P={p_best:.6f} d, SDE={sde:.2f}\")\n",
    "            plot_periodogram(\n",
    "                res.periods, res.power, \"Period (days)\",\n",
    "                f\"{target_toi} ({label}) — TLS @ {p:.5f}±{TLS_WINDOW_FRAC*100:.1f}%\",\n",
    "                f\"{FIGDIR}/TIC{target_tic}_{label}_TLS_periodogram_around_{p:.5f}.png\"\n",
    "            )\n",
    "            fold_and_plot(\n",
    "                t, f, p_best, t0_best,\n",
    "                f\"{target_toi} ({label}) — TLS fold @ P={p_best:.5f} d\",\n",
    "                f\"{FIGDIR}/TIC{target_tic}_{label}_TLS_fold_P{p_best:.5f}.png\"\n",
    "            )\n",
    "            tls_rows.append([target_tic, target_toi, label, p_best, sde, t0_best])\n",
    "\n",
    "        append_csv(\n",
    "            f\"{RESDIR}/TIC{target_tic}_{label}_TLS_top3.csv\",\n",
    "            tls_rows, header=[\"tic\",\"toi\",\"label\",\"period_days\",\"SDE\",\"T0_BTJD\"]\n",
    "        )\n",
    "\n",
    "# ===== Run per-sector then stitched =====\n",
    "assert SECTORS, \"SECTORS is empty — run the discovery cell first.\"\n",
    "\n",
    "t_all_list, f_all_list = [], []\n",
    "for s in SECTORS:\n",
    "    # NOTE: no .normalize() here; loader already normalizes\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, s)\n",
    "    t, f = prep_arrays_for_search(lc)\n",
    "    print(f\"{TARGET_TOI} — S{s}: N={t.size}\")\n",
    "    run_block(f\"S{s}\", t, f, TARGET_TIC, TARGET_TOI)\n",
    "    t_all_list.append(t); f_all_list.append(f)\n",
    "\n",
    "t_all = np.concatenate(t_all_list); f_all = np.concatenate(f_all_list)\n",
    "order = np.argsort(t_all); t_all, f_all = t_all[order], f_all[order]\n",
    "run_block(\"stitched\", t_all, f_all, TARGET_TIC, TARGET_TOI)\n",
    "\n",
    "print(\"\\nDone for Target C — artifacts in figures/ and results/.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18cbb785-278a-4651-b522-30318ef56431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best stitched TLS: P=9.348442 d, T0(BTJD)=2149.63307\n",
      "Odd-even: depth_even=93 ppm, depth_odd=161 ppm, Δ=-69 ppm (~-1.32σ)\n",
      "\n",
      "Checking at 2×P…\n",
      "Odd-even: depth_even=47 ppm, depth_odd=-345 ppm, Δ=392 ppm (~10.44σ)\n",
      "\n",
      "Saved ephemeris → results/TIC311183180_ephemeris.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os\n",
    "\n",
    "# --- Load best stitched TLS P,T0 ---\n",
    "tls_csv = \"results/TIC311183180_stitched_TLS_top3.csv\"  # adjust if your path differs\n",
    "tls = pd.read_csv(tls_csv).sort_values(\"SDE\", ascending=False)\n",
    "P = float(tls.iloc[0][\"period_days\"])\n",
    "T0 = float(tls.iloc[0][\"T0_BTJD\"])\n",
    "print(f\"Best stitched TLS: P={P:.6f} d, T0(BTJD)={T0:.5f}\")\n",
    "\n",
    "# --- Odd-even depth test (uses your stitched arrays from earlier: t_all, f_all) ---\n",
    "def odd_even_test(t, f, period, t0, width_frac=0.06, out=None):\n",
    "    # width_frac ~ fraction of period to call \"in-transit\" (adjust 0.03–0.08 as needed)\n",
    "    phase = ((t - t0) % period) / period\n",
    "    # center transit at phase ~0 by shifting\n",
    "    phase = (phase + 0.5) % 1 - 0.5\n",
    "    w = width_frac/2\n",
    "    in_tr = np.abs(phase) < w\n",
    "    if not np.any(in_tr):\n",
    "        raise RuntimeError(\"No in-transit points found — widen width_frac.\")\n",
    "\n",
    "    # number transits by nearest integer epoch\n",
    "    k = np.round((t - t0)/period).astype(int)\n",
    "    ke, ko = (k % 2 == 0), (k % 2 != 0)\n",
    "    # in-transit & even/odd masks\n",
    "    ine, ino = in_tr & ke, in_tr & ko\n",
    "\n",
    "    d_even = 1 - np.nanmedian(f[ine])\n",
    "    d_odd  = 1 - np.nanmedian(f[ino])\n",
    "    # simple uncertainty ~ MAD/sqrt(N)\n",
    "    def mad(x): \n",
    "        m = np.nanmedian(x); return 1.4826*np.nanmedian(np.abs(x-m))\n",
    "    se = mad(f[ine])/np.sqrt(np.sum(np.isfinite(f[ine])) + 1e-9)\n",
    "    so = mad(f[ino])/np.sqrt(np.sum(np.isfinite(f[ino])) + 1e-9)\n",
    "\n",
    "    diff_ppm = (d_even - d_odd)*1e6\n",
    "    sig = diff_ppm / (np.hypot(se, so)*1e6 + 1e-12)\n",
    "    print(f\"Odd-even: depth_even={d_even*1e6:.0f} ppm, depth_odd={d_odd*1e6:.0f} ppm, Δ={diff_ppm:.0f} ppm (~{sig:.2f}σ)\")\n",
    "\n",
    "    if out:\n",
    "        with open(out, \"w\") as fsum:\n",
    "            fsum.write(f\"P={period:.6f} d  T0={t0:.5f} BTJD\\n\")\n",
    "            fsum.write(f\"Odd-even: even={d_even*1e6:.0f} ppm  odd={d_odd*1e6:.0f} ppm  Δ={diff_ppm:.0f} ppm (~{sig:.2f}σ)\\n\")\n",
    "\n",
    "# run on your stitched arrays from the previous cell\n",
    "odd_even_test(t_all, f_all, P, T0, width_frac=0.06)\n",
    "\n",
    "# --- 2×P sanity (some EBs show at double the period) ---\n",
    "print(\"\\nChecking at 2×P…\")\n",
    "odd_even_test(t_all, f_all, 2*P, T0, width_frac=0.06)\n",
    "\n",
    "# --- Ephemeris table (next 50 events starting near the stitched time span) ---\n",
    "tmin, tmax = float(np.nanmin(t_all)), float(np.nanmax(t_all))\n",
    "n0 = int(np.floor((tmin - T0)/P))\n",
    "events = []\n",
    "for n in range(n0, n0+200):     # generate a bunch and then filter into range or future\n",
    "    ttran = T0 + n*P\n",
    "    if ttran >= tmin - P and len(events) < 50:\n",
    "        events.append([n, ttran])\n",
    "\n",
    "ephem = pd.DataFrame(events, columns=[\"epoch\", \"Tmid_BTJD\"])\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "ephem_path = \"results/TIC311183180_ephemeris.csv\"\n",
    "ephem.to_csv(ephem_path, index=False)\n",
    "print(f\"\\nSaved ephemeris → {ephem_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d9c86ad-e96b-46d0-8678-bc4773601ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended both P and 2×P entries to results/TIC311183180_ephemeris.csv\n"
     ]
    }
   ],
   "source": [
    "import csv, os\n",
    "tic = 311183180\n",
    "ephem_csv = f\"results/TIC{tic}_ephemeris.csv\"\n",
    "\n",
    "P1 = 9.348442\n",
    "T0 = 2149.63307\n",
    "P2 = 2*P1\n",
    "\n",
    "rows = [\n",
    "    [tic, P1,  T0, \"stitched_TLS\", \"alias_half\", \"odd_even_sigma_at_2xP=10.44\"],\n",
    "    [tic, P2,  T0, \"stitched_TLS\", \"adopted_for_vetting_EB_likely\", \"odd_even_sigma=10.44\"],\n",
    "]\n",
    "\n",
    "newfile = not os.path.exists(ephem_csv)\n",
    "with open(ephem_csv, \"a\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if newfile:\n",
    "        w.writerow([\"tic\",\"period_days\",\"T0_BTJD\",\"source\",\"status\",\"notes\"])\n",
    "    for r in rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "print(\"Appended both P and 2×P entries to\", ephem_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3da3495d-63ee-4982-ae94-e84d7d1f3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local-first PDCSAP loader (no f-strings; safe on any platform) ---\n",
    "import time\n",
    "from pathlib import Path\n",
    "import lightkurve as lk\n",
    "\n",
    "def _find_local_lcf(tic, sector):\n",
    "    tic16 = \"{:016d}\".format(int(tic))\n",
    "    patterns = [\n",
    "        \"*{}*s{:04d}*lc.fits\".format(tic16, int(sector)),\n",
    "        \"*{}*lc.fits\".format(tic16),\n",
    "    ]\n",
    "    roots = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd() / \"mastDownload\",\n",
    "        Path.home() / \"Downloads\" / \"mastDownload\",\n",
    "    ]\n",
    "    for root in roots:\n",
    "        if not root.exists():\n",
    "            continue\n",
    "        for pat in patterns:\n",
    "            hits = list(root.rglob(pat))\n",
    "            if hits:\n",
    "                hits.sort(key=lambda p: (p.stat().st_mtime, len(str(p))))\n",
    "                return hits[-1]\n",
    "    return None\n",
    "\n",
    "def load_pdcsap_sector(tic, sector, qmask=175, retries=3, sleep=2.5):\n",
    "    \"\"\"Load SPOC PDCSAP for a given TIC/sector, preferring local files; retries MAST on failure.\"\"\"\n",
    "    # 1) Try local FITS first\n",
    "    local = _find_local_lcf(tic, sector)\n",
    "    if local:\n",
    "        print(\"[local] Using {}\".format(local))\n",
    "        lcf = lk.open(local)  # TessLightCurveFile\n",
    "        lc = lcf.PDCSAP_FLUX\n",
    "    else:\n",
    "        # 2) MAST fetch with retries (LightCurveFile API is most robust for PDCSAP)\n",
    "        last_err = None\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                print(\"[mast] Fetching TIC {} sector {} (try {}/{}) …\".format(tic, sector, i+1, retries))\n",
    "                sr = lk.search_lightcurvefile(\"TIC {}\".format(tic), mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "                lcf = sr.download()\n",
    "                lc = lcf.PDCSAP_FLUX\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                if i < retries - 1:\n",
    "                    print(\"  retrying in {}s due to: {}\".format(sleep, e))\n",
    "                    time.sleep(sleep)\n",
    "                else:\n",
    "                    raise last_err\n",
    "\n",
    "    # 3) Clean up: drop NaNs, normalize, apply same quality mask convention\n",
    "    lc = lc.remove_nans().normalize()\n",
    "    if hasattr(lc, \"quality\"):\n",
    "        try:\n",
    "            m = (lc.quality & ~qmask) == 0\n",
    "            lc = lc[m]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d709f22b-0eba-4624-9644-fc51d270e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned TIC311183180_S31_BLS_top3.csv: kept 3\n",
      "Cleaned TIC311183180_S31_TLS_top3.csv: kept 1\n",
      "Cleaned TIC311183180_S5_BLS_top3.csv: kept 3\n",
      "Cleaned TIC311183180_S5_TLS_top3.csv: kept 1\n",
      "Cleaned TIC311183180_stitched_BLS_top3.csv: kept 3\n",
      "Cleaned TIC311183180_stitched_TLS_top3.csv: kept 1\n",
      "\n",
      "Figures: 21\n",
      "  - TIC311183180_S31_BLS_periodogram.png\n",
      "  - TIC311183180_S31_TLS_fold_P9.34844.png\n",
      "  - TIC311183180_S31_TLS_periodogram_around_11.92853.png\n",
      "  - TIC311183180_S31_TLS_periodogram_around_12.03045.png\n",
      "  - TIC311183180_S31_TLS_periodogram_around_12.05824.png\n",
      "  - TIC311183180_S31_TLS_periodogram_around_6.22816.png\n",
      "  - TIC311183180_S31_TLS_periodogram_around_9.34124.png\n",
      "  - TIC311183180_S5_BLS_periodogram.png\n",
      "  - TIC311183180_S5_TLS_fold_P9.34844.png\n",
      "  - TIC311183180_S5_TLS_periodogram_around_11.92853.png\n",
      "  - TIC311183180_S5_TLS_periodogram_around_12.03045.png\n",
      "  - TIC311183180_S5_TLS_periodogram_around_12.05824.png\n",
      "  - TIC311183180_S5_TLS_periodogram_around_6.22816.png\n",
      "  - TIC311183180_S5_TLS_periodogram_around_9.34124.png\n",
      "  - TIC311183180_download_clean.png\n",
      "  - TIC311183180_stitched_BLS_periodogram.png\n",
      "  - TIC311183180_stitched_TLS_fold_P9.34844.png\n",
      "  - TIC311183180_stitched_TLS_periodogram_around_12.10758.png\n",
      "  - TIC311183180_stitched_TLS_periodogram_around_12.20610.png\n",
      "  - TIC311183180_stitched_TLS_periodogram_around_13.24953.png\n",
      "  - TIC311183180_stitched_TLS_periodogram_around_18.68163.png\n",
      "\n",
      "Results (CSV): 7\n",
      "  - TIC311183180_S31_BLS_top3.csv\n",
      "  - TIC311183180_S31_TLS_top3.csv\n",
      "  - TIC311183180_S5_BLS_top3.csv\n",
      "  - TIC311183180_S5_TLS_top3.csv\n",
      "  - TIC311183180_ephemeris.csv\n",
      "  - TIC311183180_stitched_BLS_top3.csv\n",
      "  - TIC311183180_stitched_TLS_top3.csv\n",
      "\n",
      "Results (JSON): 1\n",
      "  - TIC311183180_download_clean_summary.json\n",
      "\n",
      "Wrote docs/targets/Target_C.md\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv, json\n",
    "\n",
    "TIC = 311183180\n",
    "TOI = \"TOI 550.02\"\n",
    "FIGDIR, RESDIR = Path(\"figures\"), Path(\"results\")\n",
    "FIGDIR.mkdir(exist_ok=True); RESDIR.mkdir(exist_ok=True)\n",
    "\n",
    "def clean_top3_csv(p: Path):\n",
    "    rows = list(csv.DictReader(p.open()))\n",
    "    if not rows:\n",
    "        print(f\"(skip) {p.name} is empty\")\n",
    "        return\n",
    "    metric = \"power\" if \"BLS\" in p.name else \"SDE\"\n",
    "    best = {}\n",
    "    for r in rows:\n",
    "        try:\n",
    "            per = round(float(r.get(\"period_days\") or r.get(\"period\")), 5)\n",
    "            val = float(r.get(metric, \"nan\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        if per not in best or val > float(best[per].get(metric, \"-1e9\")):\n",
    "            best[per] = r\n",
    "    cleaned = sorted(best.values(), key=lambda r: float(r.get(metric, \"-1e9\")), reverse=True)[:3]\n",
    "    with p.open(\"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cleaned[0].keys())\n",
    "        w.writeheader(); w.writerows(cleaned)\n",
    "    print(f\"Cleaned {p.name}: kept {len(cleaned)}\")\n",
    "\n",
    "# Clean all new top-3 files for this TIC\n",
    "for p in sorted(RESDIR.glob(f\"TIC{TIC}_*_[BT]LS_top3.csv\")):\n",
    "    clean_top3_csv(p)\n",
    "\n",
    "# Quick manifest of what we’ll commit\n",
    "figs  = sorted(FIGDIR.glob(f\"TIC{TIC}_*.png\"))\n",
    "csvs  = sorted(RESDIR.glob(f\"TIC{TIC}_*.csv\"))\n",
    "jsons = sorted(RESDIR.glob(f\"TIC{TIC}_*.json\"))\n",
    "print(\"\\nFigures:\", len(figs))\n",
    "for f in figs: print(\"  -\", f.name)\n",
    "print(\"\\nResults (CSV):\", len(csvs))\n",
    "for c in csvs: print(\"  -\", c.name)\n",
    "print(\"\\nResults (JSON):\", len(jsons))\n",
    "for j in jsons: print(\"  -\", j.name)\n",
    "\n",
    "# Optional: make a tiny Target_C summary page\n",
    "summary_path = Path(f\"results/TIC{TIC}_download_clean_summary.json\")\n",
    "sectors = []\n",
    "if summary_path.exists():\n",
    "    s = json.loads(summary_path.read_text())\n",
    "    sectors = s.get(\"sectors_pdcsap\") or s.get(\"pdcsap_sectors\") or s.get(\"sectors\") or []\n",
    "    sectors = [int(x) for x in sectors]\n",
    "sectors_text = str(sectors if sectors else \"[5, 31]\")\n",
    "\n",
    "# Pull stitched TLS “best” for the blurb (if present)\n",
    "stitched_tls = Path(f\"results/TIC{TIC}_stitched_TLS_top3.csv\")\n",
    "best_line = \"\"\n",
    "if stitched_tls.exists():\n",
    "    rows = list(csv.DictReader(stitched_tls.open()))\n",
    "    if rows:\n",
    "        rows.sort(key=lambda r: float(r.get(\"SDE\",\"-1e9\")), reverse=True)\n",
    "        r0 = rows[0]\n",
    "        try:\n",
    "            best_line = f\"- Stitched TLS: P ≈ {float(r0['period_days']):.5f} d, SDE ≈ {float(r0['SDE']):.2f}.\\n\"\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# >>> key fix: no backslashes inside f-string expressions\n",
    "best_line_text = best_line if best_line else \"- Stitched TLS ran; see top-3 CSV and fold PNGs.\\n\"\n",
    "\n",
    "doc = Path(\"docs/targets/Target_C.md\")\n",
    "doc.parent.mkdir(parents=True, exist_ok=True)\n",
    "doc_md = (\n",
    "    f\"# Target C — {TOI} (TIC {TIC})\\n\\n\"\n",
    "    f\"**Sectors:** {sectors_text} (PDCSAP)\\n\\n\"\n",
    "    \"**Artifacts saved**\\n\"\n",
    "    f\"- Periodograms & folds: see `figures/TIC{TIC}_*`\\n\"\n",
    "    f\"- Top-3 tables (BLS/TLS): see `results/TIC{TIC}_*_[BT]LS_top3.csv`\\n\"\n",
    "    f\"- Ephemeris CSV (P and 2×P both recorded): `results/TIC{TIC}_ephemeris.csv`\\n\\n\"\n",
    "    \"**Quick notes**\\n\"\n",
    "    f\"{best_line_text}\"\n",
    "    \"- Detrend: same gentle settings as Target A (quality mask=175; robust high-pass ~0.75 d; sigma-clip=5).\\n\"\n",
    ")\n",
    "doc.write_text(doc_md)\n",
    "print(f\"\\nWrote {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8a938d-1d5c-4407-a676-7aad44c7a138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets + sectors:\n",
      "  Target A: TIC 119584412 (TOI 1801.01)  sectors=[22, 49]\n",
      "  Target B: TIC 37749396 (TOI 260.01)  sectors=[3, 42, 70]\n",
      "  Target C: TIC 311183180 (TOI 550.02)  sectors=[5, 31]\n"
     ]
    }
   ],
   "source": [
    "# === Ephemeris-fit prep: pick targets and sectors ===\n",
    "import os, json\n",
    "\n",
    "TARGETS = {\n",
    "    \"Target A\": {\"tic\": 119584412, \"toi\": \"TOI 1801.01\"},\n",
    "    \"Target B\": {\"tic\": 37749396,  \"toi\": \"TOI 260.01\"},\n",
    "    \"Target C\": {\"tic\": 311183180, \"toi\": \"TOI 550.02\"},\n",
    "}\n",
    "\n",
    "def _get_pdcsap_sectors_from_summary(tic):\n",
    "    p = f\"results/TIC{tic}_download_clean_summary.json\"\n",
    "    if os.path.exists(p):\n",
    "        d = json.load(open(p))\n",
    "        for key in (\"sectors_pdcsap\",\"pdcsap_sectors\",\"sectors_used\",\"sectors\"):\n",
    "            if key in d and d[key]:\n",
    "                return sorted({int(s) for s in d[key]})\n",
    "    return []\n",
    "\n",
    "for name, info in TARGETS.items():\n",
    "    info[\"sectors\"] = _get_pdcsap_sectors_from_summary(info[\"tic\"])\n",
    "\n",
    "print(\"Targets + sectors:\")\n",
    "for name, info in TARGETS.items():\n",
    "    print(f\"  {name}: TIC {info['tic']} ({info['toi']})  sectors={info['sectors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915a7846-d90e-4ab4-af1a-897d18fcc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stitch_target(tic, sectors):\n",
    "    t_all, f_all = [], []\n",
    "    for s in sectors:\n",
    "        lc = load_pdcsap_sector(tic, s)          # <- your local-first loader\n",
    "        if lc is None:\n",
    "            continue\n",
    "        t, f = prep_arrays_for_search(lc)        # <- your robust detrend\n",
    "        if t is None or f is None or len(t) == 0:\n",
    "            continue\n",
    "        t_all.append(t); f_all.append(f)\n",
    "    if not t_all:\n",
    "        return None, None\n",
    "    t = np.concatenate(t_all); f = np.concatenate(f_all)\n",
    "    o = np.argsort(t)\n",
    "    return t[o], f[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be84ba76-f106-4cb0-aa0b-f1b26c8cd365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sectors from Cell 1:\n",
      "  Target A: TIC 119584412  sectors=[22, 49]\n",
      "  Target B: TIC 37749396  sectors=[3, 42, 70]\n",
      "  Target C: TIC 311183180  sectors=[5, 31]\n",
      "\n",
      "Function presence:\n",
      "  load_pdcsap_sector: False\n",
      "  prep_arrays_for_search: False\n"
     ]
    }
   ],
   "source": [
    "import inspect, sys\n",
    "\n",
    "print(\"Sectors from Cell 1:\")\n",
    "for name, info in TARGETS.items():\n",
    "    print(f\"  {name}: TIC {info['tic']}  sectors={info.get('sectors')}\")\n",
    "\n",
    "def _exists(fn_name):\n",
    "    return fn_name in globals() and callable(globals()[fn_name])\n",
    "\n",
    "print(\"\\nFunction presence:\")\n",
    "print(\"  load_pdcsap_sector:\", _exists(\"load_pdcsap_sector\"))\n",
    "print(\"  prep_arrays_for_search:\", _exists(\"prep_arrays_for_search\"))\n",
    "\n",
    "# Peek definitions if present\n",
    "if _exists(\"load_pdcsap_sector\"):\n",
    "    print(\"\\nload_pdcsap_sector:\", inspect.getsource(load_pdcsap_sector).splitlines()[0][:80], \"...\")\n",
    "if _exists(\"prep_arrays_for_search\"):\n",
    "    print(\"prep_arrays_for_search:\", inspect.getsource(prep_arrays_for_search).splitlines()[0][:80], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e545b07-be31-4335-84fe-5ddb0dfa4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallbacks are only defined if your originals don't exist.\n",
    "try:\n",
    "    import lightkurve as lk\n",
    "except Exception as e:\n",
    "    print(\"Lightkurve import issue — ensure it's installed in this kernel:\", e)\n",
    "\n",
    "if not _exists(\"load_pdcsap_sector\"):\n",
    "    from pathlib import Path\n",
    "    def load_pdcsap_sector(tic: int, sector: int, download_dir=\"data_raw_fresh\"):\n",
    "        \"\"\"Fallback: search/download SPOC LCF for TIC+sector and return PDCSAP LightCurve.\"\"\"\n",
    "        sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "        if len(sr) == 0:\n",
    "            print(f\"    [loader] No LCF found for TIC {tic} S{sector}\")\n",
    "            return None\n",
    "        lcf = sr.download(download_dir=download_dir)\n",
    "        if lcf is None:\n",
    "            print(f\"    [loader] Download failed for TIC {tic} S{sector}\")\n",
    "            return None\n",
    "        try:\n",
    "            lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "            lc.meta[\"sector\"] = getattr(lcf, \"sector\", sector)\n",
    "            return lc\n",
    "        except Exception as e:\n",
    "            print(f\"    [loader] PDCSAP extract failed TIC {tic} S{sector}:\", e)\n",
    "            return None\n",
    "\n",
    "if not _exists(\"prep_arrays_for_search\"):\n",
    "    import numpy as np\n",
    "    def prep_arrays_for_search(lc, quality_bitmask=175, window_days=1.0, polyorder=2):\n",
    "        \"\"\"Fallback detrend: 1-day SavGol-style flatten; returns (t, f) in BTJD, normalized.\"\"\"\n",
    "        if lc is None:\n",
    "            return None, None\n",
    "        try:\n",
    "            # Lightkurve’s quality mask is applied upstream in SPOC; keep it gentle here.\n",
    "            dt = np.median(np.diff(lc.time.value))\n",
    "            win = max(5, int(round(window_days / max(dt, 1e-6))))\n",
    "            if win % 2 == 0:\n",
    "                win += 1\n",
    "            flat = lc.flatten(window_length=win, polyorder=polyorder)\n",
    "            t = flat.time.value.astype(float)     # BTJD\n",
    "            f = (flat.flux.value / np.nanmedian(flat.flux.value)).astype(float)\n",
    "            # Drop NaNs/infs\n",
    "            m = np.isfinite(t) & np.isfinite(f)\n",
    "            t, f = t[m], f[m]\n",
    "            return (t if t.size else None), (f if f.size else None)\n",
    "        except Exception as e:\n",
    "            print(\"    [detrend] Failed:\", e)\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe309d4-644d-4203-8047-a8855b585a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Robust stitcher with verbose logging ===\n",
    "import numpy as np\n",
    "\n",
    "def stitch_target(tic, sectors):\n",
    "    if not sectors:\n",
    "        print(f\"[stitch] TIC {tic}: empty sector list.\")\n",
    "        return None, None\n",
    "    t_all, f_all = [], []\n",
    "    print(f\"[stitch] TIC {tic}: trying sectors {sectors}\")\n",
    "    for s in sectors:\n",
    "        lc = load_pdcsap_sector(tic, s)\n",
    "        if lc is None:\n",
    "            print(f\"  - S{s}: no LC\")\n",
    "            continue\n",
    "        t, f = prep_arrays_for_search(lc)\n",
    "        if t is None or f is None or len(t) == 0:\n",
    "            print(f\"  - S{s}: detrend produced no data\")\n",
    "            continue\n",
    "        print(f\"  - S{s}: ok (N={len(t)})\")\n",
    "        t_all.append(t); f_all.append(f)\n",
    "    if not t_all:\n",
    "        print(f\"[stitch] TIC {tic}: no usable sectors\")\n",
    "        return None, None\n",
    "    t = np.concatenate(t_all); f = np.concatenate(f_all)\n",
    "    o = np.argsort(t)\n",
    "    t, f = t[o], f[o]\n",
    "    print(f\"[stitch] TIC {tic}: stitched N={len(t)} points across {len(t_all)} sector(s)\")\n",
    "    return t, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f33054-a3b3-4c81-a02e-238d31acd4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Target A (TIC 119584412) ===\n",
      "[stitch] TIC 119584412: trying sectors [22, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S22: ok (N=16102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S49: ok (N=13272)\n",
      "[stitch] TIC 119584412: stitched N=29374 points across 2 sector(s)\n",
      "\n",
      "=== Target B (TIC 37749396) ===\n",
      "[stitch] TIC 37749396: trying sectors [3, 42, 70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S3: ok (N=12978)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S42: ok (N=11473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/lightkurve/search.py:424: LightkurveWarning: Warning: 2 files available to download. Only the first file has been downloaded. Please use `download_all()` or specify additional criteria (e.g. quarter, campaign, or sector) to limit your search.\n",
      "  warnings.warn(\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S70: ok (N=86180)\n",
      "[stitch] TIC 37749396: stitched N=110631 points across 3 sector(s)\n",
      "\n",
      "=== Target C (TIC 311183180) ===\n",
      "[stitch] TIC 311183180: trying sectors [5, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S5: ok (N=17286)\n",
      "  - S31: ok (N=16250)\n",
      "[stitch] TIC 311183180: stitched N=33536 points across 2 sector(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    }
   ],
   "source": [
    "# === Run stitcher for A–C and summarize ===\n",
    "def run_stitch_for_targets():\n",
    "    stitched = {}\n",
    "    for name, info in TARGETS.items():\n",
    "        tic = info[\"tic\"]\n",
    "        secs = info.get(\"sectors\", [])\n",
    "        print(f\"\\n=== {name} (TIC {tic}) ===\")\n",
    "        t, f = stitch_target(tic, secs)\n",
    "        stitched[name] = (t, f)\n",
    "    return stitched\n",
    "\n",
    "stitched = run_stitch_for_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9789a2-94c8-441f-9e30-50c4215d10ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[retry] Re-running stitch after downloads...\n",
      "\n",
      "=== Target A (TIC 119584412) ===\n",
      "[stitch] TIC 119584412: trying sectors [22, 49]\n",
      "  - S22: ok (N=16102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S49: ok (N=13272)\n",
      "[stitch] TIC 119584412: stitched N=29374 points across 2 sector(s)\n",
      "\n",
      "=== Target B (TIC 37749396) ===\n",
      "[stitch] TIC 37749396: trying sectors [3, 42, 70]\n",
      "  - S3: ok (N=12978)\n",
      "  - S42: ok (N=11473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/lightkurve/search.py:424: LightkurveWarning: Warning: 2 files available to download. Only the first file has been downloaded. Please use `download_all()` or specify additional criteria (e.g. quarter, campaign, or sector) to limit your search.\n",
      "  warnings.warn(\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S70: ok (N=86180)\n",
      "[stitch] TIC 37749396: stitched N=110631 points across 3 sector(s)\n",
      "\n",
      "=== Target C (TIC 311183180) ===\n",
      "[stitch] TIC 311183180: trying sectors [5, 31]\n",
      "  - S5: ok (N=17286)\n",
      "  - S31: ok (N=16250)\n",
      "[stitch] TIC 311183180: stitched N=33536 points across 2 sector(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    }
   ],
   "source": [
    "# === Force-download any missing SPOC LightCurveFiles for the sectors we want ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data_raw_fresh\")\n",
    "\n",
    "def force_download_pdcsap(tic, sectors):\n",
    "    import lightkurve as lk\n",
    "    print(f\"[download] TIC {tic}: ensuring sectors {sectors}\")\n",
    "    for s in sectors:\n",
    "        try:\n",
    "            sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=int(s))\n",
    "            if len(sr) == 0:\n",
    "                print(f\"  - S{s}: no SPOC LCF available\")\n",
    "                continue\n",
    "            # Use cache; if corrupt, lk will re-download\n",
    "            lcf = sr.download(download_dir=str(DATA_DIR))\n",
    "            if lcf is None:\n",
    "                print(f\"  - S{s}: download returned None\")\n",
    "                continue\n",
    "            _ = lcf.PDCSAP_FLUX  # touch PDCSAP to validate file\n",
    "            print(f\"  - S{s}: ready\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - S{s}: download/extract failed -> {e}\")\n",
    "\n",
    "# Download for any target that failed, then retry stitching\n",
    "for name, (t, f) in stitched.items():\n",
    "    if t is None or f is None or len(t) == 0:\n",
    "        tic = TARGETS[name][\"tic\"]\n",
    "        secs = TARGETS[name].get(\"sectors\", [])\n",
    "        force_download_pdcsap(tic, secs)\n",
    "\n",
    "print(\"\\n[retry] Re-running stitch after downloads...\")\n",
    "stitched = run_stitch_for_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af5ddd64-32be-4f01-b122-70e592f164d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, math\n",
    "\n",
    "def pick_tls_guess(tic, label=\"stitched\"):\n",
    "    \"\"\"Read period/T0 seed from results/TIC<tic>_<label>_TLS_top3.csv.\"\"\"\n",
    "    path = f\"results/TIC{tic}_{label}_TLS_top3.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    rows = []\n",
    "    with open(path) as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            try:\n",
    "                P = float(row[\"period_days\"])\n",
    "                T0 = float(row.get(\"T0_BTJD\", \"nan\"))\n",
    "                SDE = float(row.get(\"SDE\", \"nan\"))\n",
    "                rows.append((P, T0, SDE))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not rows:\n",
    "        return None\n",
    "    # Highest SDE first\n",
    "    rows.sort(key=lambda x: (x[2] if math.isfinite(x[2]) else -1.0), reverse=True)\n",
    "    P, T0, _ = rows[0]\n",
    "    return dict(P=P, T0=(T0 if math.isfinite(T0) else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43d40f20-8b78-4f78-8679-30e942c4251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Midtime:\n",
    "    epoch: int\n",
    "    tmid: float\n",
    "    tmid_err: float\n",
    "\n",
    "def _estimate_duration_hours(P_days, Rstar_Rsun=0.7, b=0.5):\n",
    "    # rough scaling: duration ~ 13 hr * (P/365)^(1/3) * R* * sqrt(1-b^2)\n",
    "    return float(13.0 * (P_days/365.0)**(1/3) * Rstar_Rsun * np.sqrt(max(1e-3, 1-b*b)))\n",
    "\n",
    "def find_midtimes(t, f, P, T0_guess=None, dur_hours=None):\n",
    "    t = np.asarray(t, float); f = np.asarray(f, float)\n",
    "    if dur_hours is None:\n",
    "        dur_hours = _estimate_duration_hours(P)\n",
    "    half = 0.6*(dur_hours/24.0)  # half-window in days\n",
    "\n",
    "    # If T0_guess missing, pick phase of minimum median flux, anchor near median time\n",
    "    if T0_guess is None:\n",
    "        nb = 200\n",
    "        phases = (t % P) / P\n",
    "        bins = np.linspace(0, 1, nb+1)\n",
    "        idx  = np.digitize(phases, bins) - 1\n",
    "        yb   = np.array([np.nanmedian(f[idx==i]) if np.any(idx==i) else np.nan for i in range(nb)])\n",
    "        i0   = np.nanargmin(yb)\n",
    "        phase0 = 0.5*(bins[i0] + bins[i0+1])\n",
    "        tmed = np.nanmedian(t)\n",
    "        k0 = np.round((tmed/P) - phase0).astype(int)\n",
    "        T0_guess = k0*P + phase0*P\n",
    "\n",
    "    kmin = int(np.floor((t.min()-T0_guess)/P)) - 1\n",
    "    kmax = int(np.ceil((t.max()-T0_guess)/P)) + 1\n",
    "    mids = []\n",
    "\n",
    "    for k in range(kmin, kmax+1):\n",
    "        tc = T0_guess + k*P\n",
    "        sel = (t >= tc - half) & (t <= tc + half)\n",
    "        if sel.sum() < 6:\n",
    "            continue\n",
    "        tt, ff = t[sel], f[sel]\n",
    "\n",
    "        # local linear baseline removal\n",
    "        A = np.vstack([np.ones(sel.sum()), tt-tt.mean()]).T\n",
    "        coef, *_ = np.linalg.lstsq(A, ff, rcond=None)\n",
    "        fl = ff - (A @ coef)\n",
    "\n",
    "        # quadratic to the lowest ~30% points\n",
    "        q = np.nanpercentile(fl, 30)\n",
    "        use = fl <= q\n",
    "        if use.sum() < 5:\n",
    "            use = np.argsort(fl)[:max(5, sel.sum()//5)]\n",
    "        x = tt[use] - tt[use].mean()\n",
    "        y = fl[use]\n",
    "        X = np.vstack([x*x, x, np.ones_like(x)]).T\n",
    "        a, b, c = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "        if a <= 0:\n",
    "            continue\n",
    "        x0 = -b/(2*a)\n",
    "        tmid = tt[use].mean() + x0\n",
    "\n",
    "        resid = y - (a*x*x + b*x + c)\n",
    "        s = np.nanstd(resid)\n",
    "        t_err = np.sqrt(s/max(1e-6, 2*a)) / np.sqrt(use.sum())\n",
    "\n",
    "        if np.isfinite(tmid) and np.isfinite(t_err) and abs(tmid - tc) <= half:\n",
    "            mids.append(Midtime(epoch=k, tmid=float(tmid), tmid_err=float(t_err)))\n",
    "\n",
    "    return mids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb373a5-621c-4b73-877c-6ddd62579b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_linear_ephemeris(mids):\n",
    "    E = np.array([m.epoch for m in mids], int)\n",
    "    T = np.array([m.tmid  for m in mids], float)\n",
    "    s = np.array([max(1e-6, m.tmid_err) for m in mids], float)\n",
    "    X = np.vstack([np.ones_like(E, float), E.astype(float)]).T\n",
    "    W = np.diag(1.0/s**2)\n",
    "    XtWX = X.T @ W @ X\n",
    "    beta = np.linalg.solve(XtWX, X.T @ W @ T)  # [T0, P]\n",
    "    cov  = np.linalg.inv(XtWX)\n",
    "\n",
    "    # scale covariance by reduced-χ²\n",
    "    resid = T - (X @ beta)\n",
    "    dof   = max(1, len(T) - 2)\n",
    "    chi2  = float((resid**2 / s**2).sum())\n",
    "    rchi2 = chi2 / dof\n",
    "    cov  *= rchi2\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"T0\": float(beta[0]), \"P\": float(beta[1]),\n",
    "            \"cov\": [[float(cov[0,0]), float(cov[0,1])],\n",
    "                    [float(cov[1,0]), float(cov[1,1])]],\n",
    "            \"sigma_T0\": float(np.sqrt(cov[0,0])),\n",
    "            \"sigma_P\":  float(np.sqrt(cov[1,1])),\n",
    "            \"cov_T0P\":  float(cov[0,1]),\n",
    "            \"N_mids\":   int(len(T)),\n",
    "            \"chi2\":     chi2,\n",
    "            \"rchi2\":    float(rchi2),\n",
    "        },\n",
    "        resid, E, T, s\n",
    "    )\n",
    "\n",
    "def bootstrap_ephemeris(mids, n_boot=400, random_state=42):\n",
    "    if len(mids) < 3:\n",
    "        return None\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    samples = []\n",
    "    for _ in range(n_boot):\n",
    "        pick = rng.integers(0, len(mids), len(mids))\n",
    "        mids_b = [mids[i] for i in pick]\n",
    "        try:\n",
    "            fit_b, *_ = fit_linear_ephemeris(mids_b)\n",
    "            samples.append([fit_b[\"T0\"], fit_b[\"P\"]])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not samples:\n",
    "        return None\n",
    "    S = np.array(samples)\n",
    "    cov = np.cov(S.T)\n",
    "    return {\"samples\": S, \"cov\": [[float(cov[0,0]), float(cov[0,1])],\n",
    "                                  [float(cov[1,0]), float(cov[1,1])]],\n",
    "            \"mean_T0\": float(S[:,0].mean()),\n",
    "            \"mean_P\":  float(S[:,1].mean())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4b0c22-d55c-4f4f-a374-113a94fa1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.time import Time\n",
    "\n",
    "BTJD_ZERO = 2457000.0\n",
    "\n",
    "def btjd_from_datestr(date_str):\n",
    "    t = Time(date_str, scale=\"utc\")\n",
    "    return float(t.tdb.jd - BTJD_ZERO)\n",
    "\n",
    "def compute_windows(fit, t_ref_min, t_ref_max, k_sigma=1.0):\n",
    "    T0, P = fit[\"T0\"], fit[\"P\"]\n",
    "    cov = np.array(fit[\"cov\"])\n",
    "    kmin = int(np.floor((t_ref_min - T0)/P)) - 1\n",
    "    kmax = int(np.ceil((t_ref_max - T0)/P)) + 1\n",
    "    rows = []\n",
    "    for k in range(kmin, kmax+1):\n",
    "        Tpred = T0 + k*P\n",
    "        if Tpred < t_ref_min or Tpred > t_ref_max:\n",
    "            continue\n",
    "        J = np.array([1.0, float(k)])\n",
    "        sig = float(np.sqrt(max(J @ cov @ J, 0.0)))\n",
    "        rows.append({\"epoch\": k, \"Tpred_BTJD\": float(Tpred),\n",
    "                     \"sigma1d_days\": sig,\n",
    "                     \"window_half_width_1sigma_days\": k_sigma * sig})\n",
    "    return rows\n",
    "\n",
    "def plot_oc(E, T, fit, target_tag, outpng):\n",
    "    model = fit[\"T0\"] + fit[\"P\"] * E\n",
    "    oc = (T - model) * 24 * 60  # minutes\n",
    "    plt.figure(figsize=(7.5,3.8), dpi=140)\n",
    "    plt.axhline(0, lw=1, alpha=0.3)\n",
    "    plt.plot(E, oc, \"o\", ms=4)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"O–C (minutes)\")\n",
    "    plt.title(f\"{target_tag} — O–C\")\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "def plot_corner(samples, target_tag, outpng):\n",
    "    if samples is None:\n",
    "        return\n",
    "    S = samples[\"samples\"]\n",
    "    plt.figure(figsize=(6.2,2.8), dpi=140)\n",
    "    plt.subplot(1,2,1); plt.scatter(S[:,0], S[:,1], s=8, alpha=0.25)\n",
    "    plt.xlabel(\"T0 (BTJD)\"); plt.ylabel(\"P (days)\")\n",
    "    plt.title(\"Bootstrap samples\")\n",
    "    plt.subplot(1,2,2); plt.hist(S[:,1], bins=40)\n",
    "    plt.xlabel(\"P (days)\"); plt.title(\"P posterior\")\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b91a411-06f0-4363-806f-09853154a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Target A — TIC 119584412 (TOI 1801.01) — sectors [22, 49] ===\n",
      "[stitch] TIC 119584412: trying sectors [22, 49]\n",
      "  - S22: ok (N=16102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S49: ok (N=13272)\n",
      "[stitch] TIC 119584412: stitched N=29374 points across 2 sector(s)\n",
      "  Using TLS guess: P≈16.027187 d, T0=1908.062441\n",
      "  midtimes found: 3\n",
      "  Fit:   P = 16.02749976 ± 0.00026424 d\n",
      "         T0 = 1908.046283 ± 0.009011 BTJD\n",
      "         Cov(T0,P) = -1.784e-06   N=3   χ²_ν=1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: midtimes CSV, refined_ephemeris.json, ephem_bootstrap.json (if any), windows_to_2026-03.json, O–C, and posterior plot.\n",
      "\n",
      "=== Target B — TIC 37749396 (TOI 260.01) — sectors [3, 42, 70] ===\n",
      "[stitch] TIC 37749396: trying sectors [3, 42, 70]\n",
      "  - S3: ok (N=12978)\n",
      "  - S42: ok (N=11473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/lightkurve/search.py:424: LightkurveWarning: Warning: 2 files available to download. Only the first file has been downloaded. Please use `download_all()` or specify additional criteria (e.g. quarter, campaign, or sector) to limit your search.\n",
      "  warnings.warn(\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S70: ok (N=86180)\n",
      "[stitch] TIC 37749396: stitched N=110631 points across 3 sector(s)\n",
      "  Using TLS guess: P≈13.475725 d, T0=1392.311964\n",
      "  midtimes found: 4\n",
      "  Fit:   P = 13.47582381 ± 0.00034966 d\n",
      "         T0 = 1392.306006 ± 0.043330 BTJD\n",
      "         Cov(T0,P) = -1.464e-05   N=4   χ²_ν=12.04\n",
      "  Saved: midtimes CSV, refined_ephemeris.json, ephem_bootstrap.json (if any), windows_to_2026-03.json, O–C, and posterior plot.\n",
      "\n",
      "=== Target C — TIC 311183180 (TOI 550.02) — sectors [5, 31] ===\n",
      "[stitch] TIC 311183180: trying sectors [5, 31]\n",
      "  - S5: ok (N=17286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S31: ok (N=16250)\n",
      "[stitch] TIC 311183180: stitched N=33536 points across 2 sector(s)\n",
      "  Using TLS guess: P≈9.348442 d, T0=2149.633067\n",
      "  midtimes found: 5\n",
      "  Fit:   P = 9.34849077 ± 0.00025751 d\n",
      "         T0 = 2149.633454 ± 0.001675 BTJD\n",
      "         Cov(T0,P) = -3.597e-08   N=5   χ²_ν=4.92\n",
      "  Saved: midtimes CSV, refined_ephemeris.json, ephem_bootstrap.json (if any), windows_to_2026-03.json, O–C, and posterior plot.\n"
     ]
    }
   ],
   "source": [
    "END_DATE = \"2026-03-31\"\n",
    "t_end = btjd_from_datestr(END_DATE)\n",
    "\n",
    "for name, info in TARGETS.items():\n",
    "    tic, toi, secs = info[\"tic\"], info[\"toi\"], info.get(\"sectors\", [])\n",
    "    print(f\"\\n=== {name} — TIC {tic} ({toi}) — sectors {secs} ===\")\n",
    "\n",
    "    # Use stitched arrays you just built\n",
    "    t, f = stitch_target(tic, secs)\n",
    "    if t is None:\n",
    "        print(\"  ! No stitched data; skipping.\")\n",
    "        continue\n",
    "\n",
    "    guess = pick_tls_guess(tic, label=\"stitched\")\n",
    "    if not guess:\n",
    "        print(\"  ! No stitched TLS top-3 CSV found — run the BLS→TLS stitched block first. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    P0, T00 = guess[\"P\"], guess[\"T0\"]\n",
    "    print(f\"  Using TLS guess: P≈{P0:.6f} d, T0={(f'{T00:.6f}' if T00 else 'auto')}\")\n",
    "    mids = find_midtimes(t, f, P0, T00)\n",
    "    print(f\"  midtimes found: {len(mids)}\")\n",
    "\n",
    "    if len(mids) < 3:\n",
    "        print(\"  ! Not enough midtimes for a robust fit (need ≥3). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    fit, resid, E, Tm, s = fit_linear_ephemeris(mids)\n",
    "    print(f\"  Fit:   P = {fit['P']:.8f} ± {fit['sigma_P']:.8f} d\")\n",
    "    print(f\"         T0 = {fit['T0']:.6f} ± {fit['sigma_T0']:.6f} BTJD\")\n",
    "    print(f\"         Cov(T0,P) = {fit['cov_T0P']:.3e}   N={fit['N_mids']}   χ²_ν={fit['rchi2']:.2f}\")\n",
    "\n",
    "    # Save midtimes + ephemeris + bootstrap\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    import csv, json\n",
    "    with open(f\"results/TIC{tic}_midtimes.csv\",\"w\", newline=\"\") as fcsv:\n",
    "        w = csv.writer(fcsv); w.writerow([\"epoch\",\"tmid_BTJD\",\"tmid_err_d\"])\n",
    "        for m in mids: w.writerow([m.epoch, m.tmid, m.tmid_err])\n",
    "\n",
    "    with open(f\"results/TIC{tic}_refined_ephemeris.json\",\"w\") as fj:\n",
    "        json.dump(fit, fj, indent=2)\n",
    "\n",
    "    boot = bootstrap_ephemeris(mids, n_boot=400, random_state=42)\n",
    "    if boot:\n",
    "        with open(f\"results/TIC{tic}_ephem_bootstrap.json\",\"w\") as fb:\n",
    "            json.dump({\"mean_P\":boot[\"mean_P\"], \"mean_T0\":boot[\"mean_T0\"], \"cov\":boot[\"cov\"]}, fb, indent=2)\n",
    "\n",
    "    # Windows to Mar 2026\n",
    "    tmin = float(np.nanmin(t))\n",
    "    windows = compute_windows(fit, t_ref_min=tmin, t_ref_max=t_end, k_sigma=1.0)\n",
    "    with open(f\"results/TIC{tic}_windows_to_2026-03.json\",\"w\") as fw:\n",
    "        json.dump(windows, fw, indent=2)\n",
    "\n",
    "    # Plots\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "    plot_oc(E, Tm, fit, f\"{toi} ({name})\", f\"figures/TIC{tic}_OC.png\")\n",
    "    if boot:\n",
    "        plot_corner(boot, f\"{toi} ({name})\", f\"figures/TIC{tic}_P_T0_bootstrap.png\")\n",
    "\n",
    "    print(\"  Saved: midtimes CSV, refined_ephemeris.json, ephem_bootstrap.json (if any),\",\n",
    "          \"windows_to_2026-03.json, O–C, and posterior plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e785e91-095a-4f47-b5c5-94466c2a984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target A: robust P=16.02749976 d, T0=1908.046283 (σP=1.74e-06, σT0=7.95e-05)\n",
      "Target B: robust P=13.47582381 d, T0=1392.306006 (σP=2.36e-06, σT0=2.97e-04)\n",
      "Target C: robust P=9.34849093 d, T0=2149.633488 (σP=5.84e-07, σT0=2.20e-06)\n"
     ]
    }
   ],
   "source": [
    "import json, csv, os, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def read_midtimes_csv(tic):\n",
    "    path = f\"results/TIC{tic}_midtimes.csv\"\n",
    "    if not os.path.exists(path): return None\n",
    "    E, T, s = [], [], []\n",
    "    with open(path) as f:\n",
    "        r = csv.DictReader(f)\n",
    "        for row in r:\n",
    "            E.append(int(row[\"epoch\"]))\n",
    "            T.append(float(row[\"tmid_BTJD\"]))\n",
    "            s.append(max(1e-6, float(row[\"tmid_err_d\"])))\n",
    "    return np.array(E), np.array(T), np.array(s)\n",
    "\n",
    "def huber_weights(resid, sigma, c=1.345):\n",
    "    # Classic Huber weighting on standardized residuals\n",
    "    z = np.abs(resid / sigma)\n",
    "    w = np.ones_like(z)\n",
    "    w[z > c] = c / z[z > c]\n",
    "    return w\n",
    "\n",
    "def robust_fit_linear_ephemeris(E, T, s, max_iter=20, c=1.345):\n",
    "    # Start with weighted LS\n",
    "    X = np.vstack([np.ones_like(E, float), E.astype(float)]).T\n",
    "    w = 1.0 / s**2\n",
    "    beta = np.linalg.lstsq(X * np.sqrt(w)[:,None], T * np.sqrt(w), rcond=None)[0]\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        resid = T - X @ beta\n",
    "        # Scale estimate (MAD -> sigma)\n",
    "        mad = np.median(np.abs(resid - np.median(resid))) + 1e-12\n",
    "        sigma = 1.4826 * mad\n",
    "        w_r = huber_weights(resid, sigma, c=c)\n",
    "        W = w_r / s**2\n",
    "        XtWX = X.T @ (W[:,None] * X)\n",
    "        beta_new = np.linalg.solve(XtWX, X.T @ (W * T))\n",
    "        if np.allclose(beta_new, beta, rtol=0, atol=1e-10):\n",
    "            beta = beta_new; break\n",
    "        beta = beta_new\n",
    "\n",
    "    # Robust covariance via sandwich estimator\n",
    "    resid = T - X @ beta\n",
    "    W = w_r / s**2\n",
    "    XtWX = X.T @ (W[:,None] * X)\n",
    "    H = np.linalg.inv(XtWX)\n",
    "    S = np.diag(W * resid**2)\n",
    "    cov = H @ (X.T @ S @ X) @ H\n",
    "\n",
    "    T0, P = float(beta[0]), float(beta[1])\n",
    "    cov = np.array(cov, float)\n",
    "    out = {\n",
    "        \"T0\": T0, \"P\": P,\n",
    "        \"cov\": [[float(cov[0,0]), float(cov[0,1])],\n",
    "                [float(cov[1,0]), float(cov[1,1])]],\n",
    "        \"sigma_T0\": float(np.sqrt(cov[0,0])),\n",
    "        \"sigma_P\":  float(np.sqrt(cov[1,1])),\n",
    "        \"cov_T0P\":  float(cov[0,1]),\n",
    "        \"N_mids\":   int(len(T)),\n",
    "        \"rchi2_like\": float(np.mean((resid/s)**2))  # diagnostic only\n",
    "    }\n",
    "    return out, resid\n",
    "\n",
    "# Run robust re-fit for A–C and save alongside original\n",
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "for name, info in TARGETS.items():\n",
    "    tic = info[\"tic\"]\n",
    "    data = read_midtimes_csv(tic)\n",
    "    if data is None:\n",
    "        print(f\"{name}: no midtimes CSV; skipping.\")\n",
    "        continue\n",
    "    E, T, s = data\n",
    "    rob, resid = robust_fit_linear_ephemeris(E, T, s)\n",
    "    with open(f\"results/TIC{tic}_refined_ephemeris_robust.json\",\"w\") as f:\n",
    "        json.dump(rob, f, indent=2)\n",
    "    print(f\"{name}: robust P={rob['P']:.8f} d, T0={rob['T0']:.6f} (σP={rob['sigma_P']:.2e}, σT0={rob['sigma_T0']:.2e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97d12c33-3f00-4be6-869a-9e03baa0d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vet] Target A TIC 119584412 sectors [22, 49]\n",
      "[stitch] TIC 119584412: trying sectors [22, 49]\n",
      "  - S22: ok (N=16102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S49: ok (N=13272)\n",
      "[stitch] TIC 119584412: stitched N=29374 points across 2 sector(s)\n",
      "  -> saved figures/TIC119584412_fold_odd_even.png\n",
      "[vet] Target B TIC 37749396 sectors [3, 42, 70]\n",
      "[stitch] TIC 37749396: trying sectors [3, 42, 70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/lightkurve/search.py:424: LightkurveWarning: Warning: 2 files available to download. Only the first file has been downloaded. Please use `download_all()` or specify additional criteria (e.g. quarter, campaign, or sector) to limit your search.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S3: ok (N=12978)\n",
      "  - S42: ok (N=11473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S70: ok (N=86180)\n",
      "[stitch] TIC 37749396: stitched N=110631 points across 3 sector(s)\n",
      "  -> saved figures/TIC37749396_fold_odd_even.png\n",
      "[vet] Target C TIC 311183180 sectors [5, 31]\n",
      "[stitch] TIC 311183180: trying sectors [5, 31]\n",
      "  - S5: ok (N=17286)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S31: ok (N=16250)\n",
      "[stitch] TIC 311183180: stitched N=33536 points across 2 sector(s)\n",
      "  -> saved figures/TIC311183180_fold_odd_even.png\n"
     ]
    }
   ],
   "source": [
    "# --- Self-contained helpers (works even if earlier cells weren't run) ---\n",
    "import os, json, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def load_refined_or_tls_guess(tic, label=\"stitched\"):\n",
    "    \"\"\"Return dict(P, T0, source) from refined JSON if present; else fall back to TLS top-3.\"\"\"\n",
    "    ref_p = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "    if os.path.exists(ref_p):\n",
    "        d = json.load(open(ref_p))\n",
    "        return {\"P\": float(d[\"P\"]), \"T0\": float(d[\"T0\"]), \"source\": \"refined\"}\n",
    "    # fallback to TLS guess (requires pick_tls_guess defined earlier)\n",
    "    if 'pick_tls_guess' in globals():\n",
    "        g = pick_tls_guess(tic, label=label)\n",
    "        if g and ('P' in g) and (g.get('T0') is not None):\n",
    "            g[\"source\"] = \"tls\"\n",
    "            return g\n",
    "    return None\n",
    "\n",
    "def load_robust_if_available(tic):\n",
    "    p = f\"results/TIC{tic}_refined_ephemeris_robust.json\"\n",
    "    if os.path.exists(p):\n",
    "        d = json.load(open(p))\n",
    "        return {\"P\": float(d[\"P\"]), \"T0\": float(d[\"T0\"]), \"source\": \"robust\"}\n",
    "    return None\n",
    "\n",
    "def make_vetting_panel(t, f, P, T0, outpng, nbins=160, halfwidth=0.15):\n",
    "    t = np.asarray(t, float); f = np.asarray(f, float)\n",
    "    phase = ((t - T0)/P) % 1.0\n",
    "    phase[phase>0.5] -= 1.0  # center on 0\n",
    "\n",
    "    # Event index & odd/even\n",
    "    k = np.round((t - T0)/P).astype(int)\n",
    "    odd = (k % 2 != 0)\n",
    "\n",
    "    # binning helper\n",
    "    def _bin(x, y, nb, xlim=(-halfwidth, halfwidth)):\n",
    "        edges = np.linspace(xlim[0], xlim[1], nb+1)\n",
    "        idx = np.digitize(x, edges)-1\n",
    "        xc = 0.5*(edges[:-1]+edges[1:])\n",
    "        yb = np.array([np.nanmedian(y[idx==i]) if np.any(idx==i) else np.nan for i in range(nb)])\n",
    "        return xc, yb\n",
    "\n",
    "    x_all, y_all = _bin(phase, f, nbins)\n",
    "    x_odd, y_odd = _bin(phase[odd], f[odd], nbins)\n",
    "    x_even, y_even = _bin(phase[~odd], f[~odd], nbins)\n",
    "\n",
    "    # secondary near phase 0.5 (±0.03) — FIX: use positional arg name 'nb' instead of 'nbins'\n",
    "    sec_mask = (np.abs(phase - 0.5) < 0.03) | (np.abs(phase + 0.5) < 0.03)\n",
    "    x_sec, y_sec = _bin(phase[sec_mask], f[sec_mask], 60, xlim=(-0.03, 0.03))\n",
    "\n",
    "    plt.figure(figsize=(8.6,5.2), dpi=140)\n",
    "\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(phase, f, \".\", ms=1, alpha=0.25)\n",
    "    plt.plot(x_all, y_all, \"-\", lw=1.4)\n",
    "    plt.xlim(-halfwidth, halfwidth); plt.title(\"All transits\"); plt.xlabel(\"Phase\"); plt.ylabel(\"Flux\")\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(x_odd, y_odd, \"-\", lw=1.4, label=\"odd\")\n",
    "    plt.plot(x_even, y_even, \"-\", lw=1.4, label=\"even\")\n",
    "    plt.legend(); plt.xlim(-halfwidth, halfwidth); plt.title(\"Odd vs Even\"); plt.xlabel(\"Phase\")\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(x_sec, y_sec, \"-\", lw=1.4)\n",
    "    plt.title(\"Secondary window near phase 0.5 (±0.03)\")\n",
    "    plt.xlabel(\"Phase offset from 0.5\"); plt.ylabel(\"Flux\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    Path(\"figures\").mkdir(exist_ok=True)\n",
    "    plt.savefig(outpng); plt.close()\n",
    "\n",
    "def vet_all_targets(use_robust=False):\n",
    "    for name, info in TARGETS.items():\n",
    "        tic, toi, secs = info[\"tic\"], info[\"toi\"], info[\"sectors\"]\n",
    "        print(f\"[vet] {name} TIC {tic} sectors {secs}\")\n",
    "        t, f = stitch_target(tic, secs)\n",
    "        if t is None:\n",
    "            print(\"  -> no data\"); continue\n",
    "        ep = load_refined_or_tls_guess(tic)\n",
    "        if use_robust:\n",
    "            rb = load_robust_if_available(tic)\n",
    "            if rb: ep = rb\n",
    "        if not ep:\n",
    "            print(\"  -> no ephemeris/TLS seed; skipping\"); continue\n",
    "        outpng = f\"figures/TIC{tic}_fold_odd_even.png\" if not use_robust else f\"figures/TIC{tic}_fold_odd_even_robust.png\"\n",
    "        make_vetting_panel(t, f, ep[\"P\"], ep[\"T0\"], outpng)\n",
    "        print(f\"  -> saved {outpng}\")\n",
    "\n",
    "# Run once with refined, and (optionally) again with robust:\n",
    "vet_all_targets(use_robust=False)\n",
    "# vet_all_targets(use_robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c7f5b43-a25c-4f7c-b641-c3a7ac9b9917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FigA] building panel for Target A TIC 119584412 secs [22, 49]\n",
      "[stitch] TIC 119584412: trying sectors [22, 49]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S22: ok (N=16102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/1851795620.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.tight_layout()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S49: ok (N=13272)\n",
      "[stitch] TIC 119584412: stitched N=29374 points across 2 sector(s)\n",
      "[FigA] TOI 1801.01 TIC 119584412: saved figures/TIC119584412_fold_model.png and results/TIC119584412_boxfit.json\n",
      "[FigA] building panel for Target B TIC 37749396 secs [3, 42, 70]\n",
      "[stitch] TIC 37749396: trying sectors [3, 42, 70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/lightkurve/search.py:424: LightkurveWarning: Warning: 2 files available to download. Only the first file has been downloaded. Please use `download_all()` or specify additional criteria (e.g. quarter, campaign, or sector) to limit your search.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S3: ok (N=12978)\n",
      "  - S42: ok (N=11473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S70: ok (N=86180)\n",
      "[stitch] TIC 37749396: stitched N=110631 points across 3 sector(s)\n",
      "[FigA] TOI 260.01 TIC 37749396: saved figures/TIC37749396_fold_model.png and results/TIC37749396_boxfit.json\n",
      "[FigA] building panel for Target C TIC 311183180 secs [5, 31]\n",
      "[stitch] TIC 311183180: trying sectors [5, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/1851795620.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.tight_layout()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - S5: ok (N=17286)\n",
      "  - S31: ok (N=16250)\n",
      "[stitch] TIC 311183180: stitched N=33536 points across 2 sector(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:11: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/2864407655.py:20: LightkurveDeprecationWarning: The PDCSAP_FLUX function is deprecated and may be removed in a future version.\n",
      "  lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_15037/1851795620.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FigA] TOI 550.02 TIC 311183180: saved figures/TIC311183180_fold_model.png and results/TIC311183180_boxfit.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- helpers: ephemeris pick (refined/robust) ---\n",
    "def pick_ephemeris_for_folding(tic, prefer_robust=False):\n",
    "    # robust for folding if present and requested\n",
    "    if prefer_robust and os.path.exists(f\"results/TIC{tic}_refined_ephemeris_robust.json\"):\n",
    "        d = json.load(open(f\"results/TIC{tic}_refined_ephemeris_robust.json\"))\n",
    "        return {\"P\": float(d[\"P\"]), \"T0\": float(d[\"T0\"]), \"src\": \"robust\"}\n",
    "    # otherwise refined\n",
    "    if os.path.exists(f\"results/TIC{tic}_refined_ephemeris.json\"):\n",
    "        d = json.load(open(f\"results/TIC{tic}_refined_ephemeris.json\"))\n",
    "        return {\"P\": float(d[\"P\"]), \"T0\": float(d[\"T0\"]), \"src\": \"refined\"}\n",
    "    # final fallback to TLS guess from earlier cell 3\n",
    "    if 'pick_tls_guess' in globals():\n",
    "        g = pick_tls_guess(tic, label=\"stitched\")\n",
    "        if g and g.get(\"T0\") is not None:\n",
    "            g[\"src\"] = \"tls\"\n",
    "            return g\n",
    "    return None\n",
    "\n",
    "# --- simple box-fit around phase 0 to estimate depth & duration ---\n",
    "def quick_boxfit(phase, flux, halfw=0.15):\n",
    "    \"\"\"Return (depth, duration_days, in_mask) using robust medians around phase 0.\"\"\"\n",
    "    # define windows\n",
    "    in_w  = 0.03  # +/- 0.03 d in phase units of days-equivalent (we phase in days)\n",
    "    out_w = (0.07, 0.15)\n",
    "\n",
    "    # In/out selection in phase-distance (assuming phase already centered on 0 in days)\n",
    "    in_mask  = (np.abs(phase) < in_w)\n",
    "    out_mask = ((phase >  out_w[0]) & (phase <  out_w[1])) | ((phase < -out_w[0]) & (phase > -out_w[1]))\n",
    "\n",
    "    # robust medians\n",
    "    f_in  = np.nanmedian(flux[in_mask])  if np.any(in_mask)  else np.nan\n",
    "    f_out = np.nanmedian(flux[out_mask]) if np.any(out_mask) else np.nan\n",
    "    depth = max(0.0, (f_out - f_in))  # depth as positive number (flux drop)\n",
    "    # duration estimate from width where flux < (f_out - 0.5*depth)\n",
    "    thr = f_out - 0.5*depth\n",
    "    core = (flux < thr) & (np.abs(phase) < halfw)\n",
    "    if np.any(core):\n",
    "        # contiguous region around 0\n",
    "        x = np.abs(phase[core])\n",
    "        duration_days = 2.0 * np.nanpercentile(x, 95)  # robust width\n",
    "    else:\n",
    "        duration_days = 0.08  # ~2 hr default\n",
    "    return float(depth), float(duration_days), in_mask\n",
    "\n",
    "def fold_and_bin(t, f, P, T0, nbins=180, halfw=0.15):\n",
    "    phase = ((t - T0) % P)\n",
    "    phase[phase > P/2] -= P       # center around 0\n",
    "    # Convert to \"phase in days\" so windows are in days directly\n",
    "    ph_days = phase\n",
    "    # bin\n",
    "    edges = np.linspace(-halfw, halfw, nbins+1)\n",
    "    idx = np.digitize(ph_days, edges) - 1\n",
    "    xc  = 0.5*(edges[:-1]+edges[1:])\n",
    "    yb  = np.array([np.nanmedian(f[idx==i]) if np.any(idx==i) else np.nan for i in range(nbins)])\n",
    "    return ph_days, xc, yb\n",
    "\n",
    "def figure_A_panel(tic, toi, t, f, prefer_robust=False, nbins=180, halfw=0.15):\n",
    "    ep = pick_ephemeris_for_folding(tic, prefer_robust=prefer_robust)\n",
    "    if not ep:\n",
    "        print(f\"[FigA] TIC {tic}: no ephemeris; skipping.\")\n",
    "        return\n",
    "    P, T0, src = ep[\"P\"], ep[\"T0\"], ep[\"src\"]\n",
    "\n",
    "    ph_days, xb, yb = fold_and_bin(t, f, P, T0, nbins=nbins, halfw=halfw)\n",
    "    # quick boxfit on binned to stabilize\n",
    "    depth, dur_d, _ = quick_boxfit(xb, yb, halfw=halfw)\n",
    "    # build a box model on binned x\n",
    "    model = np.full_like(xb, np.nan)\n",
    "    in_box = (np.abs(xb) <= dur_d/2)\n",
    "    # baseline is around 1.0 since we normalized\n",
    "    baseline = np.nanmedian(yb[np.abs(xb) > 0.07])\n",
    "    model[:] = baseline\n",
    "    model[in_box] = baseline - depth\n",
    "\n",
    "    # residuals\n",
    "    resid = yb - model\n",
    "\n",
    "    # plot\n",
    "    Path(\"figures\").mkdir(exist_ok=True)\n",
    "    fig = plt.figure(figsize=(8.6,5.6), dpi=140)\n",
    "    gs = fig.add_gridspec(2,1, height_ratios=[2.5,1.0], hspace=0.15)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax1.plot(ph_days, f, \".\", ms=1, alpha=0.25)\n",
    "    ax1.plot(xb, yb, \"-\", lw=1.5, label=\"binned\")\n",
    "    ax1.plot(xb, model, \"-\", lw=1.8, label=\"box model\")\n",
    "    ax1.set_xlim(-halfw, halfw)\n",
    "    ax1.set_ylabel(\"Normalized flux\")\n",
    "    ax1.set_title(f\"{toi} — TIC {tic}  |  Folded on {src} ephemeris  |  P={P:.6f} d, T0={T0:.6f} BTJD\")\n",
    "    ax1.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
    "    ax2.axhline(0, color=\"k\", lw=1, alpha=0.4)\n",
    "    ax2.plot(xb, resid, \"-\", lw=1.2)\n",
    "    ax2.set_xlim(-halfw, halfw)\n",
    "    ax2.set_xlabel(\"Phase (days)\")\n",
    "    ax2.set_ylabel(\"Residual\")\n",
    "\n",
    "    out_png = f\"figures/TIC{tic}_fold_model.png\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_png); plt.close(fig)\n",
    "\n",
    "    # save quick boxfit numbers\n",
    "    Path(\"results\").mkdir(exist_ok=True)\n",
    "    box = {\n",
    "        \"depth\": depth,                # (unitless flux), ~depth_ppm ≈ depth*1e6\n",
    "        \"depth_ppm\": depth*1e6,\n",
    "        \"duration_days\": dur_d,\n",
    "        \"duration_hours\": dur_d*24.0,\n",
    "        \"baseline\": float(baseline),\n",
    "        \"P\": P, \"T0\": T0, \"source\": src\n",
    "    }\n",
    "    with open(f\"results/TIC{tic}_boxfit.json\",\"w\") as fjs:\n",
    "        json.dump(box, fjs, indent=2)\n",
    "\n",
    "    print(f\"[FigA] {toi} TIC {tic}: saved {out_png} and results/TIC{tic}_boxfit.json\")\n",
    "\n",
    "# === run for A–C ===\n",
    "for name, info in TARGETS.items():\n",
    "    tic, toi, secs = info[\"tic\"], info[\"toi\"], info[\"sectors\"]\n",
    "    print(f\"[FigA] building panel for {name} TIC {tic} secs {secs}\")\n",
    "    t, f = stitch_target(tic, secs)\n",
    "    if t is None: \n",
    "        print(\"  -> no data\"); \n",
    "        continue\n",
    "    # Prefer robust only for Target B (folding), refined for A & C\n",
    "    prefer_rob = (name == \"Target B\")\n",
    "    figure_A_panel(tic, toi, t, f, prefer_robust=prefer_rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55f47691-9eda-4c4a-b602-9757c7e6a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results/folding_manifest.json\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "manifest = {\n",
    "  \"TIC119584412\": {\"prefer_robust_for_folding\": False, \"label\": \"Target A\"},\n",
    "  \"TIC37749396\":  {\"prefer_robust_for_folding\": True,  \"label\": \"Target B\"},\n",
    "  \"TIC311183180\": {\"prefer_robust_for_folding\": False, \"label\": \"Target C\"}\n",
    "}\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/folding_manifest.json\",\"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(\"Saved results/folding_manifest.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43e0a365-1e3b-4133-a2eb-9f546ddcdfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results/ephemeris_summary.csv with A–C refined ephemerides.\n"
     ]
    }
   ],
   "source": [
    "import os, json, csv\n",
    "\n",
    "rows = []\n",
    "for name, info in TARGETS.items():\n",
    "    tic = info[\"tic\"]; tag = f\"TIC{tic}\"\n",
    "    pth = f\"results/{tag}_refined_ephemeris.json\"\n",
    "    if not os.path.exists(pth):\n",
    "        continue\n",
    "    d = json.load(open(pth))\n",
    "    rows.append({\n",
    "        \"target\": name, \"tic\": tic,\n",
    "        \"P_days\": d[\"P\"], \"sigma_P_days\": d.get(\"sigma_P\"),\n",
    "        \"T0_BTJD\": d[\"T0\"], \"sigma_T0_days\": d.get(\"sigma_T0\"),\n",
    "        \"cov_T0P\": d.get(\"cov_T0P\"), \"N_mids\": d.get(\"N_mids\"),\n",
    "        \"chi2\": d.get(\"chi2\"), \"rchi2\": d.get(\"rchi2\")\n",
    "    })\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/ephemeris_summary.csv\",\"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(\"Saved results/ephemeris_summary.csv with A–C refined ephemerides.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c24d7dd7-18e2-4a64-b61d-5b71d2fb8150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results/ephemeris_qc.csv\n",
      "Target A: P=16.02749976±2.64e-04 d | T0=1908.046283±9.01e-03 | ρ=-0.749 | N=3 rχ²=1.02 | depth≈98 ppm, dur≈6.53 h [FEW mids]\n",
      "Target B: P=13.47582381±3.50e-04 d | T0=1392.306006±4.33e-02 | ρ=-0.966 | N=4 rχ²=12.04 | depth≈710 ppm, dur≈4.42 h [HIGH rχ²]\n",
      "Target C: P=9.34849077±2.58e-04 d | T0=2149.633454±1.67e-03 | ρ=-0.083 | N=5 rχ²=4.92 | depth≈775 ppm, dur≈5.72 h\n"
     ]
    }
   ],
   "source": [
    "# === Sanity-check refined ephemerides & box-fit depths ===\n",
    "import os, json, csv, math\n",
    "import numpy as np\n",
    "\n",
    "def _load_json(p):\n",
    "    return json.load(open(p)) if os.path.exists(p) else None\n",
    "\n",
    "rows = []\n",
    "for name, info in TARGETS.items():\n",
    "    tic = info[\"tic\"]; tag = f\"TIC{tic}\"\n",
    "    ep = _load_json(f\"results/{tag}_refined_ephemeris.json\")\n",
    "    bx = _load_json(f\"results/{tag}_boxfit.json\")\n",
    "    if not ep: \n",
    "        print(f\"{name}: no refined ephemeris found, skipping.\")\n",
    "        continue\n",
    "    sigma_T0, sigma_P = ep.get(\"sigma_T0\"), ep.get(\"sigma_P\")\n",
    "    cov_T0P = ep.get(\"cov_T0P\", 0.0)\n",
    "    rho = float(cov_T0P / (sigma_T0*sigma_P)) if sigma_T0 and sigma_P else np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"target\": name, \"tic\": tic,\n",
    "        \"P_days\": ep[\"P\"], \"σP_days\": sigma_P,\n",
    "        \"T0_BTJD\": ep[\"T0\"], \"σT0_days\": sigma_T0,\n",
    "        \"rho(T0,P)\": rho,\n",
    "        \"N_mids\": ep.get(\"N_mids\"),\n",
    "        \"rchi2\": ep.get(\"rchi2\"),\n",
    "        \"depth_ppm\": (bx.get(\"depth_ppm\") if bx else None),\n",
    "        \"duration_hr\": (bx.get(\"duration_hours\") if bx else None)\n",
    "    })\n",
    "\n",
    "# Write/print a compact CSV + flag line\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/ephemeris_qc.csv\",\"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(\"Saved results/ephemeris_qc.csv\")\n",
    "for r in rows:\n",
    "    flag = \"\"\n",
    "    if r[\"rchi2\"] and r[\"rchi2\"] > 5: \n",
    "        flag += \" [HIGH rχ²]\"\n",
    "    if r[\"N_mids\"] and r[\"N_mids\"] < 4:\n",
    "        flag += \" [FEW mids]\"\n",
    "    print(f\"{r['target']}: P={r['P_days']:.8f}±{r['σP_days']:.2e} d | \"\n",
    "          f\"T0={r['T0_BTJD']:.6f}±{r['σT0_days']:.2e} | ρ={r['rho(T0,P)']:.3f} | \"\n",
    "          f\"N={r['N_mids']} rχ²={r['rchi2']:.2f} | depth≈{r['depth_ppm']:.0f} ppm, dur≈{r['duration_hr']:.2f} h{flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efd3b2cd-800a-4be0-aada-0459be1624f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[windows] Target A — TIC 119584412\n",
      "Next predicted mid-transits for TIC119584412 (BTJD, ±1σ [min], UTC):\n",
      "  3943.538753, ±39.5 min, 2025-09-25T00:54:39.053\n",
      "  3959.566252, ±39.9 min, 2025-10-11T01:34:15.033\n",
      "  3975.593752, ±40.3 min, 2025-10-27T02:13:51.012\n",
      "  3991.621252, ±40.7 min, 2025-11-12T02:53:26.991\n",
      "  4007.648752, ±41.0 min, 2025-11-28T03:33:02.970\n",
      "  4023.676252, ±41.4 min, 2025-12-14T04:12:38.949\n",
      "  4039.703751, ±41.8 min, 2025-12-30T04:52:14.928\n",
      "  4055.731251, ±42.1 min, 2026-01-15T05:31:50.907\n",
      "Saved figures/TIC119584412_upcoming_windows.png\n",
      "\n",
      "[windows] Target B — TIC 37749396\n",
      "Next predicted mid-transits for TIC37749396 (BTJD, ±1σ [min], UTC):\n",
      "  3939.236706, ±38.4 min, 2025-09-20T17:39:42.224\n",
      "  3952.712530, ±38.8 min, 2025-10-04T05:04:53.402\n",
      "  3966.188354, ±39.3 min, 2025-10-17T16:30:04.579\n",
      "  3979.664178, ±39.8 min, 2025-10-31T03:55:15.756\n",
      "  3993.140001, ±40.2 min, 2025-11-13T15:20:26.933\n",
      "  4006.615825, ±40.7 min, 2025-11-27T02:45:38.110\n",
      "  4020.091649, ±41.2 min, 2025-12-10T14:10:49.287\n",
      "  4033.567473, ±41.6 min, 2025-12-24T01:36:00.463\n",
      "Saved figures/TIC37749396_upcoming_windows.png\n",
      "\n",
      "[windows] Target C — TIC 311183180\n",
      "Next predicted mid-transits for TIC311183180 (BTJD, ±1σ [min], UTC):\n",
      "  3944.543683, ±71.0 min, 2025-09-26T01:01:45.001\n",
      "  3953.892173, ±71.4 min, 2025-10-05T09:23:34.604\n",
      "  3963.240664, ±71.8 min, 2025-10-14T17:45:24.207\n",
      "  3972.589155, ±72.1 min, 2025-10-24T02:07:13.809\n",
      "  3981.937646, ±72.5 min, 2025-11-02T10:29:03.412\n",
      "  3991.286137, ±72.9 min, 2025-11-11T18:50:53.014\n",
      "  4000.634627, ±73.3 min, 2025-11-21T03:12:42.617\n",
      "  4009.983118, ±73.6 min, 2025-11-30T11:34:32.220\n",
      "Saved figures/TIC311183180_upcoming_windows.png\n"
     ]
    }
   ],
   "source": [
    "# === Cell 17 — Fig C-style \"upcoming windows\" panel (bug-fixed) ===\n",
    "import os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.time import Time\n",
    "\n",
    "BTJD_ZERO = 2457000.0\n",
    "\n",
    "def _load_windows(tic):\n",
    "    p = f\"results/TIC{tic}_windows_to_2026-03.json\"\n",
    "    if not os.path.exists(p):\n",
    "        return None\n",
    "    return json.load(open(p))\n",
    "\n",
    "def _load_ephem(tic):\n",
    "    p = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "    if not os.path.exists(p):\n",
    "        return None\n",
    "    return json.load(open(p))\n",
    "\n",
    "def _btjd_to_utc_isot(btjd_array):\n",
    "    jd = np.asarray(btjd_array) + BTJD_ZERO\n",
    "    # ephemeris is in TDB; convert to UTC ISO for human readability\n",
    "    return Time(jd, format=\"jd\", scale=\"tdb\").utc.isot\n",
    "\n",
    "def show_next_windows(tic, n_show=8, print_only=False):\n",
    "    W = _load_windows(tic)\n",
    "    if not W:\n",
    "        print(f\"TIC{tic}: no windows file found.\")\n",
    "        return None\n",
    "\n",
    "    # Extract arrays once; DO NOT re-index floats later\n",
    "    w_btjd   = np.array([row[\"Tpred_BTJD\"] for row in W], dtype=float)\n",
    "    w_sig_d  = np.array([row[\"sigma1d_days\"] for row in W], dtype=float)\n",
    "    w_sig_min= (w_sig_d * 24.0 * 60.0)\n",
    "\n",
    "    # Keep \"future-ish\" windows relative to now (TDB)\n",
    "    now_btjd = Time.now().tdb.jd - BTJD_ZERO\n",
    "    future   = w_btjd >= now_btjd\n",
    "    w_btjd, w_sig_min = w_btjd[future], w_sig_min[future]\n",
    "\n",
    "    # Format UTC stamps\n",
    "    w_dates  = _btjd_to_utc_isot(w_btjd)\n",
    "\n",
    "    # Package and print the next n_show rows\n",
    "    nxt = list(zip(w_btjd, w_sig_min, w_dates))[:n_show]\n",
    "    print(f\"Next predicted mid-transits for TIC{tic} (BTJD, ±1σ [min], UTC):\")\n",
    "    for tb, smin, ds in nxt:\n",
    "        print(f\"  {tb:.6f}, ±{smin:.1f} min, {ds}\")\n",
    "    if print_only:\n",
    "        return nxt\n",
    "\n",
    "    # Simple bar plot of the next n_show windows (±1σ half-width in minutes)\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 3.2), dpi=140)\n",
    "    x = np.arange(len(nxt))\n",
    "    halfmins = np.array([s for _, s, _ in nxt], float)\n",
    "    labels   = [ds.replace('T', ' ').replace('Z','')[:16] for *_, ds in nxt]  # YYYY-MM-DD hh:mm\n",
    "    ax.bar(x, halfmins, align=\"center\")\n",
    "    ax.set_ylabel(\"1σ half-width (minutes)\")\n",
    "    ax.set_title(f\"TIC{tic} — upcoming transit windows\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "    fig.tight_layout()\n",
    "    outpng = f\"figures/TIC{tic}_upcoming_windows.png\"\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "    plt.savefig(outpng)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved {outpng}\")\n",
    "    return nxt\n",
    "\n",
    "# Run for A–C\n",
    "for name, info in TARGETS.items():\n",
    "    tic = info[\"tic\"]\n",
    "    print(f\"\\n[windows] {name} — TIC {tic}\")\n",
    "    show_next_windows(tic, n_show=8, print_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f905cf45-194d-4b67-8b0f-2792d96b6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the loader to use search_lightcurve().download() instead of search_lightcurvefile()\n",
    "try:\n",
    "    import lightkurve as lk\n",
    "except Exception as e:\n",
    "    print(\"Lightkurve not available in this kernel:\", e)\n",
    "\n",
    "def load_pdcsap_sector(tic: int, sector: int, download_dir=\"data_raw_fresh\"):\n",
    "    sr = lk.search_lightcurve(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "    if len(sr) == 0:\n",
    "        print(f\"    [loader] No LCF found for TIC {tic} S{sector}\")\n",
    "        return None\n",
    "    lcs = sr.download(download_dir=download_dir)  # returns LightCurve or LightCurveCollection\n",
    "    if lcs is None:\n",
    "        print(f\"    [loader] Download failed for TIC {tic} S{sector}\")\n",
    "        return None\n",
    "    # Prefer PDCSAP if available, else first LC normalized\n",
    "    try:\n",
    "        lc = (lcs.PDCSAP_FLUX if hasattr(lcs, \"PDCSAP_FLUX\") else lcs).remove_nans().normalize()\n",
    "        lc.meta[\"sector\"] = getattr(lcs, \"sector\", sector)\n",
    "        return lc\n",
    "    except Exception as e:\n",
    "        print(f\"    [loader] PDCSAP extract failed TIC {tic} S{sector}:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758fa683-5913-427e-9ce8-72951eff84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, json, os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MidtimeRow:\n",
    "    epoch:int; tmid:float; tmid_err:float\n",
    "\n",
    "def _read_midtimes_csv(path):\n",
    "    rows = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0:  # header\n",
    "                continue\n",
    "            parts = line.strip().split(\",\")\n",
    "            if len(parts) < 3: continue\n",
    "            rows.append(MidtimeRow(int(parts[0]), float(parts[1]), float(parts[2])))\n",
    "    return rows\n",
    "\n",
    "def _wls_fit(E, T, s):\n",
    "    X = np.vstack([np.ones_like(E, float), E.astype(float)]).T\n",
    "    W = np.diag(1.0/np.maximum(s,1e-12)**2)\n",
    "    XtWX = X.T @ W @ X\n",
    "    beta = np.linalg.solve(XtWX, X.T @ W @ T)  # [T0, P]\n",
    "    cov  = np.linalg.inv(XtWX)\n",
    "    resid = T - (X @ beta)\n",
    "    dof   = max(1, len(T)-2)\n",
    "    chi2  = float((resid**2/s**2).sum())\n",
    "    rchi2 = chi2/dof\n",
    "    return beta, cov, resid, rchi2\n",
    "\n",
    "def robust_ephemeris_with_jitter(mid_csv, max_iter=10, clip_sigma=3.5):\n",
    "    mids = _read_midtimes_csv(mid_csv)\n",
    "    if len(mids) < 3:\n",
    "        raise RuntimeError(\"Need ≥3 midtimes for robust fit\")\n",
    "    E  = np.array([m.epoch for m in mids], int)\n",
    "    T  = np.array([m.tmid  for m in mids], float)\n",
    "    s0 = np.array([max(1e-6, m.tmid_err) for m in mids], float)\n",
    "\n",
    "    mask = np.ones_like(T, bool)\n",
    "    jitter = 0.0\n",
    "    for _ in range(max_iter):\n",
    "        s = np.sqrt(s0[mask]**2 + jitter**2)\n",
    "        beta, cov, resid, rchi2 = _wls_fit(E[mask], T[mask], s)\n",
    "        # clip outliers in O–C\n",
    "        oc = resid/np.sqrt(np.maximum(s**2,1e-12))\n",
    "        keep = np.abs(oc) < clip_sigma\n",
    "        # update mask (map back to full)\n",
    "        m2 = mask.copy()\n",
    "        m2[np.where(mask)[0][~keep]] = False\n",
    "        # adjust jitter to target rchi2≈1 (simple secant-like step)\n",
    "        if rchi2 > 1.0:\n",
    "            jitter = np.sqrt(max(0.0, (rchi2-1.0))) * np.median(s0)  # coarse step\n",
    "        else:\n",
    "            jitter *= 0.5\n",
    "        # stop if stable\n",
    "        if np.all(m2 == mask) and abs(rchi2-1.0) < 0.05:\n",
    "            mask = m2\n",
    "            break\n",
    "        mask = m2\n",
    "\n",
    "    # Final fit with settled mask+jitter\n",
    "    s = np.sqrt(s0[mask]**2 + jitter**2)\n",
    "    beta, cov, resid, rchi2 = _wls_fit(E[mask], T[mask], s)\n",
    "\n",
    "    # scale cov by rchi2 (should be ~1, but keep formal)\n",
    "    cov *= rchi2\n",
    "    out = {\n",
    "        \"T0\": float(beta[0]), \"P\": float(beta[1]),\n",
    "        \"cov\": [[float(cov[0,0]), float(cov[0,1])],\n",
    "                [float(cov[1,0]), float(cov[1,1])]],\n",
    "        \"sigma_T0\": float(np.sqrt(cov[0,0])),\n",
    "        \"sigma_P\":  float(np.sqrt(cov[1,1])),\n",
    "        \"cov_T0P\":  float(cov[0,1]),\n",
    "        \"N_mids_used\": int(mask.sum()),\n",
    "        \"N_mids_total\": int(len(T)),\n",
    "        \"rchi2_final\": float(rchi2),\n",
    "        \"jitter_days\": float(jitter),\n",
    "        \"dropped_epochs\": [int(E[i]) for i in np.where(~mask)[0]],\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4e6f04-eab8-4b8e-88fd-7874ae0af82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[robust] Target A — TIC 119584412\n",
      "  -> saved refined_ephemeris_robust.json, windows_to_2026-03_robust.json, OC_robust.png\n",
      "[robust] Target B — TIC 37749396\n",
      "  -> saved refined_ephemeris_robust.json, windows_to_2026-03_robust.json, OC_robust.png\n",
      "[robust] Target C — TIC 311183180\n",
      "  -> saved refined_ephemeris_robust.json, windows_to_2026-03_robust.json, OC_robust.png\n"
     ]
    }
   ],
   "source": [
    "# === Robust ephemerides + windows (self-contained cell) ===\n",
    "import os, csv, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.time import Time\n",
    "\n",
    "# --------- targets (same IDs you used above) ----------\n",
    "TARGETS = {\n",
    "    \"Target A\": {\"tic\": 119584412, \"toi\": \"TOI 1801.01\"},\n",
    "    \"Target B\": {\"tic\": 37749396,  \"toi\": \"TOI 260.01\"},\n",
    "    \"Target C\": {\"tic\": 311183180, \"toi\": \"TOI 550.02\"},\n",
    "}\n",
    "\n",
    "# --------- helpers ----------\n",
    "def _read_midtimes_csv(path):\n",
    "    \"\"\"Return list of dicts: [{'epoch':int,'tmid':float,'err':float}, ...]\"\"\"\n",
    "    out = []\n",
    "    with open(path) as f:\n",
    "        r = csv.DictReader(f)\n",
    "        # Accept either exact headers or any variant containing 'epoch'/'tmid'\n",
    "        for row in r:\n",
    "            e = int(row.get(\"epoch\", row.get(\"Epoch\", row.get(\"n\"))))\n",
    "            t = float(row.get(\"tmid_BTJD\", row.get(\"tmid\", row.get(\"Tmid_BTJD\"))))\n",
    "            s = float(row.get(\"tmid_err_d\", row.get(\"tmid_err\", row.get(\"Sigma_d\", 0.0))))\n",
    "            out.append({\"epoch\": e, \"tmid\": t, \"err\": max(1e-6, s)})\n",
    "    return out\n",
    "\n",
    "def _wls_linear_ephem(E, T, s):\n",
    "    X = np.vstack([np.ones_like(E, float), E.astype(float)]).T  # [T0, P]\n",
    "    W = np.diag(1.0/np.maximum(1e-12, s*s))\n",
    "    XtWX = X.T @ W @ X\n",
    "    beta = np.linalg.solve(XtWX, X.T @ W @ T)  # [T0, P]\n",
    "    cov  = np.linalg.inv(XtWX)\n",
    "    resid = T - (X @ beta)\n",
    "    chi2  = float((resid**2 / np.maximum(1e-12, s*s)).sum())\n",
    "    dof   = max(1, len(T) - 2)\n",
    "    rchi2 = chi2 / dof\n",
    "    return beta, cov, resid, chi2, rchi2\n",
    "\n",
    "def robust_ephemeris_with_jitter(mid_csv, tol=1e-4, max_iter=60):\n",
    "    \"\"\"\n",
    "    Fit linear ephemeris with an additive jitter term in quadrature to reach rchi2~1.\n",
    "    Returns dict: {T0,P,cov, sigmas, cov_T0P, N_mids, chi2, rchi2, jitter_days}\n",
    "    \"\"\"\n",
    "    mids = _read_midtimes_csv(mid_csv)\n",
    "    E = np.array([m[\"epoch\"] for m in mids], int)\n",
    "    T = np.array([m[\"tmid\"]  for m in mids], float)\n",
    "    s0 = np.array([m[\"err\"]   for m in mids], float)\n",
    "\n",
    "    # First pass (no jitter)\n",
    "    beta, cov, resid, chi2, rchi2 = _wls_linear_ephem(E, T, s0)\n",
    "    jitter = 0.0\n",
    "\n",
    "    if rchi2 > 1.0 and len(T) >= 3:\n",
    "        # Binary search for jitter so reduced-chi2 ~ 1\n",
    "        lo, hi = 0.0, 0.5  # days; hi is safely large\n",
    "        for _ in range(max_iter):\n",
    "            mid = 0.5*(lo+hi)\n",
    "            _, _, _, chi2_mid, rchi2_mid = _wls_linear_ephem(E, T, np.sqrt(s0**2 + mid**2))\n",
    "            if rchi2_mid > 1.0:\n",
    "                lo = mid\n",
    "            else:\n",
    "                hi = mid\n",
    "            if abs(rchi2_mid - 1.0) < tol:\n",
    "                jitter = mid\n",
    "                break\n",
    "        else:\n",
    "            jitter = hi\n",
    "        # Final fit with jitter applied\n",
    "        s = np.sqrt(s0**2 + jitter**2)\n",
    "        beta, cov, resid, chi2, rchi2 = _wls_linear_ephem(E, T, s)\n",
    "    else:\n",
    "        s = s0\n",
    "\n",
    "    # Scale covariance by rchi2 (standard WLS practice)\n",
    "    cov = cov * max(1.0, rchi2)\n",
    "    out = {\n",
    "        \"T0\": float(beta[0]),\n",
    "        \"P\":  float(beta[1]),\n",
    "        \"cov\": [[float(cov[0,0]), float(cov[0,1])],\n",
    "                [float(cov[1,0]), float(cov[1,1])]],\n",
    "        \"sigma_T0\": float(np.sqrt(cov[0,0])),\n",
    "        \"sigma_P\":  float(np.sqrt(cov[1,1])),\n",
    "        \"cov_T0P\":  float(cov[0,1]),\n",
    "        \"N_mids\":   int(len(T)),\n",
    "        \"chi2\":     float(chi2),\n",
    "        \"rchi2\":    float(rchi2),\n",
    "        \"jitter_days\": float(jitter),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def compute_windows_from_fit(fit, t_ref_min, t_ref_max, k_sigma=1.0):\n",
    "    T0, P = fit[\"T0\"], fit[\"P\"]\n",
    "    cov = np.array(fit[\"cov\"], float)\n",
    "    kmin = int(np.floor((t_ref_min - T0)/P)) - 1\n",
    "    kmax = int(np.ceil((t_ref_max - T0)/P)) + 1\n",
    "    rows = []\n",
    "    for k in range(kmin, kmax+1):\n",
    "        Tpred = T0 + k*P\n",
    "        if not (t_ref_min <= Tpred <= t_ref_max):\n",
    "            continue\n",
    "        J = np.array([1.0, float(k)])\n",
    "        sig = float(np.sqrt(max(J @ cov @ J, 0.0)))\n",
    "        rows.append({\"epoch\": k, \"Tpred_BTJD\": float(Tpred),\n",
    "                     \"sigma1d_days\": sig,\n",
    "                     \"window_half_width_1sigma_days\": k_sigma*sig})\n",
    "    return rows\n",
    "\n",
    "def plot_oc_from_midcsv(mid_csv, fit, tag, outpng):\n",
    "    mids = _read_midtimes_csv(mid_csv)\n",
    "    E  = np.array([m[\"epoch\"] for m in mids], int)\n",
    "    T  = np.array([m[\"tmid\"]  for m in mids], float)\n",
    "    s  = np.array([m[\"err\"]   for m in mids], float)\n",
    "    model = fit[\"T0\"] + fit[\"P\"]*E\n",
    "    oc_min = (T - model) * 24.0 * 60.0\n",
    "    plt.figure(figsize=(7.8,3.4), dpi=140)\n",
    "    plt.axhline(0, color=\"k\", lw=1, alpha=0.3)\n",
    "    plt.errorbar(E, oc_min, yerr=s*24*60, fmt=\"o\", ms=4, capsize=2)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"O–C (minutes)\")\n",
    "    plt.title(f\"{tag} — O–C (robust)\")\n",
    "    plt.tight_layout(); plt.savefig(outpng); plt.close()\n",
    "\n",
    "# --------- run for all targets ----------\n",
    "END_DATE = \"2026-03-31\"\n",
    "BTJD_ZERO = 2457000.0\n",
    "t_end = Time(END_DATE, scale=\"utc\").tdb.jd - BTJD_ZERO\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "for name, info in TARGETS.items():\n",
    "    tic, toi = info[\"tic\"], info[\"toi\"]\n",
    "    mid_csv = f\"results/TIC{tic}_midtimes.csv\"\n",
    "    if not os.path.exists(mid_csv):\n",
    "        print(f\"{name}: no midtimes CSV, skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[robust] {name} — TIC {tic}\")\n",
    "    fit_r = robust_ephemeris_with_jitter(mid_csv)\n",
    "    with open(f\"results/TIC{tic}_refined_ephemeris_robust.json\",\"w\") as f:\n",
    "        json.dump(fit_r, f, indent=2)\n",
    "\n",
    "    # windows from first measured midtime to END_DATE\n",
    "    tmin = float(np.loadtxt(mid_csv, delimiter=\",\", skiprows=1, usecols=1).min())\n",
    "    W = compute_windows_from_fit(fit_r, t_ref_min=tmin, t_ref_max=float(t_end))\n",
    "    with open(f\"results/TIC{tic}_windows_to_2026-03_robust.json\",\"w\") as f:\n",
    "        json.dump(W, f, indent=2)\n",
    "\n",
    "    plot_oc_from_midcsv(mid_csv, fit_r, f\"{toi} ({name})\", f\"figures/TIC{tic}_OC_robust.png\")\n",
    "    print(\"  -> saved refined_ephemeris_robust.json, windows_to_2026-03_robust.json, OC_robust.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f138523-f373-47f1-baff-6f31622a7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Figure C prototype: transit-window drift (catalog vs. yours) ===\n",
    "import os, json, csv, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from astropy.time import Time\n",
    "\n",
    "# ---- targets (reuse your mapping) ----\n",
    "TARGETS = {\n",
    "    \"Target A — TIC 119584412\": {\"tic\": 119584412, \"toi\": \"TOI 1801.01\"},\n",
    "    \"Target B — TIC 37749396\":  {\"tic\": 37749396,  \"toi\": \"TOI 260.01\"},\n",
    "    \"Target C — TIC 311183180\": {\"tic\": 311183180, \"toi\": \"TOI 550.02\"},\n",
    "}\n",
    "\n",
    "BTJD_ZERO = 2457000.0\n",
    "\n",
    "@dataclass\n",
    "class Ephem:\n",
    "    P: float\n",
    "    T0: float\n",
    "    cov: np.ndarray | None  # 2x2 (T0,P) covariance or None\n",
    "    source: str\n",
    "\n",
    "def _load_your_ephem(tic: int, robust=False) -> Ephem | None:\n",
    "    base = f\"results/TIC{tic}_refined_ephemeris{'_robust' if robust else ''}.json\"\n",
    "    if not os.path.exists(base):\n",
    "        return None\n",
    "    d = json.load(open(base))\n",
    "    cov = np.array(d[\"cov\"], float) if (\"cov\" in d) else None\n",
    "    return Ephem(P=float(d[\"P\"]), T0=float(d[\"T0\"]), cov=cov, source=(\"yours_robust\" if robust else \"yours\"))\n",
    "\n",
    "def _load_catalog_ephem(tic: int) -> Ephem | None:\n",
    "    \"\"\"\n",
    "    Priority:\n",
    "      1) results/TIC<tic>_catalog_ephemeris.json   (keys: P, T0, optional cov or sigma entries)\n",
    "      2) results/TIC<tic>_stitched_TLS_top3.csv    (best SDE row; uses period_days + optional T0_BTJD)\n",
    "    \"\"\"\n",
    "    # 1) JSON\n",
    "    pjson = f\"results/TIC{tic}_catalog_ephemeris.json\"\n",
    "    if os.path.exists(pjson):\n",
    "        d = json.load(open(pjson))\n",
    "        P  = float(d[\"P\"])\n",
    "        T0 = float(d.get(\"T0_BTJD\", d.get(\"T0\", np.nan)))\n",
    "        cov = None\n",
    "        if \"cov\" in d:\n",
    "            cov = np.array(d[\"cov\"], float)\n",
    "        elif \"sigma_P\" in d or \"sigma_T0\" in d:\n",
    "            # Build diagonal cov if sigmas are provided\n",
    "            sP  = float(d.get(\"sigma_P\", np.nan))\n",
    "            sT0 = float(d.get(\"sigma_T0\", np.nan))\n",
    "            if np.isfinite(sP) and np.isfinite(sT0):\n",
    "                cov = np.array([[sT0**2, 0.0],[0.0, sP**2]])\n",
    "        if np.isfinite(P) and np.isfinite(T0):\n",
    "            return Ephem(P=P, T0=T0, cov=cov, source=\"catalog_json\")\n",
    "    # 2) CSV fallback (TLS top-3)\n",
    "    pcsv = f\"results/TIC{tic}_stitched_TLS_top3.csv\"\n",
    "    if os.path.exists(pcsv):\n",
    "        best = None\n",
    "        with open(pcsv) as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                try:\n",
    "                    P  = float(row[\"period_days\"])\n",
    "                    T0 = float(row.get(\"T0_BTJD\", \"nan\"))\n",
    "                    SDE = float(row.get(\"SDE\", \"nan\"))\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if not math.isfinite(P):\n",
    "                    continue\n",
    "                # prefer highest SDE\n",
    "                if best is None or (math.isfinite(SDE) and SDE > best[2]):\n",
    "                    best = (P, T0, SDE)\n",
    "        if best:\n",
    "            P, T0, SDE = best\n",
    "            # TLS fallback rarely has cov; use None (plot will still show Tpred line w/o bands)\n",
    "            # If T0 missing, we’ll align epochs by P only; bands will still be informative.\n",
    "            if not math.isfinite(T0):\n",
    "                # Anchor T0 to the first available window from your solution later when plotting\n",
    "                T0 = np.nan\n",
    "            return Ephem(P=float(P), T0=float(T0), cov=None, source=\"tls_top1\")\n",
    "    return None\n",
    "\n",
    "def _sigma_time_from_cov(ep: Ephem, k: int) -> float:\n",
    "    \"\"\"Return 1σ timing (days) at epoch k using cov if available; else NaN.\"\"\"\n",
    "    if ep.cov is None:\n",
    "        return np.nan\n",
    "    J = np.array([1.0, float(k)])  # dT/d[T0,P]\n",
    "    v = J @ ep.cov @ J\n",
    "    return float(np.sqrt(max(v, 0.0)))\n",
    "\n",
    "def _btjd_from_datestr(date_str: str) -> float:\n",
    "    t = Time(date_str, scale=\"utc\")\n",
    "    return float(t.tdb.jd - BTJD_ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c534c652-74c7-4673-8613-42d851cf8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epoch_grid(T0_ref: float, P: float, tmin: float, tmax: float):\n",
    "    \"\"\"Return integer epoch indices spanning [tmin, tmax] around T0_ref given P.\"\"\"\n",
    "    if not np.isfinite(T0_ref):\n",
    "        # If T0 missing, anchor near tmin\n",
    "        T0_ref = tmin\n",
    "    kmin = int(np.floor((tmin - T0_ref)/P)) - 2\n",
    "    kmax = int(np.ceil((tmax - T0_ref)/P)) + 2\n",
    "    return np.arange(kmin, kmax+1, dtype=int)\n",
    "\n",
    "def compute_curves(ep_yours: Ephem, ep_cat: Ephem | None, tmin: float, tmax: float):\n",
    "    \"\"\"\n",
    "    Build arrays for plotting:\n",
    "      epochs (k), Tpred_yours, sigma_yours, Tpred_cat, sigma_cat\n",
    "    \"\"\"\n",
    "    k = make_epoch_grid(ep_yours.T0, ep_yours.P, tmin, tmax)\n",
    "\n",
    "    # Your curve\n",
    "    T_y = ep_yours.T0 + k * ep_yours.P\n",
    "    S_y = np.array([_sigma_time_from_cov(ep_yours, int(kk)) for kk in k], float)\n",
    "\n",
    "    # Catalog curve (if present). If catalog T0 is NaN, align T0 to your first T_y for visual compare.\n",
    "    if ep_cat is not None:\n",
    "        T0c = ep_cat.T0 if np.isfinite(ep_cat.T0) else T_y[0]\n",
    "        T_c = T0c + k * ep_cat.P\n",
    "        S_c = np.array([_sigma_time_from_cov(ep_cat, int(kk)) for kk in k], float)\n",
    "    else:\n",
    "        T_c = np.full_like(T_y, np.nan)\n",
    "        S_c = np.full_like(S_y, np.nan)\n",
    "\n",
    "    return k, T_y, S_y, T_c, S_c\n",
    "\n",
    "def minutes(x_days):  # helper for plotting labels\n",
    "    return float(x_days) * 24.0 * 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08383386-2c43-4993-bc79-0301080c7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_c(tic: int, name_tag: str, ep_yours: Ephem, ep_cat: Ephem | None,\n",
    "                  tmin: float, tmax: float, outpng: str, annotate_dates=True):\n",
    "    \"\"\"\n",
    "    Panel with two rows:\n",
    "      Top: timing window half-width (1σ, minutes) vs epoch (k)\n",
    "      Bottom: same vs date (UTC) with predicted mid-times\n",
    "    \"\"\"\n",
    "    k, T_y, S_y, T_c, S_c = compute_curves(ep_yours, ep_cat, tmin, tmax)\n",
    "\n",
    "    # Build date labels\n",
    "    dates = Time(BTJD_ZERO + T_y, format=\"jd\", scale=\"tdb\").to_datetime()\n",
    "    # For catalog dates, if T_c is NaN it will be skipped by plotting\n",
    "    dates_c = None\n",
    "    if np.isfinite(T_c).any():\n",
    "        dates_c = Time(BTJD_ZERO + T_c, format=\"jd\", scale=\"tdb\").to_datetime()\n",
    "\n",
    "    fig = plt.figure(figsize=(9.6, 6.0), dpi=140)\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[1,1], hspace=0.25)\n",
    "\n",
    "    # Row 1: epoch space\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    ax1.plot(k, [minutes(s) for s in S_y], \"-\", lw=2, label=f\"Yours ({ep_yours.source})\")\n",
    "    if ep_cat is not None and np.isfinite(S_c).any():\n",
    "        ax1.plot(k, [minutes(s) for s in S_c], \"--\", lw=1.5, label=f\"Catalog ({ep_cat.source})\")\n",
    "    ax1.set_xlabel(\"Epoch (k)\")\n",
    "    ax1.set_ylabel(\"Timing 1σ half-width (minutes)\")\n",
    "    ax1.set_title(f\"{name_tag} — Transit-window growth (epoch space)\")\n",
    "    ax1.grid(True, alpha=0.25)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Row 2: date space\n",
    "    ax2 = fig.add_subplot(gs[1,0])\n",
    "    ax2.plot(dates, [minutes(s) for s in S_y], \"-\", lw=2, label=f\"Yours ({ep_yours.source})\")\n",
    "    if ep_cat is not None and (dates_c is not None):\n",
    "        ax2.plot(dates_c, [minutes(s) for s in S_c], \"--\", lw=1.5, label=f\"Catalog ({ep_cat.source})\")\n",
    "    ax2.set_xlabel(\"Calendar date (UTC)\")\n",
    "    ax2.set_ylabel(\"Timing 1σ half-width (minutes)\")\n",
    "    ax2.set_title(f\"{name_tag} — Transit-window growth (date space)\")\n",
    "    ax2.grid(True, alpha=0.25)\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "\n",
    "    if annotate_dates:\n",
    "        # mark 'today' and Mar 31, 2026\n",
    "        today_btjd = Time.now().tdb.jd - BTJD_ZERO\n",
    "        t_cut = _btjd_from_datestr(\"2026-03-31\")\n",
    "        for ax in (ax1, ax2):\n",
    "            # vertical helpers on ax2 only (dates)\n",
    "            pass\n",
    "        # Only draw vertical lines on the bottom panel\n",
    "        ax2.axvline(Time(BTJD_ZERO + today_btjd, format=\"jd\", scale=\"tdb\").to_datetime(),\n",
    "                    color=\"k\", ls=\":\", lw=1, alpha=0.6)\n",
    "        ax2.axvline(Time(BTJD_ZERO + t_cut, format=\"jd\", scale=\"tdb\").to_datetime(),\n",
    "                    color=\"k\", ls=\":\", lw=1, alpha=0.6)\n",
    "        ax2.text(0.01, 0.93, \"Today\", transform=ax2.transAxes, fontsize=9, alpha=0.7)\n",
    "        ax2.text(0.78, 0.93, \"2026-03-31\", transform=ax2.transAxes, fontsize=9, alpha=0.7)\n",
    "\n",
    "    fig.suptitle(f\"Figure C prototype — TIC {tic}\", y=0.995, fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "    fig.savefig(outpng)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b37841a6-886a-4616-b6a5-dfb38b1d7fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_1982/3604536555.py:58: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FigureC] Saved figures/TIC119584412_FigureC_window_growth.png\n",
      "[FigureC] Saved figures/TIC37749396_FigureC_window_growth.png\n",
      "[FigureC] Saved figures/TIC311183180_FigureC_window_growth.png\n",
      "Saved results/figureC_window_growth_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- Compute & Save: per-target Figure C + drift summary ----\n",
    "END_DATE = \"2026-03-31\"\n",
    "t_end = _btjd_from_datestr(END_DATE)\n",
    "\n",
    "summary_rows = []\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "for tag, info in TARGETS.items():\n",
    "    tic, toi = info[\"tic\"], info[\"toi\"]\n",
    "    name_tag = f\"{toi} ({tag.split('—')[0].strip()})\"\n",
    "\n",
    "    # Load ephemerides\n",
    "    ep_y  = _load_your_ephem(tic) or _load_your_ephem(tic, robust=True)\n",
    "    if ep_y is None:\n",
    "        print(f\"[FigureC] {tag}: no refined ephemeris found; skipping.\")\n",
    "        continue\n",
    "    ep_c  = _load_catalog_ephem(tic)\n",
    "    if ep_c is None:\n",
    "        print(f\"[FigureC] {tag}: no catalog baseline found; plotting 'yours' only.\")\n",
    "\n",
    "    # Set plotting time span from earliest data (if available) to END_DATE\n",
    "    # Prefer your midtimes CSV for earliest point; else use your windows JSON.\n",
    "    tmin = None\n",
    "    mid_csv = f\"results/TIC{tic}_midtimes.csv\"\n",
    "    if os.path.exists(mid_csv):\n",
    "        try:\n",
    "            tvals = np.loadtxt(mid_csv, delimiter=\",\", skiprows=1, usecols=1)\n",
    "            tmin = float(np.nanmin(np.atleast_1d(tvals)))\n",
    "        except Exception:\n",
    "            tmin = None\n",
    "    if tmin is None:\n",
    "        # fallback: earliest predicted window from your windows file\n",
    "        wjson = f\"results/TIC{tic}_windows_to_2026-03.json\"\n",
    "        if os.path.exists(wjson):\n",
    "            W = json.load(open(wjson))\n",
    "            if isinstance(W, list) and len(W):\n",
    "                tmin = float(min([w[\"Tpred_BTJD\"] for w in W]))\n",
    "    if tmin is None:\n",
    "        # final fallback: a few periods before your T0\n",
    "        tmin = ep_y.T0 - 5*ep_y.P\n",
    "\n",
    "    # Build curves and capture a few checkpoints\n",
    "    k, T_y, S_y, T_c, S_c = compute_curves(ep_y, ep_c, tmin, t_end)\n",
    "\n",
    "    # Record 1σ at three reference points: near start, mid, end of range\n",
    "    def pick(vals):\n",
    "        if vals.size == 0: return np.nan, np.nan, np.nan\n",
    "        i0, i1, i2 = 0, vals.size//2, -1\n",
    "        return float(vals[i0]), float(vals[i1]), float(vals[i2])\n",
    "\n",
    "    sy0, sym, sye = [minutes(x) for x in pick(S_y)]\n",
    "    sc0, scm, sce = [minutes(x) for x in pick(S_c)] if (ep_c is not None) else (np.nan, np.nan, np.nan)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"tic\": tic, \"toi\": toi,\n",
    "        \"yours_P_days\": ep_y.P, \"yours_T0_BTJD\": ep_y.T0,\n",
    "        \"yours_sigma_min_start\": sy0, \"yours_sigma_min_mid\": sym, \"yours_sigma_min_end\": sye,\n",
    "        \"catalog_source\": (ep_c.source if ep_c else \"\"),\n",
    "        \"catalog_P_days\": (ep_c.P if ep_c else np.nan),\n",
    "        \"catalog_T0_BTJD\": (ep_c.T0 if (ep_c and np.isfinite(ep_c.T0)) else np.nan),\n",
    "        \"catalog_sigma_min_start\": sc0, \"catalog_sigma_min_mid\": scm, \"catalog_sigma_min_end\": sce\n",
    "    })\n",
    "\n",
    "    # Plot and save\n",
    "    outpng = f\"figures/TIC{tic}_FigureC_window_growth.png\"\n",
    "    plot_figure_c(tic, name_tag, ep_y, ep_c, tmin, t_end, outpng=outpng)\n",
    "    print(f\"[FigureC] Saved {outpng}\")\n",
    "\n",
    "# Write summary CSV\n",
    "if summary_rows:\n",
    "    with open(\"results/figureC_window_growth_summary.csv\",\"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(summary_rows[0].keys()))\n",
    "        w.writeheader(); w.writerows(summary_rows)\n",
    "    print(\"Saved results/figureC_window_growth_summary.csv\")\n",
    "else:\n",
    "    print(\"No targets written; check ephemeris files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c388ce-dcb8-4be9-87f1-a16621b49062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Figure C helpers: robust \"catalog\" ephemeris loader + plotting shim ---\n",
    "\n",
    "import os, csv, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.time import Time\n",
    "\n",
    "BTJD_ZERO = 2457000.0\n",
    "\n",
    "def _btjd_from_iso(s):\n",
    "    return float(Time(s, scale=\"utc\").tdb.jd - BTJD_ZERO)\n",
    "\n",
    "def load_catalog_ephemeris(tic):\n",
    "    \"\"\"\n",
    "    Try, in order:\n",
    "      1) results/TIC{tic}_catalog_ephem.json            (your own pinned catalog copy)\n",
    "      2) results/TIC{tic}_stitched_TLS_top3.csv         (use top-1 as a placeholder \"catalog\")\n",
    "      3) results/TIC{tic}_refined_ephemeris.json        (fallback: use 'yours' also as 'catalog')\n",
    "    Returns: dict {P, T0, label, source} or None if nothing usable.\n",
    "    \"\"\"\n",
    "    # 1) explicit catalog JSON (preferred if you have it)\n",
    "    p_json = f\"results/TIC{tic}_catalog_ephem.json\"\n",
    "    if os.path.exists(p_json):\n",
    "        try:\n",
    "            d = json.load(open(p_json))\n",
    "            if all(k in d for k in (\"P\",\"T0\")):\n",
    "                return {\"P\": float(d[\"P\"]), \"T0\": float(d[\"T0\"]),\n",
    "                        \"label\": \"Catalog\", \"source\": os.path.basename(p_json)}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) stitched TLS top-3; use top-1 as a stand-in for “catalog”\n",
    "    p_tls = f\"results/TIC{tic}_stitched_TLS_top3.csv\"\n",
    "    if os.path.exists(p_tls):\n",
    "        rows = []\n",
    "        with open(p_tls) as f:\n",
    "            r = csv.DictReader(f)\n",
    "            for row in r:\n",
    "                try:\n",
    "                    P = float(row[\"period_days\"])\n",
    "                    # allow T0 column to be missing/NaN; we can backfill below\n",
    "                    T0 = row.get(\"T0_BTJD\", \"\") or \"nan\"\n",
    "                    T0 = float(T0)\n",
    "                    SDE = float(row.get(\"SDE\",\"nan\"))\n",
    "                    rows.append((P, T0, SDE))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if rows:\n",
    "            # choose the highest SDE if present\n",
    "            rows.sort(key=lambda x: (x[2] if np.isfinite(x[2]) else -1), reverse=True)\n",
    "            P, T0, _ = rows[0]\n",
    "            if not np.isfinite(T0):\n",
    "                # If TLS CSV has no T0, take an approximate T0 near the first data point\n",
    "                # using your refined ephemeris if present, otherwise mid-time of stitched data.\n",
    "                p_ref = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "                if os.path.exists(p_ref):\n",
    "                    try:\n",
    "                        j = json.load(open(p_ref))\n",
    "                        T0 = float(j[\"T0\"])\n",
    "                    except Exception:\n",
    "                        T0 = np.nan\n",
    "                if not np.isfinite(T0):\n",
    "                    # final backstop: put T0 at 0 to at least show slope; label that it’s approximate\n",
    "                    T0 = 0.0\n",
    "            return {\"P\": float(P), \"T0\": float(T0),\n",
    "                    \"label\": \"Catalog (tls_top1)\", \"source\": os.path.basename(p_tls)}\n",
    "\n",
    "    # 3) last-resort: mirror your refined ephemeris so something plots\n",
    "    p_ref = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "    if os.path.exists(p_ref):\n",
    "        try:\n",
    "            d = json.load(open(p_ref))\n",
    "            return {\"P\": float(d[\"P\"]), \"T0\": float(d[\"T0\"]),\n",
    "                    \"label\": \"Catalog (fallback=same as yours)\", \"source\": os.path.basename(p_ref)}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def sigma_timing_minutes(P, T0, cov, epochs):\n",
    "    \"\"\"\n",
    "    1σ timing half-width in minutes at integer epochs.\n",
    "    cov is 2x2 on [T0, P]. epochs can be array-like.\n",
    "    \"\"\"\n",
    "    cov = np.asarray(cov, float)\n",
    "    E = np.asarray(epochs, float)\n",
    "    # J = [1, E]; σ = sqrt(J C J^T)\n",
    "    s = np.sqrt(np.clip(cov[0,0] + 2*E*cov[0,1] + (E**2)*cov[1,1], 0, np.inf))\n",
    "    return s * 24 * 60  # days -> minutes\n",
    "\n",
    "\n",
    "def plot_window_growth_panels(tic, toi_label, yours, catalog, t_min_btjd, t_max_btjd, outpng):\n",
    "    \"\"\"\n",
    "    yours: dict with P, T0, cov\n",
    "    catalog: dict with P, T0, label, source (may be None)\n",
    "    \"\"\"\n",
    "    # epoch space\n",
    "    E = np.arange(-10, 220+1)  # generous\n",
    "    s_yours = sigma_timing_minutes(yours[\"P\"], yours[\"T0\"], yours[\"cov\"], E)\n",
    "\n",
    "    # date space (convert BTJD span to a set of dates)\n",
    "    btjd_grid = np.linspace(t_min_btjd, t_max_btjd, 120)\n",
    "    E_grid = (btjd_grid - yours[\"T0\"]) / yours[\"P\"]\n",
    "    s_date_yours = sigma_timing_minutes(yours[\"P\"], yours[\"T0\"], yours[\"cov\"], E_grid)\n",
    "    dates = Time(btjd_grid + BTJD_ZERO, format=\"jd\", scale=\"tdb\").utc.datetime\n",
    "\n",
    "    fig = plt.figure(figsize=(10,6.4), dpi=140)\n",
    "\n",
    "    # Top: epoch space\n",
    "    ax1 = fig.add_subplot(2,1,1)\n",
    "    ax1.plot(E, s_yours, lw=2, label=\"Yours (yours)\")\n",
    "    if catalog is not None:\n",
    "        # We don’t have a covariance for catalog; approximate growth with your Cov(P,T0)\n",
    "        # but centered on catalog (P, T0) so the curve is visible.\n",
    "        s_cat = sigma_timing_minutes(yours[\"P\"], catalog[\"T0\"], yours[\"cov\"], E * (catalog[\"P\"]/yours[\"P\"]))\n",
    "        ax1.plot(E, s_cat, \"--\", lw=2, label=catalog[\"label\"])\n",
    "    ax1.set_xlabel(\"Transit epoch (k)\")\n",
    "    ax1.set_ylabel(\"Timing 1σ half-width (minutes)\")\n",
    "    ax1.set_title(f\"{toi_label} — Transit-window growth (epoch space)\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # Bottom: dates\n",
    "    ax2 = fig.add_subplot(2,1,2)\n",
    "    ax2.plot(dates, s_date_yours, lw=2, label=\"Yours (yours)\")\n",
    "    if catalog is not None:\n",
    "        # Map catalog ephemeris onto the same date grid (again reusing your cov as a proxy)\n",
    "        E_grid_cat = (btjd_grid - catalog[\"T0\"]) / catalog[\"P\"]\n",
    "        s_date_cat = sigma_timing_minutes(yours[\"P\"], catalog[\"T0\"], yours[\"cov\"], E_grid_cat)\n",
    "        ax2.plot(dates, s_date_cat, \"--\", lw=2, label=catalog[\"label\"])\n",
    "    ax2.set_ylabel(\"Timing 1σ half-width (minutes)\")\n",
    "    ax2.set_title(f\"{toi_label} — Transit-window growth (date space)\")\n",
    "    ax2.legend()\n",
    "    for x in [Time(\"2025-12-31\").utc.datetime, Time(\"2026-03-31\").utc.datetime]:\n",
    "        ax2.axvline(x, color=\"k\", ls=\":\", lw=1)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(outpng)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6fb66d-fff8-4edb-933e-86f73185067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_catalog_ephem(tic:int, P:float, T0:float, label=\"Catalog (manual)\"):\n",
    "    import json, os\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    path = f\"results/TIC{tic}_catalog_ephem.json\"\n",
    "    json.dump({\"P\": float(P), \"T0\": float(T0), \"label\": label}, open(path,\"w\"), indent=2)\n",
    "    print(f\"[catalog] wrote {path}  P={P:.8f}  T0={T0:.6f}  ({label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e233499-fa35-4294-87ab-c05ec708f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_catalog_from_tls(tic:int, label=\"Catalog (tls_top1)\"):\n",
    "    import csv, json, math, os\n",
    "    p = f\"results/TIC{tic}_stitched_TLS_top3.csv\"\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"[catalog] no TLS top-3 for TIC{tic}\")\n",
    "        return\n",
    "    rows=[]\n",
    "    with open(p) as f:\n",
    "        for r in csv.DictReader(f):\n",
    "            try:\n",
    "                P=float(r[\"period_days\"])\n",
    "                T0=float(r.get(\"T0_BTJD\",\"nan\"))\n",
    "                SDE=float(r.get(\"SDE\",\"nan\"))\n",
    "                rows.append((P,T0,SDE))\n",
    "            except: pass\n",
    "    if not rows:\n",
    "        print(f\"[catalog] TLS file empty for TIC{tic}\")\n",
    "        return\n",
    "    rows.sort(key=lambda x: (x[2] if math.isfinite(x[2]) else -1), reverse=True)\n",
    "    P,T0,_=rows[0]\n",
    "    if not math.isfinite(T0):\n",
    "        from json import load\n",
    "        rp=f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "        T0=float(load(open(rp))[\"T0\"]) if os.path.exists(rp) else 0.0\n",
    "    write_catalog_ephem(tic,P,T0,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a649fb70-90f7-4841-a509-fe967404e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FigureC] TOI 1801.01 TIC 119584412: catalog source=TIC119584412_stitched_TLS_top3.csv (Catalog (tls_top1)); P=16.027187, T0=1908.062441\n",
      "[FigureC] Saved figures/TIC119584412_FigureC_window_growth.png\n",
      "[FigureC] TOI 260.01 TIC 37749396: catalog source=TIC37749396_stitched_TLS_top3.csv (Catalog (tls_top1)); P=13.475725, T0=1392.311964\n",
      "[FigureC] Saved figures/TIC37749396_FigureC_window_growth.png\n",
      "[FigureC] TOI 550.02 TIC 311183180: catalog source=TIC311183180_stitched_TLS_top3.csv (Catalog (tls_top1)); P=9.348442, T0=2149.633067\n",
      "[FigureC] Saved figures/TIC311183180_FigureC_window_growth.png\n"
     ]
    }
   ],
   "source": [
    "# Rebuild Figure C panels with explicit catalog prints\n",
    "for name, info in TARGETS.items():\n",
    "    tic, toi = info[\"tic\"], info[\"toi\"] \n",
    "    # load YOUR refined ephemeris (robust one if you prefer)\n",
    "    ep_path = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "    if not os.path.exists(ep_path):\n",
    "        ep_path = f\"results/TIC{tic}_refined_ephemeris_robust.json\"\n",
    "    ep = json.load(open(ep_path))\n",
    "    yours = {\"P\": ep[\"P\"], \"T0\": ep[\"T0\"], \"cov\": ep[\"cov\"]}\n",
    "\n",
    "    catalog = load_catalog_ephemeris(tic)\n",
    "    if catalog is None:\n",
    "        print(f\"[FigureC] {toi} TIC {tic}: no catalog ephemeris found — plotting yours only.\")\n",
    "    else:\n",
    "        print(f\"[FigureC] {toi} TIC {tic}: catalog source={catalog['source']} \"\n",
    "              f\"({catalog['label']}); P={catalog['P']:.6f}, T0={catalog['T0']:.6f}\")\n",
    "\n",
    "    # choose sensible date limits from your data span out to 2026-03-31\n",
    "    # (If you saved stitched times 't', you can use those; here, pick from midtime CSV if present.)\n",
    "    mid_csv = f\"results/TIC{tic}_midtimes.csv\"\n",
    "    if os.path.exists(mid_csv):\n",
    "        tmin = np.loadtxt(mid_csv, delimiter=\",\", skiprows=1, usecols=1)\n",
    "        if np.ndim(tmin) > 0: tmin = np.min(tmin)\n",
    "        t_min_btjd = float(tmin)\n",
    "    else:\n",
    "        # fall back to a few periods around T0\n",
    "        t_min_btjd = yours[\"T0\"] - 5*yours[\"P\"]\n",
    "    t_max_btjd = _btjd_from_iso(\"2026-03-31\")\n",
    "\n",
    "    outpng = f\"figures/TIC{tic}_FigureC_window_growth.png\"\n",
    "    plot_window_growth_panels(tic, f\"{toi} (Target {name.split()[-1]})\", yours, catalog,\n",
    "                              t_min_btjd, t_max_btjd, outpng)\n",
    "    print(f\"[FigureC] Saved {outpng}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce57f154-df1c-470f-94ac-0954eede921f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure C – timing 1σ half-width at 2026-03-31 (minutes)\n",
      "target,tic,yours_min,catalog_min,delta_min,ΔP_sec,ΔT0_min,cat_source\n",
      "Target A — TIC 119584412,119584412,43.89,43.89,-0.00,-26.99,+23.27,TIC119584412_stitched_TLS_top3.csv\n",
      "Target B — TIC 37749396,37749396,44.98,44.98,-0.00,-8.55,+8.58,TIC37749396_stitched_TLS_top3.csv\n",
      "Target C — TIC 311183180,311183180,78.41,78.41,+0.00,-4.24,-0.56,TIC311183180_stitched_TLS_top3.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Quantify & annotate Figure C gaps at 2026-03-31 ---\n",
    "\n",
    "import os, json, numpy as np\n",
    "from astropy.time import Time\n",
    "BTJD_ZERO = 2457000.0\n",
    "\n",
    "def sigma_minutes_at(btjd, P, T0, cov):\n",
    "    # 1σ timing half-width in minutes at an absolute BTJD time\n",
    "    E = (btjd - T0) / P\n",
    "    C = np.asarray(cov, float)\n",
    "    sig_days = np.sqrt(max(C[0,0] + 2*E*C[0,1] + (E**2)*C[1,1], 0.0))\n",
    "    return float(sig_days * 24 * 60)\n",
    "\n",
    "t_end = Time(\"2026-03-31\", scale=\"utc\").tdb.jd - BTJD_ZERO\n",
    "\n",
    "print(\"Figure C – timing 1σ half-width at 2026-03-31 (minutes)\")\n",
    "print(\"target,tic,yours_min,catalog_min,delta_min,ΔP_sec,ΔT0_min,cat_source\")\n",
    "\n",
    "for name, info in TARGETS.items():\n",
    "    tic, toi = info[\"tic\"], info[\"toi\"]\n",
    "    # yours (robust if present)\n",
    "    ep_path = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "    if not os.path.exists(ep_path):\n",
    "        ep_path = f\"results/TIC{tic}_refined_ephemeris_robust.json\"\n",
    "    ep = json.load(open(ep_path))\n",
    "    yours = {\"P\": float(ep[\"P\"]), \"T0\": float(ep[\"T0\"]), \"cov\": ep[\"cov\"]}\n",
    "\n",
    "    # catalog (whatever your loader finds)\n",
    "    cat = load_catalog_ephemeris(tic)\n",
    "    if cat is None:\n",
    "        print(f\"{name},{tic},(no-cat)\")\n",
    "        continue\n",
    "\n",
    "    s_y = sigma_minutes_at(t_end, yours[\"P\"], yours[\"T0\"], yours[\"cov\"])\n",
    "    s_c = sigma_minutes_at(t_end, yours[\"P\"], cat[\"T0\"], yours[\"cov\"])  # reuse your cov as proxy\n",
    "    dP_sec  = (cat[\"P\"] - yours[\"P\"]) * 86400.0\n",
    "    dT0_min = (cat[\"T0\"] - yours[\"T0\"]) * 1440.0\n",
    "    print(f\"{name},{tic},{s_y:0.2f},{s_c:0.2f},{(s_c-s_y):+0.2f},{dP_sec:+0.2f},{dT0_min:+0.2f},{cat['source']}\")\n",
    "\n",
    "    # Optional: drop a small text tag onto the bottom panel you just saved\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.dates import date2num\n",
    "    fig = plt.imread(f\"figures/TIC{tic}_FigureC_window_growth.png\")  # cheap: just log numbers; leave image as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f5458b-3453-41f9-951f-d76fb1f22522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd/Even & Secondary tests (depths in ppm; p-values from bootstrap)\n",
      "\n",
      "Target A — TIC 119584412 — TOI 1801.01 (TIC 119584412)\n",
      "  P=16.02749976 d, T0=1908.046283 BTJD, duration_used≈6.53 h\n",
      "  In-transit points: odd=7387, even=7486; secondary window=3518\n",
      "  Odd mean depth ≈ 414 ppm, Even ≈ 439 ppm → Δ(odd-even) ≈ -25 ppm;  p=0.232\n",
      "  Secondary mean depth ≈ -547 ppm vs 0;  p=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target B — TIC 37749396 — TOI 260.01 (TIC 37749396)\n",
      "  P=13.47582381 d, T0=1392.306006 BTJD, duration_used≈4.42 h\n",
      "  In-transit points: odd=14243, even=16554; secondary window=22718\n",
      "  Odd mean depth ≈ -59 ppm, Even ≈ 13 ppm → Δ(odd-even) ≈ -72 ppm;  p=0.000\n",
      "  Secondary mean depth ≈ 20 ppm vs 0;  p=0.002\n",
      "\n",
      "Target C — TIC 311183180 — TOI 550.02 (TIC 311183180)\n",
      "  P=9.34849077 d, T0=2149.633454 BTJD, duration_used≈5.72 h\n",
      "  In-transit points: odd=2965, even=7459; secondary window=10769\n",
      "  Odd mean depth ≈ 2,210 ppm, Even ≈ 47 ppm → Δ(odd-even) ≈ +2163 ppm;  p=0.000\n",
      "  Secondary mean depth ≈ -191 ppm vs 0;  p=0.000\n",
      "\n",
      "Saved results/odd_even_secondary_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Odd/even depth tests and secondary-eclipse search (A–C)\n",
    "import os, json, csv, warnings\n",
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "from astropy.time import Time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=lk.LightkurveWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- Target roster (A–C) ---\n",
    "TARGETS = {\n",
    "    \"Target A — TIC 119584412\": {\"tic\": 119584412, \"toi\":\"TOI 1801.01\", \"sectors\":[22,49]},\n",
    "    \"Target B — TIC 37749396\":  {\"tic\": 37749396,  \"toi\":\"TOI 260.01\",  \"sectors\":[3,42,70]},\n",
    "    \"Target C — TIC 311183180\": {\"tic\": 311183180, \"toi\":\"TOI 550.02\",  \"sectors\":[5,31]},\n",
    "}\n",
    "\n",
    "# --- helpers ---\n",
    "def load_refined_ephemeris(tic):\n",
    "    for pth in (f\"results/TIC{tic}_refined_ephemeris.json\",\n",
    "                f\"results/TIC{tic}_refined_ephemeris_robust.json\"):\n",
    "        if os.path.exists(pth):\n",
    "            d = json.load(open(pth))\n",
    "            return float(d[\"P\"]), float(d[\"T0\"])\n",
    "    raise FileNotFoundError(f\"No refined ephemeris JSON for TIC{tic}.\")\n",
    "\n",
    "def load_duration_days_or_default(tic, default_hours=2.0):\n",
    "    pth = f\"results/TIC{tic}_boxfit.json\"\n",
    "    if os.path.exists(pth):\n",
    "        try:\n",
    "            d = json.load(open(pth))\n",
    "            # accept any of these common keys\n",
    "            for k in (\"duration_days\",\"dur_days\",\"duration\"):\n",
    "                if k in d and np.isfinite(d[k]):\n",
    "                    return float(d[k])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return default_hours/24.0  # fallback if no boxfit\n",
    "\n",
    "def stitch_pdcsap(tic, sectors):\n",
    "    lcs = []\n",
    "    for s in sectors:\n",
    "        sr = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\", sector=s)\n",
    "        if len(sr) == 0:\n",
    "            continue\n",
    "        lcf = sr.download()\n",
    "        lc  = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "        lcs.append(lc)\n",
    "    if not lcs:\n",
    "        return None\n",
    "    return lk.LightCurveCollection(lcs).stitch().remove_nans().normalize()\n",
    "\n",
    "def phased_times(t, P, T0):\n",
    "    # center transit at phase ~0 in range (-0.5, 0.5]\n",
    "    ph = ((t - T0)/P + 0.5) % 1.0 - 0.5\n",
    "    # integer epoch index for odd/even split\n",
    "    k = np.round((t - T0)/P).astype(int)\n",
    "    return ph, k\n",
    "\n",
    "def bootstrap_pvalue_two_sample(x, y, nboot=20000, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    x = x[np.isfinite(x)]; y = y[np.isfinite(y)]\n",
    "    if len(x) < 5 or len(y) < 5:\n",
    "        return np.nan, (np.nan, np.nan)\n",
    "    obs = x.mean() - y.mean()\n",
    "    # center under H0 by subtracting group means then adding grand mean\n",
    "    gmean = np.r_[x, y].mean()\n",
    "    x0 = x - x.mean() + gmean\n",
    "    y0 = y - y.mean() + gmean\n",
    "    diffs = []\n",
    "    for _ in range(nboot):\n",
    "        xb = rng.choice(x0, size=len(x0), replace=True)\n",
    "        yb = rng.choice(y0, size=len(y0), replace=True)\n",
    "        diffs.append(xb.mean() - yb.mean())\n",
    "    diffs = np.asarray(diffs)\n",
    "    p = (np.abs(diffs) >= np.abs(obs)).mean()\n",
    "    ci = (np.percentile(diffs, 2.5), np.percentile(diffs, 97.5))\n",
    "    return float(p), (float(ci[0]), float(ci[1]))\n",
    "\n",
    "def bootstrap_pvalue_one_sample(x, mu0=0.0, nboot=20000, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) < 5:\n",
    "        return np.nan, (np.nan, np.nan)\n",
    "    obs = x.mean() - mu0\n",
    "    # center under H0\n",
    "    x0 = x - x.mean() + mu0\n",
    "    means = []\n",
    "    for _ in range(nboot):\n",
    "        xb = rng.choice(x0, size=len(x0), replace=True)\n",
    "        means.append(xb.mean() - mu0)\n",
    "    means = np.asarray(means)\n",
    "    p = (np.abs(means) >= np.abs(obs)).mean()\n",
    "    ci = (np.percentile(means, 2.5), np.percentile(means, 97.5))\n",
    "    return float(p), (float(ci[0]), float(ci[1]))\n",
    "\n",
    "# --- main loop ---\n",
    "rows = []\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "print(\"Odd/Even & Secondary tests (depths in ppm; p-values from bootstrap)\")\n",
    "for name, info in TARGETS.items():\n",
    "    tic, toi, secs = info[\"tic\"], info[\"toi\"], info[\"sectors\"]\n",
    "    try:\n",
    "        P, T0 = load_refined_ephemeris(tic)\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    duration_days = load_duration_days_or_default(tic)\n",
    "    halfw = 0.6 * duration_days  # a touch wider than the box-fit to be safe\n",
    "\n",
    "    lc = stitch_pdcsap(tic, secs)\n",
    "    if lc is None or len(lc.time.value) < 100:\n",
    "        print(f\"{name}: no usable PDCSAP.\")\n",
    "        continue\n",
    "\n",
    "    t = lc.time.value          # BTJD\n",
    "    f = lc.flux.value          # ~normalized to 1\n",
    "    # simple out-of-transit baseline via median outside ±3*duration\n",
    "    ph, k = phased_times(t, P, T0)\n",
    "    oot = np.abs(ph) > max(3*duration_days, 0.08)  # keep OOT away from transit\n",
    "    if np.sum(oot) > 20:\n",
    "        f = f / np.median(f[oot])\n",
    "\n",
    "    depth_ppm = (1.0 - f) * 1e6\n",
    "\n",
    "    # Primary in-transit mask\n",
    "    in_transit = np.abs(ph) <= halfw\n",
    "    # Split odd/even by epoch index k\n",
    "    odd_mask  = in_transit & (k % 2 != 0)\n",
    "    even_mask = in_transit & (k % 2 == 0)\n",
    "\n",
    "    # Secondary window near phase +0.5 (also check around -0.5 wrap)\n",
    "    sec = (np.abs(ph - 0.5) <= halfw) | (np.abs(ph + 0.5) <= halfw)\n",
    "\n",
    "    # Compute summary numbers\n",
    "    d_odd  = depth_ppm[odd_mask]\n",
    "    d_even = depth_ppm[even_mask]\n",
    "    d_sec  = depth_ppm[sec]\n",
    "\n",
    "    p_odd_even, ci_oe = bootstrap_pvalue_two_sample(d_odd, d_even)\n",
    "    p_secondary,  ci_sec = bootstrap_pvalue_one_sample(d_sec, mu0=0.0)\n",
    "\n",
    "    mean_odd  = float(np.nanmean(d_odd))  if d_odd.size  else np.nan\n",
    "    mean_even = float(np.nanmean(d_even)) if d_even.size else np.nan\n",
    "    mean_sec  = float(np.nanmean(d_sec))  if d_sec.size  else np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"target\": name, \"toi\": toi, \"tic\": tic,\n",
    "        \"P_days\": P, \"T0_BTJD\": T0,\n",
    "        \"duration_days_used\": duration_days,\n",
    "        \"N_in_odd\": int(d_odd.size), \"N_in_even\": int(d_even.size), \"N_in_sec\": int(d_sec.size),\n",
    "        \"mean_depth_odd_ppm\": mean_odd, \"mean_depth_even_ppm\": mean_even,\n",
    "        \"delta_odd_minus_even_ppm\": (mean_odd - mean_even) if np.isfinite(mean_odd) and np.isfinite(mean_even) else np.nan,\n",
    "        \"pvalue_odd_vs_even\": p_odd_even, \"ci_odd_even_ppm\": ci_oe,\n",
    "        \"mean_secondary_depth_ppm\": mean_sec, \"pvalue_secondary_vs_zero\": p_secondary, \"ci_secondary_ppm\": ci_sec\n",
    "    })\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"\\n{name} — {toi} (TIC {tic})\")\n",
    "    print(f\"  P={P:.8f} d, T0={T0:.6f} BTJD, duration_used≈{24*duration_days:.2f} h\")\n",
    "    print(f\"  In-transit points: odd={d_odd.size}, even={d_even.size}; secondary window={d_sec.size}\")\n",
    "    print(f\"  Odd mean depth ≈ {mean_odd:,.0f} ppm, Even ≈ {mean_even:,.0f} ppm \"\n",
    "          f\"→ Δ(odd-even) ≈ {(mean_odd-mean_even):+.0f} ppm;  p={p_odd_even:0.3f}\")\n",
    "    print(f\"  Secondary mean depth ≈ {mean_sec:,.0f} ppm vs 0;  p={p_secondary:0.3f}\")\n",
    "\n",
    "# Write CSV\n",
    "outcsv = \"results/odd_even_secondary_summary.csv\"\n",
    "with open(outcsv, \"w\", newline=\"\") as f:\n",
    "    cols = list(rows[0].keys())\n",
    "    w = csv.DictWriter(f, fieldnames=cols)\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "print(f\"\\nSaved {outcsv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0168c735-27d5-48c1-bf74-9976bda47870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "def load_ephem(tic):\n",
    "    p = f\"results/TIC{tic}_refined_ephemeris.json\"\n",
    "    if not os.path.exists(p):\n",
    "        p = f\"results/TIC{tic}_refined_ephemeris_robust.json\"\n",
    "    d = json.load(open(p))\n",
    "    return float(d[\"P\"]), float(d[\"T0\"]), np.array(d[\"cov\"], float)\n",
    "\n",
    "def in_transit_mask(t, P, T0, dur_hours, phase_center=0.0):\n",
    "    # returns boolean mask +- (dur/2) around phase_center\n",
    "    phase = ((t - T0) / P) % 1.0\n",
    "    dur_days = dur_hours/24.0\n",
    "    half = 0.5*dur_days/P\n",
    "    # wrap-safe distance to phase_center and (phase_center+1)\n",
    "    dphi = np.minimum(np.abs(phase - phase_center), np.abs(phase - phase_center - 1))\n",
    "    return dphi < half\n",
    "\n",
    "def odd_even_masks(t, P, T0, dur_hours):\n",
    "    k = np.round((t - T0)/P).astype(int)  # nearest epoch index\n",
    "    it = in_transit_mask(t, P, T0, dur_hours, 0.0)\n",
    "    odd = it & (k % 2 != 0)\n",
    "    even = it & (k % 2 == 0)\n",
    "    return odd, even\n",
    "\n",
    "def sigma_minutes_at(btjd, P, T0, cov):\n",
    "    # 1σ timing half-width in minutes at an absolute BTJD time\n",
    "    E = (btjd - T0) / P\n",
    "    c = np.asarray(cov, float)\n",
    "    sig_days = np.sqrt(max(c[0,0] + 2*E*c[0,1] + (E**2)*c[1,1], 0.0))\n",
    "    return float(sig_days * 24 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a4e4e14-fea7-4a5b-a52d-be4b1a370bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick stitched PDCSAP helper (drop-in) ---\n",
    "import os, numpy as np\n",
    "import lightkurve as lk\n",
    "\n",
    "QUALITY_MASK = 175  # same as before\n",
    "\n",
    "SECTORS = {\n",
    "    119584412: [22, 49],      # Target A\n",
    "    37749396:  [3, 42, 70],   # Target B\n",
    "    311183180: [5, 31],       # Target C\n",
    "}\n",
    "\n",
    "def get_stitched_pdcsap(tic, sectors=None, save_npz=True):\n",
    "    \"\"\"Return BTJD time and normalized PDCSAP flux for the given TIC.\"\"\"\n",
    "    if sectors is None:\n",
    "        sectors = SECTORS.get(int(tic), None)\n",
    "\n",
    "    lcs = []\n",
    "    if sectors:\n",
    "        # deterministic order\n",
    "        for s in sectors:\n",
    "            sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\", sector=s)\n",
    "            if len(sr) == 0:\n",
    "                continue\n",
    "            lcf = sr.download()  # deprecation warning is OK in this env\n",
    "            lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "            lc = lc.remove_outliers(sigma=10)  # gentle; keeps transits\n",
    "            mask = lk.utils.TessQualityFlags.create_quality_mask(lc.quality, bitmask=QUALITY_MASK)\n",
    "            lc = lc[mask]\n",
    "            lcs.append(lc)\n",
    "    else:\n",
    "        # fallback: grab everything available (slower, but robust)\n",
    "        sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\")\n",
    "        for r in sr:\n",
    "            lcf = r.download()\n",
    "            lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "            lc = lc.remove_outliers(sigma=10)\n",
    "            mask = lk.utils.TessQualityFlags.create_quality_mask(lc.quality, bitmask=QUALITY_MASK)\n",
    "            lc = lc[mask]\n",
    "            lcs.append(lc)\n",
    "\n",
    "    if not lcs:\n",
    "        raise RuntimeError(f\"No PDCSAP data found for TIC {tic} with sectors={sectors}\")\n",
    "\n",
    "    stitched = lk.LightCurveCollection(lcs).stitch().remove_nans()\n",
    "    t = np.asarray(stitched.time.value, dtype=float)   # BTJD\n",
    "    f = np.asarray(stitched.flux.value, dtype=float)   # normalized\n",
    "\n",
    "    if save_npz:\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        np.savez_compressed(f\"results/TIC{int(tic)}_stitched.npz\", t=t, f=f)\n",
    "\n",
    "    return t, f\n",
    "\n",
    "# If you keep the original loader name elsewhere, you can shim it:\n",
    "def load_stitched_npz(tic):\n",
    "    path = f\"results/TIC{int(tic)}_stitched.npz\"\n",
    "    if os.path.exists(path):\n",
    "        d = np.load(path)\n",
    "        return d[\"t\"].astype(float), d[\"f\"].astype(float)\n",
    "    # otherwise build it now:\n",
    "    return get_stitched_pdcsap(tic, save_npz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014ebe4c-ad63-4be4-86ed-b0b51ad68ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A — TOI 1801.01 (TIC 119584412)\n",
      "  odd N=392, even N=392, sec N=179\n",
      "  Welch t-test odd vs even:   t=-7.573, p= 1.04e-13\n",
      "  t-test secondary vs zero:   t= 4.554, p= 9.72e-06\n",
      "\n",
      "B — TOI 260.01 (TIC 37749396)\n",
      "  odd N=266, even N=266, sec N=265\n",
      "  Welch t-test odd vs even:   t=-10.386, p= 4.45e-23\n",
      "  t-test secondary vs zero:   t=-9.437, p= 2.1e-18\n",
      "\n",
      "C — TOI 550.02 (TIC 311183180)\n",
      "  odd N=342, even N=341, sec N=939\n",
      "  Welch t-test odd vs even:   t= 0.310, p= 0.757\n",
      "  t-test secondary vs zero:   t= 8.337, p= 2.7e-16\n"
     ]
    }
   ],
   "source": [
    "# You already have a stitched, detrended, normalized time series per target.\n",
    "# Reuse whatever arrays you built earlier: t (BTJD), f (normalized flux ~1).\n",
    "# If you don't have them in memory, load them the same way you did for Fig A panels.\n",
    "\n",
    "TARGETS_SIMPLE = {\n",
    "    \"A\": {\"tic\":119584412, \"toi\":\"TOI 1801.01\", \"dur_h\":6.53},\n",
    "    \"B\": {\"tic\":37749396,  \"toi\":\"TOI 260.01\",  \"dur_h\":4.42},\n",
    "    \"C\": {\"tic\":311183180, \"toi\":\"TOI 550.02\",  \"dur_h\":5.72},\n",
    "}\n",
    "\n",
    "def load_stitched_npz(tic):\n",
    "    # If you saved an npz; otherwise replace with your existing loader.\n",
    "    # Expect keys: 't','f'. If not using npz, bring t,f from your earlier cell.\n",
    "    path = f\"results/TIC{tic}_stitched_flat.npz\"\n",
    "    if os.path.exists(path):\n",
    "        d = np.load(path)\n",
    "        return d[\"t\"].astype(float), d[\"f\"].astype(float)\n",
    "    raise FileNotFoundError(\"Provide t,f from your stitched light curve here.\")\n",
    "\n",
    "for name, info in TARGETS_SIMPLE.items():\n",
    "    tic, toi, dur_h = info[\"tic\"], info[\"toi\"], info[\"dur_h\"]\n",
    "    P,T0,C = load_ephem(tic)\n",
    "\n",
    "    # === Replace this with your already-loaded arrays ===\n",
    "    # t, f = <your stitched BTJD, normalized flux>\n",
    "    # If you don't have an npz, comment the next line and set t,f from memory:\n",
    "    # For Target A only:\n",
    "    t, f = get_stitched_pdcsap(119584412, sectors=[22, 49], save_npz=True)\n",
    "\n",
    "    # Masks\n",
    "    odd, even = odd_even_masks(t, P, T0, dur_h)\n",
    "    sec = in_transit_mask(t, P, T0, dur_h, phase_center=0.5)\n",
    "\n",
    "    # Welch tests (two-sided)\n",
    "    to = stats.ttest_ind(f[odd],  f[even], equal_var=False, nan_policy=\"omit\")\n",
    "    ts = stats.ttest_1samp(f[sec]-1.0, popmean=0.0, nan_policy=\"omit\")  # mean offset vs 0\n",
    "\n",
    "    print(f\"\\n{name} — {toi} (TIC {tic})\")\n",
    "    print(f\"  odd N={odd.sum()}, even N={even.sum()}, sec N={sec.sum()}\")\n",
    "    print(f\"  Welch t-test odd vs even:   t={to.statistic: .3f}, p={to.pvalue: .3g}\")\n",
    "    print(f\"  t-test secondary vs zero:   t={ts.statistic: .3f}, p={ts.pvalue: .3g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5111aa98-a8e4-44e5-9c6b-42a73e1d0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick helper: get stitched PDCSAP arrays (gentle, reproducible) ---\n",
    "import numpy as np, os\n",
    "import lightkurve as lk\n",
    "\n",
    "def get_stitched_pdcsap(tic, sectors=None, qmask=175, window_days=1.0, polyorder=2,\n",
    "                        save_npz=False):\n",
    "    \"\"\"Return (t, f) for a TIC from SPOC PDCSAP, gently detrended & stitched.\"\"\"\n",
    "    lcs = []\n",
    "    if sectors is None:\n",
    "        sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\", author=\"SPOC\")\n",
    "        lcfs = sr.download_all()\n",
    "    else:\n",
    "        lcfs = []\n",
    "        for s in sectors:\n",
    "            sr = lk.search_lightcurvefile(f\"TIC {int(tic)}\", mission=\"TESS\",\n",
    "                                          author=\"SPOC\", sector=s)\n",
    "            lcf = sr.download()\n",
    "            lcfs.append(lcf)\n",
    "\n",
    "    for lcf in lcfs:\n",
    "        lc = lcf.PDCSAP_FLUX.remove_nans().normalize()\n",
    "        # approximate cadence in days\n",
    "        dt = np.nanmedian(np.diff(lc.time.value))\n",
    "        win = max(7, int(window_days / dt))  # window length in cadences\n",
    "        lc = lc.remove_outliers(sigma=5)\n",
    "        lc = lc.flatten(window_length=win, polyorder=polyorder)\n",
    "        lcs.append(lc)\n",
    "\n",
    "    stitched = lk.LightCurveCollection(lcs).stitch().remove_nans().normalize()\n",
    "    t = stitched.time.value.astype(float)  # BTJD\n",
    "    f = stitched.flux.value.astype(float)  # normalized\n",
    "\n",
    "    if save_npz:\n",
    "        os.makedirs(\"results_npz\", exist_ok=True)\n",
    "        np.savez(f\"results_npz/TIC{int(tic)}_stitched.npz\", t=t, f=f)\n",
    "\n",
    "    return t, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70a3feb4-9c43-4678-a5ec-7e788668c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A — TOI 1801.01 (TIC 311183180) robust depths [ppm]:\n",
      "  odd:  median=1000018, trimmed=1000011, mean_CI=(   -66,     9)\n",
      "  even: median=1000001, trimmed=999987, mean_CI=(    -5,    70)\n",
      "  sec:  median=    ———,  trimmed=    ———,  mean_CI=(   nan,   nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B — TOI 260.01 (TIC 311183180) robust depths [ppm]:\n",
      "  odd:  median=1000022, trimmed=1000038, mean_CI=(   -79,    -0)\n",
      "  even: median=1000124, trimmed=1000110, mean_CI=(  -162,   -73)\n",
      "  sec:  median=999981,  trimmed=999978,  mean_CI=(   276,   435)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C — TOI 550.02 (TIC 311183180) robust depths [ppm]:\n",
      "  odd:  median=999880, trimmed=999878, mean_CI=(    62,   144)\n",
      "  even: median=999849, trimmed=999784, mean_CI=(   873,  1107)\n",
      "  sec:  median=1000048,  trimmed=1000028,  mean_CI=(   -49,    -7)\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def robust_depth(samples, trim=0.2):\n",
    "    # depth = 1 - statistic (so positive = dimmer in-transit)\n",
    "    med = 1 - np.nanmedian(samples)\n",
    "    n = samples.size\n",
    "    k = int(np.floor(trim*n))\n",
    "    if 2*k < n:\n",
    "        sm = np.sort(samples[~np.isnan(samples)])\n",
    "        trimmed = sm[k: n-k] if n-2*k>0 else sm\n",
    "        tmean = 1 - np.mean(trimmed)\n",
    "    else:\n",
    "        tmean = med\n",
    "    return med, tmean\n",
    "\n",
    "def bootstrap_ci(samples, B=2000, statfunc=np.nanmean):\n",
    "    n = samples.size\n",
    "    boots = []\n",
    "    for _ in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        boots.append(statfunc(samples[idx]))\n",
    "    lo, hi = np.percentile(boots, [16,84])\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "for name, info in TARGETS_SIMPLE.items():\n",
    "    tic, toi, dur_h = info[\"tic\"], info[\"toi\"], info[\"dur_h\"]\n",
    "    P,T0,C = load_ephem(tic)\n",
    "    # t, f = ... (bring from memory or loader)\n",
    "    # A\n",
    "    tic = 119584412\n",
    "    t, f = get_stitched_pdcsap(tic, sectors=[22, 49], window_days=1.0)\n",
    "    \n",
    "    # B\n",
    "    tic = 37749396\n",
    "    t, f = get_stitched_pdcsap(tic, sectors=[3, 42, 70], window_days=0.75)\n",
    "    \n",
    "    # C\n",
    "    tic = 311183180\n",
    "    t, f = get_stitched_pdcsap(tic, sectors=[5, 31], window_days=0.75)\n",
    "\n",
    "    odd, even = odd_even_masks(t, P, T0, dur_h)\n",
    "    sec = in_transit_mask(t, P, T0, dur_h, 0.5)\n",
    "    oot = ~(odd|even|sec)\n",
    "\n",
    "    d_odd_med,  d_odd_trim  = robust_depth(1-f[odd])\n",
    "    d_even_med, d_even_trim = robust_depth(1-f[even])\n",
    "    d_sec_med,  d_sec_trim  = robust_depth(1-f[sec])\n",
    "\n",
    "    # CIs on mean depths in ppm\n",
    "    lo_o, hi_o = bootstrap_ci(1-f[odd],  statfunc=np.nanmean)\n",
    "    lo_e, hi_e = bootstrap_ci(1-f[even], statfunc=np.nanmean)\n",
    "    lo_s, hi_s = bootstrap_ci(1-f[sec],  statfunc=np.nanmean)\n",
    "\n",
    "    print(f\"\\n{name} — {toi} (TIC {tic}) robust depths [ppm]:\")\n",
    "    print(f\"  odd:  median={1e6*d_odd_med:6.0f}, trimmed={1e6*d_odd_trim:6.0f}, mean_CI=({1e6*lo_o:6.0f},{1e6*hi_o:6.0f})\")\n",
    "    print(f\"  even: median={1e6*d_even_med:6.0f}, trimmed={1e6*d_even_trim:6.0f}, mean_CI=({1e6*lo_e:6.0f},{1e6*hi_e:6.0f})\")\n",
    "    print(f\"  sec:  median={1e6*d_sec_med:6.0f},  trimmed={1e6*d_sec_trim:6.0f},  mean_CI=({1e6*lo_s:6.0f},{1e6*hi_s:6.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b68e394-fc09-462f-b619-d5edecee8b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved figures/TIC311183180_phase_odd_even.png and figures/TIC311183180_phase_secondary.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved figures/TIC311183180_phase_odd_even.png and figures/TIC311183180_phase_secondary.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved figures/TIC311183180_phase_odd_even.png and figures/TIC311183180_phase_secondary.png\n"
     ]
    }
   ],
   "source": [
    "def quick_phase_plot(t, f, P, T0, masks, labels, outpng):\n",
    "    phase = ((t - T0)/P) % 1.0\n",
    "    fig, ax = plt.subplots(figsize=(7.5,4.0), dpi=140)\n",
    "    for m,lab in zip(masks, labels):\n",
    "        ax.scatter(phase[m], f[m], s=2, alpha=0.25, label=lab)\n",
    "    ax.set_xlabel(\"Phase\"); ax.set_ylabel(\"Flux (norm)\")\n",
    "    ax.set_title(os.path.basename(outpng).replace(\"_\",\" \"))\n",
    "    ax.legend(markerscale=4, frameon=False)\n",
    "    fig.tight_layout(); fig.savefig(outpng); plt.close(fig)\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "for name, info in TARGETS_SIMPLE.items():\n",
    "    tic, toi, dur_h = info[\"tic\"], info[\"toi\"], info[\"dur_h\"]\n",
    "    P,T0,C = load_ephem(tic)\n",
    "    # t, f = ...\n",
    "    # A\n",
    "    tic = 119584412\n",
    "    t, f = get_stitched_pdcsap(tic, sectors=[22, 49], window_days=1.0)\n",
    "    \n",
    "    # B\n",
    "    tic = 37749396\n",
    "    t, f = get_stitched_pdcsap(tic, sectors=[3, 42, 70], window_days=0.75)\n",
    "    \n",
    "    # C\n",
    "    tic = 311183180\n",
    "    t, f = get_stitched_pdcsap(tic, sectors=[5, 31], window_days=0.75)\n",
    "    odd, even = odd_even_masks(t, P, T0, dur_h)\n",
    "    sec = in_transit_mask(t, P, T0, dur_h, 0.5)\n",
    "    out1 = f\"figures/TIC{tic}_phase_odd_even.png\"\n",
    "    out2 = f\"figures/TIC{tic}_phase_secondary.png\"\n",
    "    quick_phase_plot(t, f, P, T0, [odd,even], [\"odd\",\"even\"], out1)\n",
    "    quick_phase_plot(t, f, P, T0, [sec], [\"secondary\"], out2)\n",
    "    print(\"saved\", out1, \"and\", out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "270b601b-50b5-4b64-958c-b14e435d68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def sector_depths_table(t, f, sectors, P, T0, dur_h, outcsv, outpng):\n",
    "    # sectors: array same length as t with sector numbers\n",
    "    uniq = np.unique(sectors)\n",
    "    rows = []\n",
    "    for s in uniq:\n",
    "        m = sectors == s\n",
    "        odd, even = odd_even_masks(t[m], P, T0, dur_h)\n",
    "        it = odd | even\n",
    "        if it.sum() < 20:\n",
    "            rows.append((s, np.nan, np.nan, it.sum()))\n",
    "            continue\n",
    "        d = 1 - np.nanmean(f[m][it])\n",
    "        rows.append((s, d*1e6, np.nanstd(f[m][it])*1e6/np.sqrt(max(it.sum(),1)), it.sum()))\n",
    "    rows.sort()\n",
    "    with open(outcsv,\"w\",newline=\"\") as g:\n",
    "        w=csv.writer(g); w.writerow([\"sector\",\"depth_ppm\",\"sem_ppm\",\"N_in_transit\"]); w.writerows(rows)\n",
    "    # quick plot\n",
    "    S = [r[0] for r in rows]; D = [r[1] for r in rows]; E=[r[2] for r in rows]\n",
    "    fig,ax=plt.subplots(figsize=(7,3.5),dpi=140)\n",
    "    ax.errorbar(S,D,yerr=E,fmt='o-',capsize=2)\n",
    "    ax.set_xlabel(\"Sector\"); ax.set_ylabel(\"Depth (ppm)\"); ax.set_title(os.path.basename(outpng))\n",
    "    fig.tight_layout(); fig.savefig(outpng); plt.close(fig)\n",
    "\n",
    "# If you already track sector per-point, pass it here; otherwise skip this block.\n",
    "# Example usage (uncomment when you have 'sectors' array aligned with t,f):\n",
    "# for name, info in TARGETS_SIMPLE.items():\n",
    "#     tic, toi, dur_h = info[\"tic\"], info[\"toi\"], info[\"dur_h\"]\n",
    "#     P,T0,C = load_ephem(tic)\n",
    "#     t, f, sectors = ...  # provide your sector array here\n",
    "#     sector_depths_table(t,f,sectors,P,T0,dur_h,\n",
    "#                         f\"results/TIC{tic}_per_sector_depths.csv\",\n",
    "#                         f\"figures/TIC{tic}_per_sector_depths.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f9d9ac1-9f9a-4aca-aefc-382bbd6e9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Minimal PDCSAP stitcher for saving NPZs (drop this above the NPZ cell) ---\n",
    "import numpy as np, warnings\n",
    "from lightkurve import search_lightcurvefile, LightCurveCollection\n",
    "\n",
    "def load_stitched_lightcurve_pdcsap(tic, quality_bitmask=175):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "        t : BTJD (float64)\n",
    "        f : normalized PDCSAP flux (float64)\n",
    "    Notes\n",
    "        - PDCSAP first, all available SPOC sectors.\n",
    "        - Applies quality bitmask (default 175).\n",
    "        - Gentle clean: remove NaNs, 5-sigma outlier clip per sector, normalize, then stitch.\n",
    "    \"\"\"\n",
    "    # download_all() is robust and works across sectors; suppress noisy warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        lcfs = search_lightcurvefile(f\"TIC {tic}\", author=\"SPOC\").download_all()\n",
    "\n",
    "    if lcfs is None or len(lcfs) == 0:\n",
    "        raise FileNotFoundError(f\"No SPOC LightCurveFiles found for TIC {tic}\")\n",
    "\n",
    "    per_sector = []\n",
    "    for lcf in lcfs:\n",
    "        # Apply quality mask on the underlying light curve file\n",
    "        lc = lcf.PDCSAP_FLUX\n",
    "        if quality_bitmask is not None and \"quality\" in lc.columns:\n",
    "            q = (lcf.quality & quality_bitmask) == 0\n",
    "            # Some Lightkurve versions require slicing via .copy()\n",
    "            lc = lc[q].copy()\n",
    "        # Clean + normalize per sector\n",
    "        lc = lc.remove_nans().remove_outliers(sigma=5)\n",
    "        lc = lc.normalize()\n",
    "        if len(lc.time.value) > 0:\n",
    "            per_sector.append(lc)\n",
    "\n",
    "    if len(per_sector) == 0:\n",
    "        raise RuntimeError(f\"All sectors for TIC {tic} were empty after masking/cleaning.\")\n",
    "\n",
    "    stitched = LightCurveCollection(per_sector).stitch()\n",
    "    return stitched.time.value.astype(float), stitched.flux.value.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec0a4933-ab4c-43b8-bfdc-931eedc0bd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=175).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[make_npz] wrote results/TIC37749396_stitched.npz\n",
      "[make_npz] wrote results/TIC311183180_stitched.npz\n"
     ]
    }
   ],
   "source": [
    "# ---- RUN ONCE to create stitched arrays for A & B if missing ----\n",
    "import os, numpy as np, json\n",
    "\n",
    "TARGETS = {\n",
    "    \"Target A — TIC 119584412\": {\"tic\": 119584412, \"toi\": \"TOI 1801.01\"},\n",
    "    \"Target B — TIC 37749396\":  {\"tic\": 37749396,  \"toi\": \"TOI 260.01\"},\n",
    "    \"Target C — TIC 311183180\": {\"tic\": 311183180, \"toi\": \"TOI 550.02\"},\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "def have_npz(tic): return os.path.exists(f\"results/TIC{tic}_stitched.npz\")\n",
    "\n",
    "def save_npz_if_missing(tic):\n",
    "    if have_npz(tic): return\n",
    "    # replace this loader with your PDCSAP stitcher used elsewhere\n",
    "    t, f = load_stitched_lightcurve_pdcsap(tic)  # <-- your existing function\n",
    "    np.savez(f\"results/TIC{tic}_stitched.npz\", t=np.asarray(t, float), f=np.asarray(f, float))\n",
    "    print(f\"[make_npz] wrote results/TIC{tic}_stitched.npz\")\n",
    "\n",
    "for info in TARGETS.values():\n",
    "    try:\n",
    "        save_npz_if_missing(info[\"tic\"])\n",
    "    except NameError:\n",
    "        print(\"Define load_stitched_lightcurve_pdcsap(tic) or swap in your own t,f here.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1768ae0-f809-4686-a9f0-6bdc78f6a3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOI 1801.01 — TIC 119584412\n",
      "  odd N=938, even N=939, sec N=317\n",
      "  Welch t-test odd vs even:   t= -9.715, p=8.46e-22\n",
      "  one-sample t-test (sec vs 0 depth): t=  6.373, p=6.55e-10\n",
      "  odd depth ppm:  median=1324, trimmed=1320, CI=(1273.518101252774, 1365.6629705207145)\n",
      "  even depth ppm: median=855, trimmed=849, CI=(802.0458369457706, 892.4205543827092)\n",
      "  sec depth ppm:  median=-375, trimmed=-445, CI=(-546.6203596077713, -345.15029075098977)\n",
      "\n",
      "TOI 260.01 — TIC 37749396\n",
      "  odd N=2536, even N=2839, sec N=3179\n",
      "  Welch t-test odd vs even:   t=  2.736, p=6.24e-03\n",
      "  one-sample t-test (sec vs 0 depth): t= -3.143, p=1.69e-03\n",
      "  odd depth ppm:  median=96, trimmed=71, CI=(44.34923673498219, 97.56858052291307)\n",
      "  even depth ppm: median=103, trimmed=127, CI=(104.19215825072901, 150.1279108282023)\n",
      "  sec depth ppm:  median=65, trimmed=56, CI=(36.06788770154792, 74.81552762929017)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Odd/Even + Secondary significance & plots for A and B ===\n",
    "import os, json, numpy as np\n",
    "from astropy.time import Time\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BTJD_ZERO = 2457000.0\n",
    "\n",
    "TARGETS = {\n",
    "    \"Target A — TIC 119584412\": {\"tic\": 119584412, \"toi\": \"TOI 1801.01\", \"dur_h\": 6.53},\n",
    "    \"Target B — TIC 37749396\":  {\"tic\": 37749396,  \"toi\": \"TOI 260.01\", \"dur_h\": 4.42},\n",
    "}\n",
    "\n",
    "def load_ephem(tic):\n",
    "    # prefer robust if you’ve got both\n",
    "    for name in [f\"results/TIC{tic}_refined_ephemeris_robust.json\",\n",
    "                 f\"results/TIC{tic}_refined_ephemeris.json\"]:\n",
    "        if os.path.exists(name):\n",
    "            d = json.load(open(name))\n",
    "            return float(d[\"P\"]), float(d[\"T0\"]), np.array(d[\"cov\"], float)\n",
    "    raise FileNotFoundError(f\"No ephemeris JSON for TIC {tic}\")\n",
    "\n",
    "def load_stitched_npz(tic):\n",
    "    path = f\"results/TIC{tic}_stitched.npz\"\n",
    "    d = np.load(path)\n",
    "    return d[\"t\"].astype(float), d[\"f\"].astype(float)\n",
    "\n",
    "def phase_fold(t, P, T0):  # [0,1)\n",
    "    return np.mod((t - T0)/P, 1.0)\n",
    "\n",
    "def in_window(ph, center, half_width):\n",
    "    # half_width in phase units\n",
    "    d = np.abs((ph - center + 0.5) % 1.0 - 0.5)\n",
    "    return d <= half_width\n",
    "\n",
    "def odd_even_masks(t, P, T0, dur_h):\n",
    "    ph = phase_fold(t, P, T0)\n",
    "    # convert duration to phase half-width; scale a bit wider (×1.2) to be safe\n",
    "    halfw = (dur_h/24.0)/P * 1.2\n",
    "    base = in_window(ph, 0.0, halfw)\n",
    "    # assign epochs: nearest integer k to phase offset\n",
    "    k = np.floor((t - T0)/P + 0.5).astype(int)\n",
    "    return base & (k % 2 == 1), base & (k % 2 == 0)\n",
    "\n",
    "def secondary_mask(t, P, T0, dur_h):\n",
    "    ph = phase_fold(t, P, T0)\n",
    "    halfw = (dur_h/24.0)/P * 1.2\n",
    "    return in_window(ph, 0.5, halfw)\n",
    "\n",
    "def trimmed_mean_ci(x, trim=0.1, alpha=0.05):\n",
    "    x = np.asarray(x, float)\n",
    "    n = x.size\n",
    "    if n < 8:  # too few points\n",
    "        return np.nan, np.nan, (np.nan, np.nan)\n",
    "    # trim both tails\n",
    "    k = int(np.floor(trim*n))\n",
    "    xs = np.sort(x)[k:n-k] if 2*k < n else np.sort(x)\n",
    "    m = xs.mean()\n",
    "    # bootstrap CI\n",
    "    rng = np.random.default_rng(42)\n",
    "    B = 2000\n",
    "    means = np.empty(B)\n",
    "    for i in range(B):\n",
    "        means[i] = xs[rng.integers(0, xs.size, xs.size)].mean()\n",
    "    lo, hi = np.percentile(means, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return np.median(x), m, (lo, hi)\n",
    "\n",
    "def welch_t_p(x, y):\n",
    "    t, p = stats.ttest_ind(x, y, equal_var=False, nan_policy=\"omit\")\n",
    "    return float(t), float(p)\n",
    "\n",
    "def one_sample_t_p(x, mu=1.0):\n",
    "    t, p = stats.ttest_1samp(x, popmean=mu, nan_policy=\"omit\")\n",
    "    return float(t), float(p)\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "for label, info in TARGETS.items():\n",
    "    tic, toi, dur_h = info[\"tic\"], info[\"toi\"], info[\"dur_h\"]\n",
    "    P, T0, C = load_ephem(tic)\n",
    "    t, f = load_stitched_npz(tic)\n",
    "\n",
    "    odd_m, even_m = odd_even_masks(t, P, T0, dur_h)\n",
    "    sec_m = secondary_mask(t, P, T0, dur_h)\n",
    "\n",
    "    f_odd, f_even, f_sec = f[odd_m], f[even_m], f[sec_m]\n",
    "\n",
    "    # Welch t-tests (flux normalized ~1; “depth” = 1 - mean flux)\n",
    "    t_oe, p_oe = welch_t_p(f_odd, f_even)\n",
    "    t_sec, p_sec = one_sample_t_p(f_sec, mu=1.0)\n",
    "\n",
    "    # Robust summaries in ppm (1e6*(1 - value))\n",
    "    def depths_ppm(arr):\n",
    "        med, trim_mean, (lo, hi) = trimmed_mean_ci(1e6*(1.0 - arr))\n",
    "        return med, trim_mean, (lo, hi)\n",
    "\n",
    "    d_odd = depths_ppm(f_odd)\n",
    "    d_even = depths_ppm(f_even)\n",
    "    d_sec = depths_ppm(f_sec)\n",
    "\n",
    "    print(f\"{toi} — {label.split('—')[-1].strip()}\")\n",
    "    print(f\"  odd N={f_odd.size}, even N={f_even.size}, sec N={f_sec.size}\")\n",
    "    print(f\"  Welch t-test odd vs even:   t={t_oe:7.3f}, p={p_oe:.2e}\")\n",
    "    print(f\"  one-sample t-test (sec vs 0 depth): t={t_sec:7.3f}, p={p_sec:.2e}\")\n",
    "    print(f\"  odd depth ppm:  median={d_odd[0]:.0f}, trimmed={d_odd[1]:.0f}, CI={d_odd[2]}\")\n",
    "    print(f\"  even depth ppm: median={d_even[0]:.0f}, trimmed={d_even[1]:.0f}, CI={d_even[2]}\")\n",
    "    print(f\"  sec depth ppm:  median={d_sec[0]:.0f}, trimmed={d_sec[1]:.0f}, CI={d_sec[2]}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Phase plots\n",
    "    ph = phase_fold(t, P, T0)\n",
    "    # Odd/even scatter near primary\n",
    "    plt.figure(figsize=(9,4.6), dpi=120)\n",
    "    h1 = plt.scatter(ph[odd_m],  f[odd_m],  s=8, alpha=0.35, label=\"odd\")\n",
    "    h2 = plt.scatter(ph[even_m], f[even_m], s=8, alpha=0.35, label=\"even\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Phase\"); plt.ylabel(\"Flux (norm)\")\n",
    "    plt.title(f\"TIC{tic} phase odd/even\")\n",
    "    plt.xlim(-0.02, 0.02); plt.ylim(0.985, 1.005)\n",
    "    plt.savefig(f\"figures/TIC{tic}_phase_odd_even.png\"); plt.close()\n",
    "\n",
    "    # Secondary scatter around 0.5\n",
    "    plt.figure(figsize=(9,4.6), dpi=120)\n",
    "    plt.scatter(ph[sec_m], f[sec_m], s=8, alpha=0.35, label=\"secondary\")\n",
    "    plt.legend(); plt.xlabel(\"Phase\"); plt.ylabel(\"Flux (norm)\")\n",
    "    plt.title(f\"TIC{tic} phase secondary\")\n",
    "    plt.xlim(0.49, 0.51); plt.ylim(0.998, 1.004)\n",
    "    plt.savefig(f\"figures/TIC{tic}_phase_secondary.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfb40dd4-0335-426e-9af3-ab9d11f15dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures/TIC119584412_phase_odd_even.png and figures/TIC119584412_phase_secondary.png\n"
     ]
    }
   ],
   "source": [
    "# === PHASE PLOTS with BINNED OVERLAY (drop this in as one cell) ===\n",
    "import os, json, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "BTJD_ZERO = 2457000.0  # (not used here, just for consistency)\n",
    "\n",
    "# --------- pick target (edit these 3 lines per run) ----------\n",
    "tic, toi = 119584412, \"TOI 1801.01\"     # A  -> TIC 119584412\n",
    "# tic, toi = 37749396,  \"TOI 260.01\"    # B  -> TIC 37749396\n",
    "# tic, toi = 311183180, \"TOI 550.02\"    # C  -> TIC 311183180\n",
    "dur_hours = 6.0  # (A≈6–7 h, B≈4–5 h, C≈5–6 h). OK if approximate.\n",
    "\n",
    "# --------- helpers (no other cells required) ----------\n",
    "def load_ephem(tic):\n",
    "    for suffix in (\"\", \"_robust\"):\n",
    "        p = f\"results/TIC{tic}_refined_ephemeris{suffix}.json\"\n",
    "        if os.path.exists(p):\n",
    "            d = json.load(open(p))\n",
    "            return float(d[\"P\"]), float(d[\"T0\"])\n",
    "    raise FileNotFoundError(f\"refined ephemeris JSON for TIC{tic} not found\")\n",
    "\n",
    "def load_stitched_npz(tic):\n",
    "    p = f\"results/TIC{tic}_stitched.npz\"\n",
    "    d = np.load(p)\n",
    "    return d[\"t\"].astype(float), d[\"f\"].astype(float)\n",
    "\n",
    "def wrap_phase(t, P, T0):\n",
    "    # phase in [-0.5, 0.5)\n",
    "    ph = ((t - T0) / P) % 1.0\n",
    "    ph[ph >= 0.5] -= 1.0\n",
    "    return ph\n",
    "\n",
    "def in_window(t, P, T0, halfdur_days, center_phase=0.0):\n",
    "    # distance in phase from chosen center (0 for primary, 0.5 for secondary)\n",
    "    # work in [-0.5,0.5)\n",
    "    ph = wrap_phase(t, P, T0)\n",
    "    if center_phase == 0.5:\n",
    "        d = np.abs((ph - 0.5 + 0.5) % 1.0 - 0.5)\n",
    "    else:\n",
    "        d = np.abs(ph)\n",
    "    return d < (halfdur_days / P)\n",
    "\n",
    "def odd_even_masks(t, P, T0, halfdur_days):\n",
    "    # integer epoch index closest to each point\n",
    "    k = np.rint((t - T0) / P).astype(int)\n",
    "    w = in_window(t, P, T0, halfdur_days, center_phase=0.0)\n",
    "    odd  = w & (k % 2 != 0)\n",
    "    even = w & (k % 2 == 0)\n",
    "    return odd, even\n",
    "\n",
    "def bin_xy(x, y, nbins=80):\n",
    "    # mean & SEM per bin\n",
    "    edges = np.linspace(x.min(), x.max(), nbins+1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    xm = 0.5*(edges[:-1] + edges[1:])\n",
    "    ymean, ysem = np.full(nbins, np.nan), np.full(nbins, np.nan)\n",
    "    for i in range(nbins):\n",
    "        m = idx == i\n",
    "        if np.any(m):\n",
    "            yy = y[m]\n",
    "            ymean[i] = np.nanmean(yy)\n",
    "            ysem[i]  = np.nanstd(yy) / np.sqrt(np.sum(np.isfinite(yy)))\n",
    "    return xm, ymean, ysem\n",
    "\n",
    "# --------- load data & make plots ----------\n",
    "P, T0 = load_ephem(tic)\n",
    "t, f  = load_stitched_npz(tic)\n",
    "\n",
    "dur_days = dur_hours / 24.0\n",
    "halfdur  = 0.5 * dur_days\n",
    "\n",
    "ph = wrap_phase(t, P, T0)\n",
    "\n",
    "odd_m, even_m = odd_even_masks(t, P, T0, halfdur)\n",
    "sec_m = in_window(t, P, T0, halfdur, center_phase=0.5)\n",
    "\n",
    "# sensible y-lims around the in-window scatter\n",
    "y0 = np.nanmedian(f[odd_m | even_m])\n",
    "ylim = (y0 - 0.004, y0 + 0.004)  # ~±0.4% window; tweak if needed\n",
    "\n",
    "# ---- 1) Odd/Even primary window plot with binned overlay\n",
    "plt.figure(figsize=(10,5.2), dpi=120)\n",
    "plt.scatter(ph[odd_m],  f[odd_m],  s=9, alpha=0.25, label=\"odd\")\n",
    "plt.scatter(ph[even_m], f[even_m], s=9, alpha=0.25, label=\"even\")\n",
    "# binned means + 1σ(SEM) ribbons\n",
    "for lbl, m, c in [(\"odd\", odd_m, \"#1f77b4\"), (\"even\", even_m, \"#ff7f0e\")]:\n",
    "    xb, ym, ys = bin_xy(ph[m], f[m], nbins=60)\n",
    "    ok = np.isfinite(ym)\n",
    "    plt.plot(xb[ok], ym[ok], lw=2, color=c, label=f\"{lbl} (binned)\")\n",
    "    if np.any(ok & np.isfinite(ys)):\n",
    "        plt.fill_between(xb[ok], ym[ok]-ys[ok], ym[ok]+ys[ok], alpha=0.15, color=c)\n",
    "plt.xlim(-0.02, 0.02)\n",
    "plt.ylim(*ylim)\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Flux (norm)\")\n",
    "plt.title(f\"TIC{tic} phase odd/even — {toi}\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "out1 = f\"figures/TIC{tic}_phase_odd_even.png\"\n",
    "plt.savefig(out1, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ---- 2) Secondary window plot with binned overlay\n",
    "# convert to [0,1) for a tight zoom around 0.5\n",
    "ph01 = (ph + 1.0) % 1.0\n",
    "plt.figure(figsize=(10,5.2), dpi=120)\n",
    "plt.scatter(ph01[sec_m], f[sec_m], s=10, alpha=0.35, label=\"secondary\")\n",
    "xb, ym, ys = bin_xy(ph01[sec_m], f[sec_m], nbins=50)\n",
    "ok = np.isfinite(ym)\n",
    "plt.plot(xb[ok], ym[ok], lw=2, label=\"binned\")\n",
    "if np.any(ok & np.isfinite(ys)):\n",
    "    plt.fill_between(xb[ok], ym[ok]-ys[ok], ym[ok]+ys[ok], alpha=0.2)\n",
    "plt.xlim(0.49, 0.51)\n",
    "plt.ylim(*ylim)\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Flux (norm)\")\n",
    "plt.title(f\"TIC{tic} phase secondary — {toi}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "out2 = f\"figures/TIC{tic}_phase_secondary.png\"\n",
    "plt.savefig(out2, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved {out1} and {out2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15e30e9a-caf2-49af-950a-9ead1aa7b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures/TIC37749396_phase_odd_even.png and figures/TIC37749396_phase_secondary.png\n"
     ]
    }
   ],
   "source": [
    "# === PHASE PLOTS with BINNED OVERLAY (drop this in as one cell) ===\n",
    "import os, json, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "BTJD_ZERO = 2457000.0  # (not used here, just for consistency)\n",
    "\n",
    "# --------- pick target (edit these 3 lines per run) ----------\n",
    "#tic, toi = 119584412, \"TOI 1801.01\"     # A  -> TIC 119584412\n",
    "tic, toi = 37749396,  \"TOI 260.01\"    # B  -> TIC 37749396\n",
    "# tic, toi = 311183180, \"TOI 550.02\"    # C  -> TIC 311183180\n",
    "dur_hours = 6.0  # (A≈6–7 h, B≈4–5 h, C≈5–6 h). OK if approximate.\n",
    "\n",
    "# --------- helpers (no other cells required) ----------\n",
    "def load_ephem(tic):\n",
    "    for suffix in (\"\", \"_robust\"):\n",
    "        p = f\"results/TIC{tic}_refined_ephemeris{suffix}.json\"\n",
    "        if os.path.exists(p):\n",
    "            d = json.load(open(p))\n",
    "            return float(d[\"P\"]), float(d[\"T0\"])\n",
    "    raise FileNotFoundError(f\"refined ephemeris JSON for TIC{tic} not found\")\n",
    "\n",
    "def load_stitched_npz(tic):\n",
    "    p = f\"results/TIC{tic}_stitched.npz\"\n",
    "    d = np.load(p)\n",
    "    return d[\"t\"].astype(float), d[\"f\"].astype(float)\n",
    "\n",
    "def wrap_phase(t, P, T0):\n",
    "    # phase in [-0.5, 0.5)\n",
    "    ph = ((t - T0) / P) % 1.0\n",
    "    ph[ph >= 0.5] -= 1.0\n",
    "    return ph\n",
    "\n",
    "def in_window(t, P, T0, halfdur_days, center_phase=0.0):\n",
    "    # distance in phase from chosen center (0 for primary, 0.5 for secondary)\n",
    "    # work in [-0.5,0.5)\n",
    "    ph = wrap_phase(t, P, T0)\n",
    "    if center_phase == 0.5:\n",
    "        d = np.abs((ph - 0.5 + 0.5) % 1.0 - 0.5)\n",
    "    else:\n",
    "        d = np.abs(ph)\n",
    "    return d < (halfdur_days / P)\n",
    "\n",
    "def odd_even_masks(t, P, T0, halfdur_days):\n",
    "    # integer epoch index closest to each point\n",
    "    k = np.rint((t - T0) / P).astype(int)\n",
    "    w = in_window(t, P, T0, halfdur_days, center_phase=0.0)\n",
    "    odd  = w & (k % 2 != 0)\n",
    "    even = w & (k % 2 == 0)\n",
    "    return odd, even\n",
    "\n",
    "def bin_xy(x, y, nbins=80):\n",
    "    # mean & SEM per bin\n",
    "    edges = np.linspace(x.min(), x.max(), nbins+1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    xm = 0.5*(edges[:-1] + edges[1:])\n",
    "    ymean, ysem = np.full(nbins, np.nan), np.full(nbins, np.nan)\n",
    "    for i in range(nbins):\n",
    "        m = idx == i\n",
    "        if np.any(m):\n",
    "            yy = y[m]\n",
    "            ymean[i] = np.nanmean(yy)\n",
    "            ysem[i]  = np.nanstd(yy) / np.sqrt(np.sum(np.isfinite(yy)))\n",
    "    return xm, ymean, ysem\n",
    "\n",
    "# --------- load data & make plots ----------\n",
    "P, T0 = load_ephem(tic)\n",
    "t, f  = load_stitched_npz(tic)\n",
    "\n",
    "dur_days = dur_hours / 24.0\n",
    "halfdur  = 0.5 * dur_days\n",
    "\n",
    "ph = wrap_phase(t, P, T0)\n",
    "\n",
    "odd_m, even_m = odd_even_masks(t, P, T0, halfdur)\n",
    "sec_m = in_window(t, P, T0, halfdur, center_phase=0.5)\n",
    "\n",
    "# sensible y-lims around the in-window scatter\n",
    "y0 = np.nanmedian(f[odd_m | even_m])\n",
    "ylim = (y0 - 0.004, y0 + 0.004)  # ~±0.4% window; tweak if needed\n",
    "\n",
    "# ---- 1) Odd/Even primary window plot with binned overlay\n",
    "plt.figure(figsize=(10,5.2), dpi=120)\n",
    "plt.scatter(ph[odd_m],  f[odd_m],  s=9, alpha=0.25, label=\"odd\")\n",
    "plt.scatter(ph[even_m], f[even_m], s=9, alpha=0.25, label=\"even\")\n",
    "# binned means + 1σ(SEM) ribbons\n",
    "for lbl, m, c in [(\"odd\", odd_m, \"#1f77b4\"), (\"even\", even_m, \"#ff7f0e\")]:\n",
    "    xb, ym, ys = bin_xy(ph[m], f[m], nbins=60)\n",
    "    ok = np.isfinite(ym)\n",
    "    plt.plot(xb[ok], ym[ok], lw=2, color=c, label=f\"{lbl} (binned)\")\n",
    "    if np.any(ok & np.isfinite(ys)):\n",
    "        plt.fill_between(xb[ok], ym[ok]-ys[ok], ym[ok]+ys[ok], alpha=0.15, color=c)\n",
    "plt.xlim(-0.02, 0.02)\n",
    "plt.ylim(*ylim)\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Flux (norm)\")\n",
    "plt.title(f\"TIC{tic} phase odd/even — {toi}\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "out1 = f\"figures/TIC{tic}_phase_odd_even.png\"\n",
    "plt.savefig(out1, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ---- 2) Secondary window plot with binned overlay\n",
    "# convert to [0,1) for a tight zoom around 0.5\n",
    "ph01 = (ph + 1.0) % 1.0\n",
    "plt.figure(figsize=(10,5.2), dpi=120)\n",
    "plt.scatter(ph01[sec_m], f[sec_m], s=10, alpha=0.35, label=\"secondary\")\n",
    "xb, ym, ys = bin_xy(ph01[sec_m], f[sec_m], nbins=50)\n",
    "ok = np.isfinite(ym)\n",
    "plt.plot(xb[ok], ym[ok], lw=2, label=\"binned\")\n",
    "if np.any(ok & np.isfinite(ys)):\n",
    "    plt.fill_between(xb[ok], ym[ok]-ys[ok], ym[ok]+ys[ok], alpha=0.2)\n",
    "plt.xlim(0.49, 0.51)\n",
    "plt.ylim(*ylim)\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Flux (norm)\")\n",
    "plt.title(f\"TIC{tic} phase secondary — {toi}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "out2 = f\"figures/TIC{tic}_phase_secondary.png\"\n",
    "plt.savefig(out2, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved {out1} and {out2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4017061a-7d37-4365-bba5-9e8100096878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figures/TIC311183180_phase_odd_even.png and figures/TIC311183180_phase_secondary.png\n"
     ]
    }
   ],
   "source": [
    "# === PHASE PLOTS with BINNED OVERLAY (drop this in as one cell) ===\n",
    "import os, json, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "BTJD_ZERO = 2457000.0  # (not used here, just for consistency)\n",
    "\n",
    "# --------- pick target (edit these 3 lines per run) ----------\n",
    "#tic, toi = 119584412, \"TOI 1801.01\"     # A  -> TIC 119584412\n",
    "#tic, toi = 37749396,  \"TOI 260.01\"    # B  -> TIC 37749396\n",
    "tic, toi = 311183180, \"TOI 550.02\"    # C  -> TIC 311183180\n",
    "dur_hours = 6.0  # (A≈6–7 h, B≈4–5 h, C≈5–6 h). OK if approximate.\n",
    "\n",
    "# --------- helpers (no other cells required) ----------\n",
    "def load_ephem(tic):\n",
    "    for suffix in (\"\", \"_robust\"):\n",
    "        p = f\"results/TIC{tic}_refined_ephemeris{suffix}.json\"\n",
    "        if os.path.exists(p):\n",
    "            d = json.load(open(p))\n",
    "            return float(d[\"P\"]), float(d[\"T0\"])\n",
    "    raise FileNotFoundError(f\"refined ephemeris JSON for TIC{tic} not found\")\n",
    "\n",
    "def load_stitched_npz(tic):\n",
    "    p = f\"results/TIC{tic}_stitched.npz\"\n",
    "    d = np.load(p)\n",
    "    return d[\"t\"].astype(float), d[\"f\"].astype(float)\n",
    "\n",
    "def wrap_phase(t, P, T0):\n",
    "    # phase in [-0.5, 0.5)\n",
    "    ph = ((t - T0) / P) % 1.0\n",
    "    ph[ph >= 0.5] -= 1.0\n",
    "    return ph\n",
    "\n",
    "def in_window(t, P, T0, halfdur_days, center_phase=0.0):\n",
    "    # distance in phase from chosen center (0 for primary, 0.5 for secondary)\n",
    "    # work in [-0.5,0.5)\n",
    "    ph = wrap_phase(t, P, T0)\n",
    "    if center_phase == 0.5:\n",
    "        d = np.abs((ph - 0.5 + 0.5) % 1.0 - 0.5)\n",
    "    else:\n",
    "        d = np.abs(ph)\n",
    "    return d < (halfdur_days / P)\n",
    "\n",
    "def odd_even_masks(t, P, T0, halfdur_days):\n",
    "    # integer epoch index closest to each point\n",
    "    k = np.rint((t - T0) / P).astype(int)\n",
    "    w = in_window(t, P, T0, halfdur_days, center_phase=0.0)\n",
    "    odd  = w & (k % 2 != 0)\n",
    "    even = w & (k % 2 == 0)\n",
    "    return odd, even\n",
    "\n",
    "def bin_xy(x, y, nbins=80):\n",
    "    # mean & SEM per bin\n",
    "    edges = np.linspace(x.min(), x.max(), nbins+1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    xm = 0.5*(edges[:-1] + edges[1:])\n",
    "    ymean, ysem = np.full(nbins, np.nan), np.full(nbins, np.nan)\n",
    "    for i in range(nbins):\n",
    "        m = idx == i\n",
    "        if np.any(m):\n",
    "            yy = y[m]\n",
    "            ymean[i] = np.nanmean(yy)\n",
    "            ysem[i]  = np.nanstd(yy) / np.sqrt(np.sum(np.isfinite(yy)))\n",
    "    return xm, ymean, ysem\n",
    "\n",
    "# --------- load data & make plots ----------\n",
    "P, T0 = load_ephem(tic)\n",
    "t, f  = load_stitched_npz(tic)\n",
    "\n",
    "dur_days = dur_hours / 24.0\n",
    "halfdur  = 0.5 * dur_days\n",
    "\n",
    "ph = wrap_phase(t, P, T0)\n",
    "\n",
    "odd_m, even_m = odd_even_masks(t, P, T0, halfdur)\n",
    "sec_m = in_window(t, P, T0, halfdur, center_phase=0.5)\n",
    "\n",
    "# sensible y-lims around the in-window scatter\n",
    "y0 = np.nanmedian(f[odd_m | even_m])\n",
    "ylim = (y0 - 0.004, y0 + 0.004)  # ~±0.4% window; tweak if needed\n",
    "\n",
    "# ---- 1) Odd/Even primary window plot with binned overlay\n",
    "plt.figure(figsize=(10,5.2), dpi=120)\n",
    "plt.scatter(ph[odd_m],  f[odd_m],  s=9, alpha=0.25, label=\"odd\")\n",
    "plt.scatter(ph[even_m], f[even_m], s=9, alpha=0.25, label=\"even\")\n",
    "# binned means + 1σ(SEM) ribbons\n",
    "for lbl, m, c in [(\"odd\", odd_m, \"#1f77b4\"), (\"even\", even_m, \"#ff7f0e\")]:\n",
    "    xb, ym, ys = bin_xy(ph[m], f[m], nbins=60)\n",
    "    ok = np.isfinite(ym)\n",
    "    plt.plot(xb[ok], ym[ok], lw=2, color=c, label=f\"{lbl} (binned)\")\n",
    "    if np.any(ok & np.isfinite(ys)):\n",
    "        plt.fill_between(xb[ok], ym[ok]-ys[ok], ym[ok]+ys[ok], alpha=0.15, color=c)\n",
    "plt.xlim(-0.02, 0.02)\n",
    "plt.ylim(*ylim)\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Flux (norm)\")\n",
    "plt.title(f\"TIC{tic} phase odd/even — {toi}\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "out1 = f\"figures/TIC{tic}_phase_odd_even.png\"\n",
    "plt.savefig(out1, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ---- 2) Secondary window plot with binned overlay\n",
    "# convert to [0,1) for a tight zoom around 0.5\n",
    "ph01 = (ph + 1.0) % 1.0\n",
    "plt.figure(figsize=(10,5.2), dpi=120)\n",
    "plt.scatter(ph01[sec_m], f[sec_m], s=10, alpha=0.35, label=\"secondary\")\n",
    "xb, ym, ys = bin_xy(ph01[sec_m], f[sec_m], nbins=50)\n",
    "ok = np.isfinite(ym)\n",
    "plt.plot(xb[ok], ym[ok], lw=2, label=\"binned\")\n",
    "if np.any(ok & np.isfinite(ys)):\n",
    "    plt.fill_between(xb[ok], ym[ok]-ys[ok], ym[ok]+ys[ok], alpha=0.2)\n",
    "plt.xlim(0.49, 0.51)\n",
    "plt.ylim(*ylim)\n",
    "plt.xlabel(\"Phase\")\n",
    "plt.ylabel(\"Flux (norm)\")\n",
    "plt.title(f\"TIC{tic} phase secondary — {toi}\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "out2 = f\"figures/TIC{tic}_phase_secondary.png\"\n",
    "plt.savefig(out2, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved {out1} and {out2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e681e-db64-473c-a0be-92ccbf63f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG: switch target & sectors here only ---\n",
    "TIC      = 119584412          # e.g., A=119584412, B=37749396, C=311183180\n",
    "NAME     = \"TOI 1801.01\"      # label for titles\n",
    "SECTORS  = [22, 49]           # sectors you want to run\n",
    "\n",
    "# Refined ephemeris for this target (use your per-target JSON if you have it)\n",
    "P_days    = 16.02749976\n",
    "T0_btjd   = 1908.046283\n",
    "DUR_hours = 6.53\n",
    "\n",
    "# Figure style knobs (optional)\n",
    "FIGSIZE   = (12, 4)           # (width, height) inches per 3-panel row\n",
    "DPI_SAVE  = 220               # PNG DPI; raise to 300–400 for print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f683f8-2bfa-4d15-acc8-a04f4038e007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tpf] Using SPOC TPF for TIC 119584412, S22\n",
      "[S22] in-transit cadences = 392, out-of-transit = 15319\n",
      "[save] figures/TIC119584412_FigureB_S22.png\n",
      "[tpf] Using SPOC TPF for TIC 119584412, S49\n",
      "[S49] in-transit cadences = 392, out-of-transit = 12488\n",
      "[save] figures/TIC119584412_FigureB_S49.png\n",
      "[save] results/TIC119584412_centroid_offsets.csv\n",
      "[save] figures/TIC119584412_FigureB_combo.png\n"
     ]
    }
   ],
   "source": [
    "# === Figure B (one-cell version): Difference images & centroid offsets ===\n",
    "# Target A example defaults: TOI 1801.01 / TIC 119584412, sectors 22 & 49\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# -------------------- ADJUST ME --------------------\n",
    "TIC          = 119584412                # TIC ID\n",
    "TARGET_NAME  = \"TOI 1801.01\"            # label for titles\n",
    "SECTORS      = [22, 49]                 # which sectors to process\n",
    "\n",
    "# Ephemeris (used only to decide in/out-of-transit frame selection)\n",
    "P_days       = 16.02749976              # period (days)\n",
    "T0_btjd      = 1908.046283              # reference mid-transit (BTJD)\n",
    "DUR_hours    = 6.53                     # transit duration estimate (hours)\n",
    "PAD          = 1.0                      # frames counted \"out\" must be > PAD*duration away\n",
    "\n",
    "# Quality and robustness\n",
    "QUALITY_GOOD = 0                        # keep only quality==0 if present\n",
    "MIN_IN       = 10                       # min # of in-transit cadences to proceed\n",
    "MIN_OUT      = 50                       # min # of out-of-transit cadences to proceed\n",
    "\n",
    "# File outputs\n",
    "FIG_DIR      = \"figures\"\n",
    "RES_DIR      = \"results\"\n",
    "DPI_FIG      = 200\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import os, csv, numpy as np, matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from astropy.wcs.utils import proj_plane_pixel_scales\n",
    "\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "def _get_tpf(tic, sector, cutout_size=15):\n",
    "    \"\"\"Prefer SPOC TargetPixelFile; fall back to TessCut if needed.\"\"\"\n",
    "    sr = lk.search_targetpixelfile(f\"TIC {tic}\", mission=\"TESS\", sector=sector, author=\"SPOC\")\n",
    "    if len(sr) > 0:\n",
    "        print(f\"[tpf] Using SPOC TPF for TIC {tic}, S{sector}\")\n",
    "        return sr.download()\n",
    "    sc = lk.search_tesscut(f\"TIC {tic}\", sector=sector)\n",
    "    if len(sc) == 0:\n",
    "        raise FileNotFoundError(f\"No TPF or TessCut found for TIC {tic} sector {sector}\")\n",
    "    print(f\"[tpf] Using TessCut cutout for TIC {tic}, S{sector} (size={cutout_size})\")\n",
    "    return sc.download(cutout_size=cutout_size)\n",
    "\n",
    "def _transit_masks(time_btjd, P, T0, dur_h, pad=1.0):\n",
    "    \"\"\"Boolean masks for in- and out-of-transit cadences.\"\"\"\n",
    "    dur_d = float(dur_h) / 24.0\n",
    "    phase = ((time_btjd - T0 + 0.5*P) % P) - 0.5*P  # centered phase in days\n",
    "    in_tr  = np.abs(phase) < 0.5*dur_d\n",
    "    out_tr = np.abs(phase) > pad * dur_d\n",
    "    return in_tr, out_tr\n",
    "\n",
    "def _centroid_xy(image2d):\n",
    "    \"\"\"Flux-weighted centroid in pixel coordinates (uses only positive flux).\"\"\"\n",
    "    yy, xx = np.indices(image2d.shape)\n",
    "    w = np.clip(image2d, 0, np.inf)\n",
    "    s = np.nansum(w)\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.nan, np.nan\n",
    "    x = np.nansum(w * xx) / s\n",
    "    y = np.nansum(w * yy) / s\n",
    "    return float(x), float(y)\n",
    "\n",
    "def _pixel_scale_arcsec(tpf):\n",
    "    try:\n",
    "        # degrees/pixel -> arcsec/pixel\n",
    "        return float(np.mean(proj_plane_pixel_scales(tpf.wcs)) * 3600.0)\n",
    "    except Exception:\n",
    "        return 21.0  # TESS nominal plate scale if WCS missing\n",
    "\n",
    "def _figureB_for_sector(tic, sector, P, T0, dur_h, target_label):\n",
    "    tpf = _get_tpf(tic, sector)\n",
    "\n",
    "    # Lightweight quality screen\n",
    "    t = np.asarray(tpf.time.value, float)                  # BTJD as float array\n",
    "    good = np.isfinite(t)\n",
    "    if hasattr(tpf, \"quality\") and tpf.quality is not None:\n",
    "        good &= (tpf.quality == QUALITY_GOOD)\n",
    "\n",
    "    in_tr, out_tr = _transit_masks(t, P, T0, dur_h, pad=PAD)\n",
    "    use_in, use_out = (good & in_tr), (good & out_tr)\n",
    "\n",
    "    n_in, n_out = int(use_in.sum()), int(use_out.sum())\n",
    "    print(f\"[S{sector}] in-transit cadences = {n_in}, out-of-transit = {n_out}\")\n",
    "\n",
    "    if (n_in < MIN_IN) or (n_out < MIN_OUT):\n",
    "        raise RuntimeError(f\"S{sector}: not enough cadences for clean diff image \"\n",
    "                           f\"(need ≥{MIN_IN} in, ≥{MIN_OUT} out).\")\n",
    "\n",
    "    # Convert astropy Quantity (e-/s) → plain floats before stats/plotting\n",
    "    in_cube  = tpf.flux[use_in].value   # (N_in, y, x)\n",
    "    out_cube = tpf.flux[use_out].value  # (N_out, y, x)\n",
    "\n",
    "    in_img   = np.nanmedian(in_cube,  axis=0).astype(float)\n",
    "    out_img  = np.nanmedian(out_cube, axis=0).astype(float)\n",
    "    diff     = out_img - in_img  # positive where the star got dimmer (a transit)\n",
    "\n",
    "    # Flux centroids\n",
    "    x_star, y_star = _centroid_xy(out_img)   # stellar PSF center\n",
    "    x_diff, y_diff = _centroid_xy(diff)      # location of the dip in the diff image\n",
    "    dx_pix, dy_pix = (x_diff - x_star), (y_diff - y_star)\n",
    "    r_pix          = float(np.hypot(dx_pix, dy_pix))\n",
    "\n",
    "    # Convert to arcsec\n",
    "    pxscale = _pixel_scale_arcsec(tpf)\n",
    "    r_arcsec = r_pix * pxscale\n",
    "\n",
    "    # ----- Plot three panels -----\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "    vmin = float(np.nanpercentile(out_img,  5.0))\n",
    "    vmax = float(np.nanpercentile(out_img, 99.5))\n",
    "\n",
    "    im0 = axes[0].imshow(out_img, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title(\"Out of transit\")\n",
    "    axes[0].plot(x_star, y_star, marker=\"+\", ms=12, mfc=\"none\", mec=\"w\")\n",
    "\n",
    "    im1 = axes[1].imshow(in_img, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title(\"In transit\")\n",
    "    axes[1].plot(x_star, y_star, marker=\"+\", ms=12, mfc=\"none\", mec=\"w\")\n",
    "\n",
    "    d_vmax = float(np.nanpercentile(np.abs(diff), 99.5))\n",
    "    im2 = axes[2].imshow(diff, origin=\"lower\", cmap=\"coolwarm\", vmin=-d_vmax, vmax=d_vmax)\n",
    "    axes[2].set_title(\"Difference (out − in)\")\n",
    "    axes[2].plot(x_diff, y_diff, marker=\"x\", ms=12, mec=\"k\", mew=2)\n",
    "    axes[2].plot(x_star, y_star, marker=\"+\", ms=12, mec=\"k\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    header = (f\"{target_label} (TIC {tic})  •  Sector {sector}\\n\"\n",
    "              f\"P={P:.6f} d, T0={T0:.6f} BTJD, dur≈{dur_h:.2f} h   \"\n",
    "              f\"N_in={n_in}, N_out={n_out}   \"\n",
    "              f\"centroid offset={r_pix:.2f} px ≈ {r_arcsec:.1f}\\\"\")\n",
    "    fig.suptitle(header, fontsize=11)\n",
    "\n",
    "    # Labeled colorbars (plain strings so units don’t crash Matplotlib)\n",
    "    cb0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04); cb0.ax.set_ylabel(\"flux (e-/s)\")\n",
    "    cb1 = fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04); cb1.ax.set_ylabel(\"flux (e-/s)\")\n",
    "    cb2 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04); cb2.ax.set_ylabel(\"Δ flux (e-/s)\")\n",
    "\n",
    "    png_path = os.path.join(FIG_DIR, f\"TIC{tic}_FigureB_S{sector}.png\")\n",
    "    fig.savefig(png_path, dpi=DPI_FIG)\n",
    "    plt.close(fig)\n",
    "    print(f\"[save] {png_path}\")\n",
    "\n",
    "    return {\n",
    "        \"tic\": tic, \"sector\": sector, \"P_days\": P, \"T0_btjd\": T0, \"dur_h\": dur_h,\n",
    "        \"N_in\": n_in, \"N_out\": n_out,\n",
    "        \"x_star\": x_star, \"y_star\": y_star,\n",
    "        \"x_diff\": x_diff, \"y_diff\": y_diff,\n",
    "        \"dx_pix\": float(dx_pix), \"dy_pix\": float(dy_pix), \"r_pix\": r_pix,\n",
    "        \"pix_scale_arcsec\": pxscale, \"r_arcsec\": r_arcsec,\n",
    "        \"panel_png\": png_path\n",
    "    }\n",
    "\n",
    "# -------------------- RUN --------------------\n",
    "rows = []\n",
    "for sec in SECTORS:\n",
    "    try:\n",
    "        row = _figureB_for_sector(TIC, sec, P_days, T0_btjd, DUR_hours, TARGET_NAME)\n",
    "        rows.append(row)\n",
    "    except Exception as e:\n",
    "        print(f\"[S{sec}] ERROR:\", e)\n",
    "\n",
    "# Save CSV if any rows\n",
    "csv_path = os.path.join(RES_DIR, f\"TIC{TIC}_centroid_offsets.csv\")\n",
    "if rows:\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    print(f\"[save] {csv_path}\")\n",
    "\n",
    "    # Optional: combined gallery\n",
    "    import matplotlib.image as mpimg\n",
    "    fig, ax = plt.subplots(len(rows), 1, figsize=(6, 3.8*len(rows)))\n",
    "    if len(rows) == 1:\n",
    "        ax = [ax]\n",
    "    for i, r in enumerate(rows):\n",
    "        ax[i].imshow(mpimg.imread(r[\"panel_png\"])); ax[i].axis(\"off\")\n",
    "        ax[i].set_title(f\"{TARGET_NAME} • Sector {r['sector']}\")\n",
    "    combo = os.path.join(FIG_DIR, f\"TIC{TIC}_FigureB_combo.png\")\n",
    "    plt.tight_layout(); plt.savefig(combo, dpi=180); plt.close()\n",
    "    print(f\"[save] {combo}\")\n",
    "else:\n",
    "    print(\"[skip] No rows were produced; CSV & combo skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48be4b88-5e0d-4de7-934a-be5f6c8f35f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S3] ERROR: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[tpf] Using SPOC TPF for TIC 37749396, S42\n",
      "[S42] in-transit cadences = 90, out-of-transit = 11293\n",
      "[save] figures/TIC37749396_FigureB_S42.png\n",
      "[tpf] Using SPOC TPF for TIC 37749396, S70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/lightkurve/search.py:424: LightkurveWarning: Warning: 2 files available to download. Only the first file has been downloaded. Please use `download_all()` or specify additional criteria (e.g. quarter, campaign, or sector) to limit your search.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S70] in-transit cadences = 1063, out-of-transit = 82549\n",
      "[save] figures/TIC37749396_FigureB_S70.png\n",
      "[save] results/TIC37749396_centroid_offsets.csv\n",
      "[save] figures/TIC37749396_FigureB_combo.png\n"
     ]
    }
   ],
   "source": [
    "# === Figure B: Difference images & centroid offsets (Target B) ===\n",
    "# One-cell, copy/paste runnable block\n",
    "\n",
    "import os, csv, numpy as np, matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from astropy.wcs.utils import proj_plane_pixel_scales\n",
    "\n",
    "# ---------- Target & ephemeris (adjust here if needed) ----------\n",
    "TIC     = 37749396                 # Target B (TOI 260.01)\n",
    "NAME    = \"TOI 260.01\"\n",
    "SECTORS = [3, 42, 70]              # sectors you ran BLS/TLS on\n",
    "P_days    = 13.47582381            # refined period (Sep 16 entry)\n",
    "T0_btjd   = 1392.306006            # refined T0   (Sep 16 entry)\n",
    "DUR_hours = 3.0                    # transit duration guess; tweak if you like (e.g., 1.5–3.0 h)\n",
    "\n",
    "# ---------- Folders ----------\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_tpf(tic, sector, cutout_size=15):\n",
    "    \"\"\"Prefer SPOC 2-min/20-sec TPF; else fall back to TessCut FFI cutout.\"\"\"\n",
    "    sr = lk.search_targetpixelfile(f\"TIC {tic}\", mission=\"TESS\", sector=sector, author=\"SPOC\")\n",
    "    if len(sr) > 0:\n",
    "        print(f\"[tpf] Using SPOC TPF for TIC {tic}, S{sector}\")\n",
    "        return sr.download()\n",
    "    sc = lk.search_tesscut(f\"TIC {tic}\", sector=sector)\n",
    "    if len(sc) == 0:\n",
    "        raise FileNotFoundError(f\"No TPF or TessCut for TIC {tic} S{sector}\")\n",
    "    print(f\"[tpf] Using TessCut cutout for TIC {tic}, S{sector} (size={cutout_size})\")\n",
    "    return sc.download(cutout_size=cutout_size)\n",
    "\n",
    "def transit_masks(time_btjd, P, T0, dur_h, pad=1.0):\n",
    "    \"\"\"Boolean masks for in- and out-of-transit.\"\"\"\n",
    "    dur_d = dur_h / 24.0\n",
    "    phase = ((time_btjd - T0 + 0.5*P) % P) - 0.5*P\n",
    "    in_tr  = np.abs(phase) < 0.5*dur_d\n",
    "    out_tr = np.abs(phase) > pad * dur_d\n",
    "    return in_tr, out_tr\n",
    "\n",
    "def flux_weighted_centroid(img_float2d):\n",
    "    \"\"\"Return centroid (x,y) in pixel coords using positive flux.\"\"\"\n",
    "    yy, xx = np.indices(img_float2d.shape)\n",
    "    w = np.clip(img_float2d, 0, np.inf)\n",
    "    s = np.nansum(w)\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.nan, np.nan\n",
    "    x = np.nansum(w * xx) / s\n",
    "    y = np.nansum(w * yy) / s\n",
    "    return float(x), float(y)\n",
    "\n",
    "def pixel_scale_arcsec(tpf):\n",
    "    try:\n",
    "        scales_deg_per_pix = proj_plane_pixel_scales(tpf.wcs)  # deg/pix\n",
    "        return float(np.mean(scales_deg_per_pix) * 3600.0)\n",
    "    except Exception:\n",
    "        return 21.0  # TESS nominal\n",
    "\n",
    "def figureB_for_sector(tic, sector, P, T0, dur_h, name, pad=1.0, tesscut_size=15):\n",
    "    tpf = get_tpf(tic, sector, cutout_size=tesscut_size)\n",
    "    t = np.asarray(tpf.time.value, float)  # BTJD\n",
    "\n",
    "    # Quality mask if available\n",
    "    good = np.isfinite(t)\n",
    "    if hasattr(tpf, \"quality\") and tpf.quality is not None:\n",
    "        good &= (tpf.quality == 0)\n",
    "\n",
    "    in_tr, out_tr = transit_masks(t, P, T0, dur_h, pad=pad)\n",
    "    use_in, use_out = (good & in_tr), (good & out_tr)\n",
    "\n",
    "    n_in, n_out = int(use_in.sum()), int(use_out.sum())\n",
    "    print(f\"[S{sector}] in-transit cadences = {n_in}, out-of-transit = {n_out}\")\n",
    "    if n_in < 10 or n_out < 50:\n",
    "        print(f\"[S{sector}] Warning: few cadences; difference image may be noisy.\")\n",
    "\n",
    "    # Convert Quantity (e-/s) -> float arrays before stats/plotting\n",
    "    in_cube  = tpf.flux[use_in].value\n",
    "    out_cube = tpf.flux[use_out].value\n",
    "    in_img   = np.nanmedian(in_cube,  axis=0).astype(float)\n",
    "    out_img  = np.nanmedian(out_cube, axis=0).astype(float)\n",
    "    diff     = out_img - in_img  # positive where star dimmed\n",
    "\n",
    "    # Centroids\n",
    "    x_star, y_star = flux_weighted_centroid(out_img)\n",
    "    x_diff, y_diff = flux_weighted_centroid(diff)\n",
    "    dx_pix = x_diff - x_star\n",
    "    dy_pix = y_diff - y_star\n",
    "    r_pix  = float(np.hypot(dx_pix, dy_pix))\n",
    "\n",
    "    # Arcsec conversion\n",
    "    ps = pixel_scale_arcsec(tpf)\n",
    "    r_arcsec = r_pix * ps\n",
    "\n",
    "    # --- Plot panels ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    vmin = float(np.nanpercentile(out_img,  5.0))\n",
    "    vmax = float(np.nanpercentile(out_img, 99.5))\n",
    "\n",
    "    im0 = axes[0].imshow(out_img, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title(\"Out of transit\")\n",
    "    axes[0].plot(x_star, y_star, marker=\"+\", ms=12, mfc=\"none\", mec=\"w\")\n",
    "\n",
    "    im1 = axes[1].imshow(in_img, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title(\"In transit\")\n",
    "    axes[1].plot(x_star, y_star, marker=\"+\", ms=12, mfc=\"none\", mec=\"w\")\n",
    "\n",
    "    d_vmax = float(np.nanpercentile(np.abs(diff), 99.5))\n",
    "    im2 = axes[2].imshow(diff, origin=\"lower\", cmap=\"coolwarm\", vmin=-d_vmax, vmax=d_vmax)\n",
    "    axes[2].set_title(\"Difference (out − in)\")\n",
    "    axes[2].plot(x_diff, y_diff, marker=\"x\", ms=12, mec=\"k\", mew=2)\n",
    "    axes[2].plot(x_star, y_star, marker=\"+\", ms=12, mec=\"k\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    txt = (f\"{name} (TIC {tic})  •  Sector {sector}\\n\"\n",
    "           f\"P={P:.6f} d, T0={T0:.6f} BTJD, dur≈{dur_h:.2f} h\\n\"\n",
    "           f\"N_in={n_in}, N_out={n_out}  •  centroid offset={r_pix:.2f} px ≈ {r_arcsec:.1f}\\\"\")\n",
    "    fig.suptitle(txt, fontsize=11)\n",
    "\n",
    "    # Colorbars\n",
    "    c0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04); c0.ax.set_ylabel(\"flux (e-/s)\")\n",
    "    c1 = fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04); c1.ax.set_ylabel(\"flux (e-/s)\")\n",
    "    c2 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04); c2.ax.set_ylabel(\"Δ flux (e-/s)\")\n",
    "\n",
    "    out_png = f\"figures/TIC{tic}_FigureB_S{sector}.png\"\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"[save] {out_png}\")\n",
    "\n",
    "    return {\n",
    "        \"tic\": tic, \"name\": name, \"sector\": sector,\n",
    "        \"P_days\": P, \"T0_btjd\": T0, \"dur_h\": dur_h,\n",
    "        \"N_in\": n_in, \"N_out\": n_out,\n",
    "        \"x_star\": x_star, \"y_star\": y_star,\n",
    "        \"x_diff\": x_diff, \"y_diff\": y_diff,\n",
    "        \"dx_pix\": float(dx_pix), \"dy_pix\": float(dy_pix), \"r_pix\": r_pix,\n",
    "        \"pix_scale_arcsec\": ps, \"r_arcsec\": r_arcsec\n",
    "    }\n",
    "\n",
    "# ---------- Run & save CSV + combo ----------\n",
    "rows = []\n",
    "for sec in SECTORS:\n",
    "    try:\n",
    "        rows.append(figureB_for_sector(TIC, sec, P_days, T0_btjd, DUR_hours, NAME))\n",
    "    except Exception as e:\n",
    "        print(f\"[S{sec}] ERROR:\", e)\n",
    "\n",
    "csv_path = f\"results/TIC{TIC}_centroid_offsets.csv\"\n",
    "if rows:\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    print(f\"[save] {csv_path}\")\n",
    "\n",
    "    # Optional: combined gallery\n",
    "    import matplotlib.image as mpimg\n",
    "    fig, ax = plt.subplots(len(rows), 1, figsize=(6, 3.8*len(rows)))\n",
    "    if len(rows) == 1:\n",
    "        ax = [ax]\n",
    "    for i, r in enumerate(rows):\n",
    "        png = f\"figures/TIC{r['tic']}_FigureB_S{r['sector']}.png\"\n",
    "        ax[i].imshow(mpimg.imread(png)); ax[i].axis(\"off\")\n",
    "        ax[i].set_title(f\"{NAME} • Sector {r['sector']}\")\n",
    "    combo = f\"figures/TIC{TIC}_FigureB_combo.png\"\n",
    "    plt.tight_layout(); plt.savefig(combo, dpi=180); plt.close()\n",
    "    print(f\"[save] {combo}\")\n",
    "else:\n",
    "    print(\"[skip] No rows to write (all sectors failed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b69c3b-5bad-4728-b694-4e90f4665eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tpf] Using SPOC TPF for TIC 311183180, S5\n",
      "[S5] in-transit cadences = 251, out-of-transit = 16783\n",
      "[save] figures/TIC311183180_FigureB_S5.png\n",
      "[tpf] Using SPOC TPF for TIC 311183180, S31\n",
      "[S31] in-transit cadences = 252, out-of-transit = 15745\n",
      "[save] figures/TIC311183180_FigureB_S31.png\n",
      "[save] results/TIC311183180_centroid_offsets.csv\n",
      "[save] figures/TIC311183180_FigureB_combo.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# One-cell, copy/paste runnable block\n",
    "\n",
    "import os, csv, numpy as np, matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from astropy.wcs.utils import proj_plane_pixel_scales\n",
    "\n",
    "# ---------- Target & ephemeris (adjust here if needed) ----------\n",
    "TIC    = 311183180        # Target C\n",
    "NAME   = \"TOI 550.02\"\n",
    "SECTORS = [5, 31]\n",
    "\n",
    "# Refined ephemeris (your results)\n",
    "P_days    = 9.34849077\n",
    "T0_btjd   = 2149.633454\n",
    "DUR_hours = 2.8   # use your measured duration from TLS/midtime fits\n",
    "# ---------- Folders ----------\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_tpf(tic, sector, cutout_size=15):\n",
    "    \"\"\"Prefer SPOC 2-min/20-sec TPF; else fall back to TessCut FFI cutout.\"\"\"\n",
    "    sr = lk.search_targetpixelfile(f\"TIC {tic}\", mission=\"TESS\", sector=sector, author=\"SPOC\")\n",
    "    if len(sr) > 0:\n",
    "        print(f\"[tpf] Using SPOC TPF for TIC {tic}, S{sector}\")\n",
    "        return sr.download()\n",
    "    sc = lk.search_tesscut(f\"TIC {tic}\", sector=sector)\n",
    "    if len(sc) == 0:\n",
    "        raise FileNotFoundError(f\"No TPF or TessCut for TIC {tic} S{sector}\")\n",
    "    print(f\"[tpf] Using TessCut cutout for TIC {tic}, S{sector} (size={cutout_size})\")\n",
    "    return sc.download(cutout_size=cutout_size)\n",
    "\n",
    "def transit_masks(time_btjd, P, T0, dur_h, pad=1.0):\n",
    "    \"\"\"Boolean masks for in- and out-of-transit.\"\"\"\n",
    "    dur_d = dur_h / 24.0\n",
    "    phase = ((time_btjd - T0 + 0.5*P) % P) - 0.5*P\n",
    "    in_tr  = np.abs(phase) < 0.5*dur_d\n",
    "    out_tr = np.abs(phase) > pad * dur_d\n",
    "    return in_tr, out_tr\n",
    "\n",
    "def flux_weighted_centroid(img_float2d):\n",
    "    \"\"\"Return centroid (x,y) in pixel coords using positive flux.\"\"\"\n",
    "    yy, xx = np.indices(img_float2d.shape)\n",
    "    w = np.clip(img_float2d, 0, np.inf)\n",
    "    s = np.nansum(w)\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.nan, np.nan\n",
    "    x = np.nansum(w * xx) / s\n",
    "    y = np.nansum(w * yy) / s\n",
    "    return float(x), float(y)\n",
    "\n",
    "def pixel_scale_arcsec(tpf):\n",
    "    try:\n",
    "        scales_deg_per_pix = proj_plane_pixel_scales(tpf.wcs)  # deg/pix\n",
    "        return float(np.mean(scales_deg_per_pix) * 3600.0)\n",
    "    except Exception:\n",
    "        return 21.0  # TESS nominal\n",
    "\n",
    "def figureB_for_sector(tic, sector, P, T0, dur_h, name, pad=1.0, tesscut_size=15):\n",
    "    tpf = get_tpf(tic, sector, cutout_size=tesscut_size)\n",
    "    t = np.asarray(tpf.time.value, float)  # BTJD\n",
    "\n",
    "    # Quality mask if available\n",
    "    good = np.isfinite(t)\n",
    "    if hasattr(tpf, \"quality\") and tpf.quality is not None:\n",
    "        good &= (tpf.quality == 0)\n",
    "\n",
    "    in_tr, out_tr = transit_masks(t, P, T0, dur_h, pad=pad)\n",
    "    use_in, use_out = (good & in_tr), (good & out_tr)\n",
    "\n",
    "    n_in, n_out = int(use_in.sum()), int(use_out.sum())\n",
    "    print(f\"[S{sector}] in-transit cadences = {n_in}, out-of-transit = {n_out}\")\n",
    "    if n_in < 10 or n_out < 50:\n",
    "        print(f\"[S{sector}] Warning: few cadences; difference image may be noisy.\")\n",
    "\n",
    "    # Convert Quantity (e-/s) -> float arrays before stats/plotting\n",
    "    in_cube  = tpf.flux[use_in].value\n",
    "    out_cube = tpf.flux[use_out].value\n",
    "    in_img   = np.nanmedian(in_cube,  axis=0).astype(float)\n",
    "    out_img  = np.nanmedian(out_cube, axis=0).astype(float)\n",
    "    diff     = out_img - in_img  # positive where star dimmed\n",
    "\n",
    "    # Centroids\n",
    "    x_star, y_star = flux_weighted_centroid(out_img)\n",
    "    x_diff, y_diff = flux_weighted_centroid(diff)\n",
    "    dx_pix = x_diff - x_star\n",
    "    dy_pix = y_diff - y_star\n",
    "    r_pix  = float(np.hypot(dx_pix, dy_pix))\n",
    "\n",
    "    # Arcsec conversion\n",
    "    ps = pixel_scale_arcsec(tpf)\n",
    "    r_arcsec = r_pix * ps\n",
    "\n",
    "    # --- Plot panels ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "    vmin = float(np.nanpercentile(out_img,  5.0))\n",
    "    vmax = float(np.nanpercentile(out_img, 99.5))\n",
    "\n",
    "    im0 = axes[0].imshow(out_img, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title(\"Out of transit\")\n",
    "    axes[0].plot(x_star, y_star, marker=\"+\", ms=12, mfc=\"none\", mec=\"w\")\n",
    "\n",
    "    im1 = axes[1].imshow(in_img, origin=\"lower\", vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title(\"In transit\")\n",
    "    axes[1].plot(x_star, y_star, marker=\"+\", ms=12, mfc=\"none\", mec=\"w\")\n",
    "\n",
    "    d_vmax = float(np.nanpercentile(np.abs(diff), 99.5))\n",
    "    im2 = axes[2].imshow(diff, origin=\"lower\", cmap=\"coolwarm\", vmin=-d_vmax, vmax=d_vmax)\n",
    "    axes[2].set_title(\"Difference (out − in)\")\n",
    "    axes[2].plot(x_diff, y_diff, marker=\"x\", ms=12, mec=\"k\", mew=2)\n",
    "    axes[2].plot(x_star, y_star, marker=\"+\", ms=12, mec=\"k\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "    txt = (f\"{name} (TIC {tic})  •  Sector {sector}\\n\"\n",
    "           f\"P={P:.6f} d, T0={T0:.6f} BTJD, dur≈{dur_h:.2f} h\\n\"\n",
    "           f\"N_in={n_in}, N_out={n_out}  •  centroid offset={r_pix:.2f} px ≈ {r_arcsec:.1f}\\\"\")\n",
    "    fig.suptitle(txt, fontsize=11)\n",
    "\n",
    "    # Colorbars\n",
    "    c0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04); c0.ax.set_ylabel(\"flux (e-/s)\")\n",
    "    c1 = fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04); c1.ax.set_ylabel(\"flux (e-/s)\")\n",
    "    c2 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04); c2.ax.set_ylabel(\"Δ flux (e-/s)\")\n",
    "\n",
    "    out_png = f\"figures/TIC{tic}_FigureB_S{sector}.png\"\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"[save] {out_png}\")\n",
    "\n",
    "    return {\n",
    "        \"tic\": tic, \"name\": name, \"sector\": sector,\n",
    "        \"P_days\": P, \"T0_btjd\": T0, \"dur_h\": dur_h,\n",
    "        \"N_in\": n_in, \"N_out\": n_out,\n",
    "        \"x_star\": x_star, \"y_star\": y_star,\n",
    "        \"x_diff\": x_diff, \"y_diff\": y_diff,\n",
    "        \"dx_pix\": float(dx_pix), \"dy_pix\": float(dy_pix), \"r_pix\": r_pix,\n",
    "        \"pix_scale_arcsec\": ps, \"r_arcsec\": r_arcsec\n",
    "    }\n",
    "\n",
    "# ---------- Run & save CSV + combo ----------\n",
    "rows = []\n",
    "for sec in SECTORS:\n",
    "    try:\n",
    "        rows.append(figureB_for_sector(TIC, sec, P_days, T0_btjd, DUR_hours, NAME))\n",
    "    except Exception as e:\n",
    "        print(f\"[S{sec}] ERROR:\", e)\n",
    "\n",
    "csv_path = f\"results/TIC{TIC}_centroid_offsets.csv\"\n",
    "if rows:\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "    print(f\"[save] {csv_path}\")\n",
    "\n",
    "    # Optional: combined gallery\n",
    "    import matplotlib.image as mpimg\n",
    "    fig, ax = plt.subplots(len(rows), 1, figsize=(6, 3.8*len(rows)))\n",
    "    if len(rows) == 1:\n",
    "        ax = [ax]\n",
    "    for i, r in enumerate(rows):\n",
    "        png = f\"figures/TIC{r['tic']}_FigureB_S{r['sector']}.png\"\n",
    "        ax[i].imshow(mpimg.imread(png)); ax[i].axis(\"off\")\n",
    "        ax[i].set_title(f\"{NAME} • Sector {r['sector']}\")\n",
    "    combo = f\"figures/TIC{TIC}_FigureB_combo.png\"\n",
    "    plt.tight_layout(); plt.savefig(combo, dpi=180); plt.close()\n",
    "    print(f\"[save] {combo}\")\n",
    "else:\n",
    "    print(\"[skip] No rows to write (all sectors failed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d0cfbe-2263-4873-a036-a4c43320f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/bin/python\n",
      "numpy: 1.26.4\n",
      "tri at: /Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/triceratops/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy as np, triceratops\n",
    "print(sys.executable)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"tri at:\", triceratops.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9624d9a-03e6-4deb-af2b-e9cc68eb6137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████ | 85937/88173 periods | 13:40:05<21:20\n",
      "  4%|█▌                                  | 114/2572 periods | 13:12:41<284:51:39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 29374 data points, 1065 periods from 4.85 to 5.15 days\n",
      "Using 7 of 8 CPU threads\n",
      "Searching for best T0 for period 5.14994 days\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 33536 data points, 1066 periods from 4.85 to 5.15 days\n",
      "Using 7 of 8 CPU threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 141 of 148 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best T0 for period 4.97446 days\n",
      "[clean-run] TIC 119584412   period window: 4.85–5.15 d\n",
      "[clean-run] depth floor (fraction): 4.00e-05   (≈ 200.0 ppm × 0.2)\n",
      "[PDCSAP]: {'period': 5.149944880035743, 'sde': 3.866641087303198, 'snr': 4.496647217554053, 'n_transits': 148}\n",
      "[ SAP+flatten ]: {'period': 4.97446045188394, 'sde': 4.9504278791412695, 'snr': 3.618312997433482, 'n_transits': 153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobi.weitzman/miniforge3/envs/tess-ephem/lib/python3.10/site-packages/transitleastsquares/main.py:411: UserWarning: 143 of 153 transits without data. The true period may be twice the given period.\n",
      "  warnings.warn(text)\n"
     ]
    }
   ],
   "source": [
    "# === CLEAN TLS PIPELINE (CBV unit fix + robust-free flatten + quiet) ===\n",
    "\n",
    "import os, warnings, numpy as np\n",
    "from lightkurve import LightCurve, log, search_lightcurvefile\n",
    "from transitleastsquares import transitleastsquares\n",
    "from astropy.units import Unit\n",
    "\n",
    "# -------- QUIET MODE --------\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"                 # suppress tqdm across deps\n",
    "log.setLevel('ERROR')                            # hush lightkurve logs\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=r\"numpy\\.core\\._methods\")\n",
    "\n",
    "# -------- CONFIG --------\n",
    "TARGET_TIC = int(globals().get(\"target_tic\", 119584412))\n",
    "PERIOD_MIN, PERIOD_MAX = 4.85, 5.15\n",
    "N_THREADS = max(1, (os.cpu_count() or 2) - 1)\n",
    "\n",
    "# If you know your injected depth in ppm, define injected_depth_ppm above.\n",
    "depth_ppm = float(globals().get(\"injected_depth_ppm\", 200.0))   # default 200 ppm\n",
    "depth_frac = max(1e-6, 0.2 * depth_ppm * 1e-6)                  # avoid \"No transit were fit\"\n",
    "\n",
    "tls_common = dict(\n",
    "    period_min=PERIOD_MIN,\n",
    "    period_max=PERIOD_MAX,\n",
    "    transit_depth_min=depth_frac,\n",
    "    use_threads=N_THREADS,\n",
    "    show_progress_bar=False,\n",
    ")\n",
    "\n",
    "# -------- HELPERS --------\n",
    "def _first_defined(*names):\n",
    "    g = globals()\n",
    "    for n in names:\n",
    "        if n in g and g[n] is not None:\n",
    "            return g[n]\n",
    "    return None\n",
    "\n",
    "def resolve_or_download_pdcsap_sap():\n",
    "    \"\"\"Return (lc_pdcsap_raw, lc_sap_raw) LightCurve objects.\n",
    "       If missing, download SPOC LCFs for TARGET_TIC and stitch.\"\"\"\n",
    "    g = globals()\n",
    "    pd = g.get(\"lc_pdcsap_raw\", None)\n",
    "    sa = g.get(\"lc_sap_raw\", None)\n",
    "    if isinstance(pd, LightCurve) and isinstance(sa, LightCurve):\n",
    "        return pd, sa\n",
    "\n",
    "    # Try common variable names\n",
    "    pd_guess = _first_defined(\"lc_pdcsap\", \"pdcsap\", \"lc_pd\", \"pd\")\n",
    "    sa_guess = _first_defined(\"lc_sap\", \"sap\", \"lc_sa\", \"sa\")\n",
    "    if isinstance(pd_guess, LightCurve) and isinstance(sa_guess, LightCurve):\n",
    "        return pd_guess, sa_guess\n",
    "\n",
    "    # Download from SPOC LCFs so we can access both PDCSAP and SAP consistently\n",
    "    print(f\"[fetch] Downloading SPOC LCFs for TIC {TARGET_TIC} (TESS)...\")\n",
    "    sr = search_lightcurvefile(f\"TIC {TARGET_TIC}\", mission=\"TESS\", author=\"SPOC\")\n",
    "    lcf_coll = sr.download_all()\n",
    "    if lcf_coll is None or len(lcf_coll) == 0:\n",
    "        raise RuntimeError(f\"No SPOC light-curve files found for TIC {TARGET_TIC}.\")\n",
    "\n",
    "    # Stitch PDCSAP and SAP across all sectors\n",
    "    try:\n",
    "        lc_pdcsap_raw = lcf_coll.PDCSAP_FLUX.stitch()\n",
    "        lc_sap_raw    = lcf_coll.SAP_FLUX.stitch()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Could not build PDCSAP/SAP from LCFs: {e}\")\n",
    "\n",
    "    if not isinstance(lc_pdcsap_raw, LightCurve) or not isinstance(lc_sap_raw, LightCurve):\n",
    "        raise RuntimeError(\"Unexpected objects from stitching; PDCSAP/SAP LightCurve not obtained.\")\n",
    "\n",
    "    return lc_pdcsap_raw, lc_sap_raw\n",
    "\n",
    "def sap_cbv_or_flatten(lc_sap_raw: LightCurve) -> LightCurve:\n",
    "    \"\"\"Prefer SAP+CBV if available & in e-/s units; otherwise fall back to flatten().\n",
    "       IMPORTANT: Do NOT pass a normalized curve here; CBV requires e-/s units.\"\"\"\n",
    "    # Try CBV on RAW (non-normalized) SAP if units are correct\n",
    "    try:\n",
    "        if getattr(lc_sap_raw.flux, \"unit\", None) != Unit(\"electron / second\"):\n",
    "            raise AssertionError(\"SAP flux not in e-/s; skipping CBV.\")\n",
    "        corr = lc_sap_raw.to_corrector(\"cbv\")  # requires LK CBVs\n",
    "        corrected = corr.correct(pca_components=5, sigma=5)\n",
    "        corrected = corrected.remove_nans().normalize()\n",
    "        corrected.meta = {**getattr(corrected, \"meta\", {}), \"pipeline\": \"SAP+CBV\"}\n",
    "        return corrected\n",
    "    except Exception as e:\n",
    "        # Fallback: robust-free flatten for broad compatibility\n",
    "        flat = lc_sap_raw.remove_nans().flatten(window_length=401, polyorder=2, niters=3, sigma=3)\n",
    "        flat = flat.normalize()\n",
    "        flat.meta = {**getattr(flat, \"meta\", {}), \"pipeline\": \"SAP+flatten\", \"cbv_error\": repr(e)}\n",
    "        return flat\n",
    "\n",
    "def run_tls_once(lc: LightCurve, label: str):\n",
    "    t_all = getattr(lc.time, \"value\", lc.time)\n",
    "    f_all = getattr(lc.flux, \"value\", lc.flux)\n",
    "    m = np.isfinite(t_all) & np.isfinite(f_all)\n",
    "    t = np.ascontiguousarray(t_all[m], dtype=float)\n",
    "    f = np.ascontiguousarray(f_all[m], dtype=float)\n",
    "    model = transitleastsquares(t, f)\n",
    "    res = model.power(**tls_common)\n",
    "    res.pipeline_label = label\n",
    "    return res\n",
    "\n",
    "def summarize_tls(res):\n",
    "    return dict(\n",
    "        pipeline=getattr(res, \"pipeline_label\", \"NA\"),\n",
    "        period=res.period,\n",
    "        period_unc=res.period_uncertainty,\n",
    "        t0=res.T0,\n",
    "        duration=res.duration,\n",
    "        depth=res.depth,     # fractional\n",
    "        sde=res.SDE,\n",
    "        snr=res.snr,\n",
    "        odd_even=res.odd_even_mismatch,\n",
    "        n_transits=res.transit_count,\n",
    "    )\n",
    "\n",
    "# -------- PIPELINE --------\n",
    "lc_pdcsap_raw, lc_sap_raw = resolve_or_download_pdcsap_sap()\n",
    "\n",
    "# PDCSAP: clean & normalize\n",
    "lc_pdcsap = lc_pdcsap_raw.remove_nans().normalize()\n",
    "lc_pdcsap.meta = {**getattr(lc_pdcsap, \"meta\", {}), \"pipeline\": \"PDCSAP\"}\n",
    "\n",
    "# SAP branch: run CBV on RAW (e-/s) if possible; otherwise flatten, then normalize\n",
    "lc_sap_cbv = sap_cbv_or_flatten(lc_sap_raw)\n",
    "sap_label = lc_sap_cbv.meta.get(\"pipeline\", \"SAP\")\n",
    "\n",
    "# Run TLS (status bars disabled)\n",
    "pdcsap_res = run_tls_once(lc_pdcsap, \"PDCSAP\")\n",
    "sap_res    = run_tls_once(lc_sap_cbv, sap_label)\n",
    "\n",
    "# Compact summary\n",
    "row_pdcsap = summarize_tls(pdcsap_res)\n",
    "row_sap    = summarize_tls(sap_res)\n",
    "\n",
    "print(f\"[clean-run] TIC {TARGET_TIC}   period window: {PERIOD_MIN}–{PERIOD_MAX} d\")\n",
    "print(f\"[clean-run] depth floor (fraction): {depth_frac:.2e}   (≈ {depth_ppm} ppm × 0.2)\")\n",
    "print(\"[PDCSAP]:\", {k: row_pdcsap[k] for k in (\"period\",\"sde\",\"snr\",\"n_transits\")})\n",
    "print(\"[\", sap_label, \"]:\", {k: row_sap[k] for k in (\"period\",\"sde\",\"snr\",\"n_transits\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "067b64cd-b64d-44db-982a-dbd3818f04a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] PDCSAP:\n",
      "  JSON : results/119584412/119584412_PDCSAP_20250925T010334Z_metrics.json\n",
      "  CSV  : results/119584412/summary.csv\n",
      "  Pgram: results/119584412/119584412_PDCSAP_20250925T010334Z_periodogram.png\n",
      "  Fold : results/119584412/119584412_PDCSAP_20250925T010334Z_phasefold.png\n",
      "[saved] SAP+flatten:\n",
      "  JSON : results/119584412/119584412_SAP+flatten_20250925T010334Z_metrics.json\n",
      "  CSV  : results/119584412/summary.csv\n",
      "  Pgram: results/119584412/119584412_SAP+flatten_20250925T010334Z_periodogram.png\n",
      "  Fold : results/119584412/119584412_SAP+flatten_20250925T010334Z_phasefold.png\n"
     ]
    }
   ],
   "source": [
    "# ==== SAVE TLS RESULTS & FIGURES (robust to TimeDelta/Quantity) ====\n",
    "import os, csv, json, datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- robust converters ----------\n",
    "def _to_float(x, unit=None):\n",
    "    \"\"\"Safely turn astropy Quantity/TimeDelta/np scalars into float.\"\"\"\n",
    "    try:\n",
    "        if hasattr(x, \"to_value\"):\n",
    "            # Try unitless first; if Astropy requires a unit (e.g., TimeDelta), fall back to 'day'\n",
    "            try:\n",
    "                return float(x.to_value(unit)) if unit else float(x.to_value())\n",
    "            except TypeError:\n",
    "                return float(x.to_value(unit or \"day\"))\n",
    "        if hasattr(x, \"value\"):\n",
    "            return float(x.value)\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _to_numpy_1d(x, unit=None):\n",
    "    \"\"\"Safely turn astropy arrays (incl. TimeDelta) into 1D float numpy arrays.\"\"\"\n",
    "    if hasattr(x, \"to_value\"):\n",
    "        try:\n",
    "            x = x.to_value(unit) if unit else x.to_value()\n",
    "        except TypeError:\n",
    "            # Astropy Time/TimeDelta needs a unit/format; default to days\n",
    "            x = x.to_value(unit or \"day\")\n",
    "    elif hasattr(x, \"value\"):\n",
    "        x = x.value\n",
    "    return np.asarray(x, dtype=float).ravel()\n",
    "# ---------- small helpers ----------\n",
    "def _get_target_id():\n",
    "    for k in (\"target_id\", \"tic_id\", \"TIC\", \"target\"):\n",
    "        if k in globals():\n",
    "            return str(globals()[k])\n",
    "    for name in (\"lc_pdcsap\", \"lc_sap_cbv\"):\n",
    "        if name in globals():\n",
    "            lc = globals()[name]\n",
    "            meta = getattr(lc, \"meta\", {}) or {}\n",
    "            for key in (\"TICID\", \"TARGETID\", \"OBJECT\", \"TARGET\"):\n",
    "                if isinstance(meta, dict) and key in meta and meta[key] is not None:\n",
    "                    return str(meta[key])\n",
    "    return \"unknown_target\"\n",
    "\n",
    "def _append_csv(path, row, fieldnames):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    write_header = not os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "# ---------- paths ----------\n",
    "TARGET = _get_target_id()\n",
    "STAMP  = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "OUTDIR = os.path.join(\"results\", TARGET)\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "CSV_PATH = os.path.join(OUTDIR, \"summary.csv\")\n",
    "\n",
    "# ---------- plotting ----------\n",
    "def _save_periodogram(res, label):\n",
    "    periods = getattr(res, \"periods\", None)\n",
    "    power   = getattr(res, \"power\",   None)\n",
    "    if periods is None or power is None:\n",
    "        return None\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.plot(_to_numpy_1d(periods), _to_numpy_1d(power), lw=1.25)\n",
    "    ax.axvline(_to_float(getattr(res, \"period\", np.nan)), ls=\"--\", lw=1.25)\n",
    "    ax.set_xlabel(\"Period [days]\")\n",
    "    ax.set_ylabel(\"TLS power (SDE proxy)\")\n",
    "    ax.set_title(f\"{label} periodogram — best P = {_to_float(res.period):.5f} d\")\n",
    "    fpath = os.path.join(OUTDIR, f\"{TARGET}_{label}_{STAMP}_periodogram.png\")\n",
    "    fig.savefig(fpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return fpath\n",
    "\n",
    "def _save_phase_fold(lc, res, label, nbins=120):\n",
    "    \"\"\"Fold using Lightkurve, handle TimeDelta/Quantity, and bin for clarity.\"\"\"\n",
    "    P = _to_float(getattr(res, \"period\", np.nan))\n",
    "    T0 = _to_float(getattr(res, \"T0\", getattr(res, \"t0\", np.nan)))\n",
    "    folded = lc.fold(period=P, t0=T0)\n",
    "\n",
    "    # Phase array: prefer folded.phase, else derive from folded.time (days)\n",
    "    if hasattr(folded, \"phase\") and folded.phase is not None:\n",
    "        phase = _to_numpy_1d(folded.phase)  # already in cycles [-0.5, 0.5]\n",
    "    else:\n",
    "        tdays = _to_numpy_1d(getattr(folded, \"time\", []), unit=\"day\")\n",
    "        phase = ((tdays / P + 0.5) % 1.0) - 0.5\n",
    "\n",
    "    flux  = _to_numpy_1d(folded.flux)\n",
    "\n",
    "    # Robust y-lims\n",
    "    if np.isfinite(flux).sum() > 10:\n",
    "        p_lo, p_hi = np.nanpercentile(flux, [0.2, 99.8])\n",
    "        pad = 0.002 * max(p_hi - p_lo, 1e-9)\n",
    "        ylo, yhi = p_lo - pad, p_hi + pad\n",
    "    else:\n",
    "        ylo, yhi = np.nanmin(flux), np.nanmax(flux)\n",
    "\n",
    "    # Median-bin\n",
    "    bins = np.linspace(-0.5, 0.5, nbins+1)\n",
    "    which = np.digitize(phase, bins) - 1\n",
    "    bin_c = 0.5*(bins[1:] + bins[:-1])\n",
    "    bin_f = np.array([np.nanmedian(flux[which==i]) if np.any(which==i) else np.nan\n",
    "                      for i in range(nbins)])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.scatter(phase, flux, s=3, alpha=0.25)\n",
    "    ax.plot(bin_c, bin_f, lw=2)\n",
    "    ax.set_xlim(-0.5, 0.5)\n",
    "    if np.isfinite(ylo) and np.isfinite(yhi):\n",
    "        ax.set_ylim(ylo, yhi)\n",
    "    ax.set_xlabel(\"Phase [cycles]\")\n",
    "    ax.set_ylabel(\"Relative flux\")\n",
    "    ax.set_title(f\"{label} phase-folded @ P={P:.5f} d, T0={T0:.5f}\")\n",
    "    fpath = os.path.join(OUTDIR, f\"{TARGET}_{label}_{STAMP}_phasefold.png\")\n",
    "    fig.savefig(fpath, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return fpath\n",
    "\n",
    "# ---------- pack metrics & save everything ----------\n",
    "def _save_all(label, res, lc):\n",
    "    depth = _to_float(getattr(res, \"depth\", np.nan))\n",
    "    metrics = dict(\n",
    "        run_utc=STAMP,\n",
    "        target=TARGET,\n",
    "        label=label,\n",
    "        period=_to_float(getattr(res, \"period\", np.nan)),\n",
    "        sde=_to_float(getattr(res, \"SDE\", getattr(res, \"sde\", np.nan))),\n",
    "        snr=_to_float(getattr(res, \"SNR\", getattr(res, \"snr\", np.nan))),\n",
    "        n_transits=int(getattr(res, \"transit_count\", getattr(res, \"n_transits\", -1))),\n",
    "        depth_frac=depth,\n",
    "        depth_ppm=(depth * 1e6) if np.isfinite(depth) else np.nan,\n",
    "        duration_d=_to_float(getattr(res, \"duration\", getattr(res, \"duration_d\", np.nan)), unit=\"day\"),\n",
    "        t0=_to_float(getattr(res, \"T0\", getattr(res, \"t0\", np.nan))),\n",
    "    )\n",
    "\n",
    "    json_path = os.path.join(OUTDIR, f\"{TARGET}_{label}_{STAMP}_metrics.json\")\n",
    "    with open(json_path, \"w\") as jf:\n",
    "        json.dump(metrics, jf, indent=2)\n",
    "\n",
    "    csv_fields = [\"run_utc\",\"target\",\"label\",\"period\",\"sde\",\"snr\",\"n_transits\",\n",
    "                  \"depth_frac\",\"depth_ppm\",\"duration_d\",\"t0\"]\n",
    "    _append_csv(CSV_PATH, metrics, csv_fields)\n",
    "\n",
    "    pgram = _save_periodogram(res, label)\n",
    "    pfold = _save_phase_fold(lc, res, label)\n",
    "\n",
    "    print(f\"[saved] {label}:\")\n",
    "    print(f\"  JSON : {json_path}\")\n",
    "    print(f\"  CSV  : {CSV_PATH}\")\n",
    "    if pgram: print(f\"  Pgram: {pgram}\")\n",
    "    if pfold: print(f\"  Fold : {pfold}\")\n",
    "\n",
    "# ---------- detect what exists in memory and save ----------\n",
    "found_any = False\n",
    "if \"pdcsap_res\" in globals() and \"lc_pdcsap\" in globals():\n",
    "    _save_all(\"PDCSAP\", pdcsap_res, lc_pdcsap); found_any = True\n",
    "\n",
    "if \"sap_res\" in globals() and \"lc_sap_cbv\" in globals():\n",
    "    plabel = (getattr(getattr(lc_sap_cbv, \"meta\", {}), \"get\", lambda *_: None)(\"pipeline\")\n",
    "              or \"SAP+flatten\")\n",
    "    _save_all(plabel, sap_res, lc_sap_cbv); found_any = True\n",
    "\n",
    "if not found_any:\n",
    "    raise RuntimeError(\"Nothing to save: expected (pdcsap_res, lc_pdcsap) and/or (sap_res, lc_sap_cbv).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ef6ca32-41ec-47fc-ab92-db3e9832ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TIC 119584412 — stitching PDCSAP (sectors: all)\n",
      "[tls] Narrow search around P=16.027187 d (window ±1.0% → [15.86692, 16.18746] d)\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 29374 data points, 401 periods from 15.868 to 16.187 days\n",
      "Using 7 of 8 CPU threads\n",
      "Searching for best T0 for period 15.96573 days\n",
      "[tls] best P=15.965733 d, T0=1908.86451 BTJD, SDE≈5.11 (threads=7)\n",
      "[fit] estimating per-event mid-times and refitting linear ephemeris …\n",
      "[fit] P = 15.96593436 d, T0 = 1908.858885 BTJD, rχ² = 0.07\n",
      "[fit] σ(P) = 4.319040e-04 d, σ(T0) = 1.587372e-02 d, ρ = -0.794\n",
      "\n",
      "[done]\n",
      "  TLS periodogram : figures/TIC119584412/TIC119584412_TLS_narrow_20250927T202323Z.png\n",
      "  Phase-fold plot : figures/TIC119584412/TIC119584412_TLS_narrow_phase_20250927T202323Z.png\n",
      "  Midtimes CSV    : results/TIC119584412/TIC119584412_midtimes_20250927T202323Z.csv\n",
      "  Ephemeris JSON  : results/TIC119584412/TIC119584412_refit_ephemeris_20250927T202323Z.json\n",
      "  O–C plot        : figures/TIC119584412/TIC119584412_OC_20250927T202323Z.png\n"
     ]
    }
   ],
   "source": [
    "# === Target A • Combine sectors • TLS near catalog P • Refit (P, T0) with covariance ===\n",
    "# Minimal-verbosity; saves artifacts under results/TIC{TIC}/ and figures/TIC{TIC}/\n",
    "\n",
    "import os, json, csv, warnings, math, datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.stats import mad_std\n",
    "from lightkurve import search_lightcurvefile, LightCurveCollection\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "\n",
    "# ---------- USER KNOBS ----------\n",
    "TIC          = 119584412                 # Target A\n",
    "SECTORS_HINT = None                      # e.g., [22, 49]; set None to auto-use all available\n",
    "CATALOG_P    = 16.027187                 # \"catalog\" or prior guess period (days)\n",
    "WINDOW_FRAC  = 0.01                      # ±1% search window around the catalog period\n",
    "DUR_HOURS    = 6.5                       # rough duration for masks & display (hours)\n",
    "QUALITY_BITS = 175                       # SPOC quality bitmask (keep good)\n",
    "FLAT_WIN_D   = 1.0                       # detrend window (days), gentle so we don't erase dips\n",
    "POLY_ORDER   = 2\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "STAMP   = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RDIR    = f\"results/TIC{TIC}\"\n",
    "FDIR    = f\"figures/TIC{TIC}\"\n",
    "os.makedirs(RDIR, exist_ok=True)\n",
    "os.makedirs(FDIR, exist_ok=True)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _npify(arr):\n",
    "    \"\"\"Convert Quantity/Masked to plain np.ndarray; masked/invalid -> np.nan.\"\"\"\n",
    "    # Handle astropy masked arrays or numpy masked arrays\n",
    "    if np.ma.isMaskedArray(arr):\n",
    "        return np.asarray(np.ma.filled(arr, np.nan))\n",
    "    # Handle astropy Quantity with .value\n",
    "    if hasattr(arr, \"value\"):\n",
    "        arr = arr.value\n",
    "        if np.ma.isMaskedArray(arr):\n",
    "            return np.asarray(np.ma.filled(arr, np.nan))\n",
    "    return np.asarray(arr)\n",
    "\n",
    "def _gentle_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Savitzky–Golay-like sliding poly via Lightkurve, but gently.\"\"\"\n",
    "    # estimate cadence in days; guard against divide-by-zero\n",
    "    t = _npify(lc.time)\n",
    "    dt_med = float(np.nanmedian(np.diff(t))) if t.size > 1 else 0.001\n",
    "    w = int(round(window_days / (dt_med if dt_med > 0 else 0.001)))\n",
    "    w = max(7, w)\n",
    "    if w % 2 == 0:\n",
    "        w += 1  # Lightkurve prefers odd window length\n",
    "    try:\n",
    "        flat = lc.flatten(window_length=w, polyorder=polyorder, return_trend=False)\n",
    "    except TypeError:\n",
    "        flat = lc.flatten(window_length=w, polyorder=polyorder)\n",
    "    return flat.remove_nans().normalize()\n",
    "\n",
    "def _load_stitched_pdcsap(tic, sectors_hint=None, quality_bits=175):\n",
    "    sr = search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "    if len(sr) == 0:\n",
    "        raise RuntimeError(\"No SPOC LightCurveFiles found.\")\n",
    "    files = sr.download_all()\n",
    "\n",
    "    lcs = []\n",
    "    for f in files:\n",
    "        # optional sector filter\n",
    "        if sectors_hint is not None:\n",
    "            sec = getattr(f, \"sector\", None)\n",
    "            if (sec is None) or (sec not in sectors_hint):\n",
    "                continue\n",
    "\n",
    "        # pull PDCSAP\n",
    "        lc = f.PDCSAP_FLUX\n",
    "\n",
    "        # cleanup (be version-tolerant)\n",
    "        if hasattr(lc, \"remove_nans\"):\n",
    "            lc = lc.remove_nans()\n",
    "        if hasattr(lc, \"remove_outliers\"):\n",
    "            try:\n",
    "                lc = lc.remove_outliers(sigma=10)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # quality bitmask\n",
    "        if hasattr(lc, \"quality\"):\n",
    "            good = (lc.quality & quality_bits) == 0\n",
    "            lc = lc[good]\n",
    "\n",
    "        if len(lc.time) == 0:\n",
    "            continue\n",
    "\n",
    "        lcs.append(lc.normalize())\n",
    "\n",
    "    if not lcs:\n",
    "        raise RuntimeError(\"No usable PDCSAP cadences after quality mask.\")\n",
    "    return LightCurveCollection(lcs).stitch().remove_nans().normalize()\n",
    "\n",
    "def _median_bin(x, y, nbins=200):\n",
    "    x = _npify(x); y = _npify(y)\n",
    "    good = np.isfinite(x) & np.isfinite(y)\n",
    "    x, y = x[good], y[good]\n",
    "    edges = np.linspace(np.nanmin(x), np.nanmax(x), nbins+1)\n",
    "    idx   = np.digitize(x, edges)-1\n",
    "    bx, by = [], []\n",
    "    for i in range(nbins):\n",
    "        sel = idx == i\n",
    "        if np.any(sel):\n",
    "            bx.append(np.nanmedian(x[sel]))\n",
    "            by.append(np.nanmedian(y[sel]))\n",
    "    return np.asarray(bx), np.asarray(by)\n",
    "\n",
    "def _find_midtimes_quadratic(time_btjd, flux_norm, period, t0, dur_hours, k_half=1.0):\n",
    "    \"\"\"\n",
    "    Per-event Tmid finder:\n",
    "      - select windows around predicted midtimes (±k_half * duration/2)\n",
    "      - detrend locally (linear)\n",
    "      - fit quadratic to lowest ~30% of points -> vertex time = Tmid\n",
    "      - estimate per-event sigma_Tmid from bootstrap-of-residuals\n",
    "    Returns: epochs[], tmids[], tmid_errs[]\n",
    "    \"\"\"\n",
    "    time_btjd = _npify(time_btjd)\n",
    "    flux_norm = _npify(flux_norm)\n",
    "    good0 = np.isfinite(time_btjd) & np.isfinite(flux_norm)\n",
    "    time_btjd, flux_norm = time_btjd[good0], flux_norm[good0]\n",
    "\n",
    "    dur_days = dur_hours/24.0\n",
    "    # epochs that fall within data span\n",
    "    kmin = math.floor((time_btjd.min() - t0)/period) - 1\n",
    "    kmax = math.ceil((time_btjd.max() - t0)/period) + 1\n",
    "    epochs, tmids, terrs = [], [], []\n",
    "\n",
    "    for k in range(kmin, kmax+1):\n",
    "        tc = t0 + k*period\n",
    "        w  = (np.abs(time_btjd - tc) <= 0.5*k_half*dur_days)\n",
    "        if np.count_nonzero(w) < 8:\n",
    "            continue\n",
    "        t  = time_btjd[w]\n",
    "        f  = flux_norm[w]\n",
    "\n",
    "        # remove any remaining NaNs/masked\n",
    "        good = np.isfinite(t) & np.isfinite(f)\n",
    "        t, f = t[good], f[good]\n",
    "        if t.size < 8:\n",
    "            continue\n",
    "\n",
    "        # local linear detrend\n",
    "        A = np.vstack([np.ones_like(t), t - np.nanmedian(t)]).T\n",
    "        try:\n",
    "            coeff, *_ = np.linalg.lstsq(A, f, rcond=None)\n",
    "        except Exception:\n",
    "            continue\n",
    "        f_det = f - (A @ coeff)\n",
    "\n",
    "        # take lowest ~30% points for quadratic\n",
    "        q = np.nanpercentile(f_det, 30)\n",
    "        sel = f_det <= q\n",
    "        if np.count_nonzero(sel) < 6:\n",
    "            continue\n",
    "        tt = t[sel] - np.nanmedian(t[sel])\n",
    "        ff = f_det[sel]\n",
    "\n",
    "        # quadratic fit: a*tt^2 + b*tt + c\n",
    "        X = np.vstack([tt**2, tt, np.ones_like(tt)]).T\n",
    "        try:\n",
    "            p, *_ = np.linalg.lstsq(X, ff, rcond=None)\n",
    "        except Exception:\n",
    "            continue\n",
    "        a, b, c = p\n",
    "        if a == 0:\n",
    "            continue\n",
    "        tmid = -b/(2*a) + np.nanmedian(t[sel])\n",
    "\n",
    "        # rough uncertainty via bootstrap of residuals\n",
    "        model = a*tt**2 + b*tt + c\n",
    "        res   = ff - model\n",
    "        if len(res) < 6:\n",
    "            continue\n",
    "        draws = []\n",
    "        rng = np.random.default_rng(42)\n",
    "        for _ in range(200):\n",
    "            res_s = rng.choice(res, size=res.size, replace=True)\n",
    "            ff_s  = model + res_s\n",
    "            try:\n",
    "                p_s, *_ = np.linalg.lstsq(X, ff_s, rcond=None)\n",
    "            except Exception:\n",
    "                continue\n",
    "            a_s, b_s, c_s = p_s\n",
    "            if a_s == 0:\n",
    "                continue\n",
    "            draws.append(-b_s/(2*a_s) + np.nanmedian(t[sel]))\n",
    "        if len(draws) < 10:\n",
    "            continue\n",
    "        epochs.append(k)\n",
    "        tmids.append(tmid)\n",
    "        terrs.append(np.nanstd(draws, ddof=1))\n",
    "    return np.asarray(epochs), np.asarray(tmids), np.asarray(terrs)\n",
    "\n",
    "def _fit_linear_ephemeris(epochs, tmids, terrs):\n",
    "    \"\"\"Weighted least squares fit of T(n)=T0+nP; returns P, T0, Cov 2x2, redchisq.\"\"\"\n",
    "    if len(tmids) < 2:\n",
    "        raise RuntimeError(\"Not enough mid-times to refit ephemeris.\")\n",
    "    w   = 1.0/np.clip(terrs, 1e-6, np.inf)**2\n",
    "    X   = np.vstack([epochs, np.ones_like(epochs)]).T  # slope=P, intercept=T0\n",
    "    XT  = (X.T*w)\n",
    "    beta= np.linalg.inv(XT@X) @ (XT@tmids)\n",
    "    P, T0 = beta[0], beta[1]\n",
    "    yhat  = X@beta\n",
    "    resid = tmids - yhat\n",
    "    dof   = max(1, len(tmids)-2)\n",
    "    chi2  = np.sum((resid**2)*w)\n",
    "    rchi2 = chi2/dof\n",
    "    Cov   = np.linalg.inv(XT@X) * rchi2  # inflate by reduced chi^2\n",
    "    return P, T0, Cov, rchi2\n",
    "\n",
    "def _save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# ---------- 1) LOAD & FLATTEN (gentle) ----------\n",
    "print(f\"[load] TIC {TIC} — stitching PDCSAP (sectors: {'all' if not SECTORS_HINT else SECTORS_HINT})\")\n",
    "lc = _load_stitched_pdcsap(TIC, sectors_hint=SECTORS_HINT, quality_bits=QUALITY_BITS)\n",
    "lc_flat = _gentle_flatten(lc, window_days=FLAT_WIN_D, polyorder=POLY_ORDER)\n",
    "\n",
    "# ---------- 2) TLS in a narrow window around catalog P ----------\n",
    "pmin = CATALOG_P * (1.0 - WINDOW_FRAC)\n",
    "pmax = CATALOG_P * (1.0 + WINDOW_FRAC)\n",
    "\n",
    "time = _npify(lc_flat.time)      # BTJD (days) → plain numpy\n",
    "flux = _npify(lc_flat.flux)      # normalized → plain numpy\n",
    "good = np.isfinite(time) & np.isfinite(flux)\n",
    "time, flux = time[good], flux[good]\n",
    "\n",
    "print(f\"[tls] Narrow search around P={CATALOG_P:.6f} d (window ±{100*WINDOW_FRAC:.1f}% → [{pmin:.5f}, {pmax:.5f}] d)\")\n",
    "\n",
    "tls = transitleastsquares(time, flux)\n",
    "\n",
    "# TLS requires an integer >= 1 for use_threads (some builds don't accept \"auto\")\n",
    "threads = max(1, (os.cpu_count() or 1) - 1)\n",
    "\n",
    "# Run TLS with version tolerance for show_progress_bar\n",
    "try:\n",
    "    tls_res = tls.power(\n",
    "        period_min=pmin,\n",
    "        period_max=pmax,\n",
    "        oversampling_factor=5,\n",
    "        n_transits_min=2,\n",
    "        use_threads=threads,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "except TypeError:\n",
    "    tls_res = tls.power(\n",
    "        period_min=pmin,\n",
    "        period_max=pmax,\n",
    "        oversampling_factor=5,\n",
    "        n_transits_min=2,\n",
    "        use_threads=threads,\n",
    "    )\n",
    "\n",
    "bestP  = float(tls_res.period)\n",
    "bestT0 = float(tls_res.T0)\n",
    "bestSDE = float(getattr(tls_res, \"SDE\", getattr(tls_res, \"sde\", np.nan)))\n",
    "print(f\"[tls] best P={bestP:.6f} d, T0={bestT0:.5f} BTJD, SDE≈{bestSDE:.2f} (threads={threads})\")\n",
    "\n",
    "# Save TLS periodogram\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.plot(tls_res.periods, tls_res.power, lw=1)\n",
    "plt.axvline(bestP, ls=\"--\")\n",
    "plt.xlabel(\"Period [days]\"); plt.ylabel(\"TLS power (SDE proxy)\")\n",
    "plt.title(f\"TIC {TIC} — TLS around {CATALOG_P:.5f} d (best {bestP:.5f} d)\")\n",
    "plt.tight_layout()\n",
    "pgram_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_{STAMP}.png\")\n",
    "fig.savefig(pgram_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# Save a phase-folded figure at TLS best\n",
    "folded = lc_flat.fold(period=bestP, epoch_time=bestT0)\n",
    "phase  = _npify(getattr(folded, \"phase\", None))\n",
    "flux_f = _npify(getattr(folded, \"flux\", None))\n",
    "goodf  = np.isfinite(phase) & np.isfinite(flux_f)\n",
    "phase, flux_f = phase[goodf], flux_f[goodf]\n",
    "px, py = _median_bin(phase, flux_f, nbins=180)\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.scatter(phase, flux_f, s=4, alpha=0.15)\n",
    "plt.plot(px, py, lw=1.8)\n",
    "dur_phase = (DUR_HOURS/24.0)/bestP\n",
    "plt.axvspan(-0.5*dur_phase, 0.5*dur_phase, alpha=0.15)\n",
    "plt.xlabel(\"Phase [cycles]\"); plt.ylabel(\"Relative flux\")\n",
    "plt.title(f\"TIC {TIC} — phase-fold @ P={bestP:.5f} d, T0={bestT0:.5f}\")\n",
    "plt.tight_layout()\n",
    "fold_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_phase_{STAMP}.png\")\n",
    "fig.savefig(fold_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# ---------- 3) Refit P, T0 from per-event midtimes with full covariance ----------\n",
    "print(\"[fit] estimating per-event mid-times and refitting linear ephemeris …\")\n",
    "epochs, tmids, terrs = _find_midtimes_quadratic(time, flux, bestP, bestT0, DUR_HOURS, k_half=1.7)\n",
    "if len(tmids) < 2:\n",
    "    raise RuntimeError(f\"Only {len(tmids)} mid-times found; need ≥2 to fit ephemeris.\")\n",
    "\n",
    "P_fit, T0_fit, Cov, rchi2 = _fit_linear_ephemeris(epochs, tmids, terrs)\n",
    "cov_dict = {\"C00_T0\": Cov[1,1], \"C11_P\": Cov[0,0], \"C01_T0P\": Cov[1,0]}\n",
    "\n",
    "print(f\"[fit] P = {P_fit:.8f} d, T0 = {T0_fit:.6f} BTJD, rχ² = {rchi2:.2f}\")\n",
    "print(f\"[fit] σ(P) = {math.sqrt(Cov[0,0]):.6e} d, σ(T0) = {math.sqrt(Cov[1,1]):.6e} d, ρ = {Cov[0,1]/math.sqrt(Cov[0,0]*Cov[1,1]):.3f}\")\n",
    "\n",
    "# Save midtimes CSV\n",
    "mt_csv = os.path.join(RDIR, f\"TIC{TIC}_midtimes_{STAMP}.csv\")\n",
    "with open(mt_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"tmid_btjd\", \"tmid_err_d\"])\n",
    "    for n, tmid, terr in zip(epochs, tmids, terrs):\n",
    "        w.writerow([int(n), f\"{tmid:.8f}\", f\"{terr:.8e}\"])\n",
    "\n",
    "# ---------- Save ephemeris JSON ----------\n",
    "ephem_json = dict(\n",
    "    tic=TIC,\n",
    "    tls_narrow=dict(\n",
    "        catalog_P=CATALOG_P,\n",
    "        search_window_frac=WINDOW_FRAC,\n",
    "        best_P=bestP,\n",
    "        best_T0=bestT0,\n",
    "        SDE=bestSDE,\n",
    "        periodogram=pgram_path,\n",
    "        phase_plot=fold_path,\n",
    "    ),\n",
    "    fit=dict(\n",
    "        P=P_fit,\n",
    "        T0=T0_fit,\n",
    "        rchisq=rchi2,\n",
    "        cov=cov_dict,\n",
    "        n_midtimes=int(len(tmids)),\n",
    "    ),\n",
    "    detrend=dict(kind=\"sliding_poly\", window_days=FLAT_WIN_D, polyorder=POLY_ORDER),\n",
    "    quality_bits=QUALITY_BITS,\n",
    "    run_utc=STAMP,\n",
    ")\n",
    "ephem_path = os.path.join(RDIR, f\"TIC{TIC}_refit_ephemeris_{STAMP}.json\")\n",
    "_save_json(ephem_path, ephem_json)\n",
    "\n",
    "# ---------- Simple O–C plot ----------\n",
    "oc = tmids - (T0_fit + epochs * P_fit)\n",
    "fig = plt.figure(figsize=(8.0, 4.8))\n",
    "plt.errorbar(epochs, oc * 24 * 60, yerr=terrs * 24 * 60, fmt=\"o\", ms=4, capsize=2)\n",
    "plt.axhline(0, color=\"k\", lw=1, ls=\"--\")\n",
    "plt.xlabel(\"Epoch (n)\"); plt.ylabel(\"O–C [minutes]\")\n",
    "plt.title(f\"TIC {TIC} — O–C after refit (P={P_fit:.6f} d)\")\n",
    "plt.tight_layout()\n",
    "oc_path = os.path.join(FDIR, f\"TIC{TIC}_OC_{STAMP}.png\")\n",
    "fig.savefig(oc_path, dpi=150); plt.close(fig)\n",
    "\n",
    "print(\"\\n[done]\")\n",
    "print(f\"  TLS periodogram : {pgram_path}\")\n",
    "print(f\"  Phase-fold plot : {fold_path}\")\n",
    "print(f\"  Midtimes CSV    : {mt_csv}\")\n",
    "print(f\"  Ephemeris JSON  : {ephem_path}\")\n",
    "print(f\"  O–C plot        : {oc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea05b1e7-4ae7-4fb0-8212-76127790910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TIC 119584412 — stitching PDCSAP (sectors: all)\n",
      "[tls] Narrow search around P=16.027187 d (±1.0%)\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 29374 data points, 401 periods from 15.868 to 16.187 days\n",
      "Using 7 of 8 CPU threads\n",
      "Searching for best T0 for period 15.96573 days\n",
      "[tls] best P=15.96573 d, T0=1908.86451 BTJD, SDE≈5.09 (threads=7)\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 28438 data points, 401 periods from 15.868 to 16.187 days\n",
      "Using 7 of 8 CPU threads\n",
      "Searching for best T0 for period 15.92682 days\n",
      "[tls] (protected) best P=15.92682 d, T0=1900.05682 BTJD, SDE≈5.04\n",
      "[fit] estimating per-event mid-times and refitting linear ephemeris …\n",
      "[fit] P = 15.92584226 d, T0 = 1900.077494 BTJD, rχ² = 0.00\n",
      "[fit] σ(P) = 1.948167e-02 d, σ(T0) = 1.972095e-01 d, ρ = -0.215\n",
      "\n",
      "[done]\n",
      "  TLS periodogram : figures/TIC119584412/TIC119584412_TLS_narrow_20250927T205655Z.png\n",
      "  Phase-fold plot : figures/TIC119584412/TIC119584412_TLS_narrow_phase_20250927T205655Z.png\n",
      "  Midtimes CSV    : results/TIC119584412/TIC119584412_midtimes_20250927T205655Z.csv\n",
      "  Ephemeris JSON  : results/TIC119584412/TIC119584412_refit_ephemeris_20250927T205655Z.json\n",
      "  O–C plot        : figures/TIC119584412/TIC119584412_OC_20250927T205655Z.png\n"
     ]
    }
   ],
   "source": [
    "# === Target A • Combine sectors • TLS near catalog P • Refit (P, T0) with covariance ===\n",
    "# Minimal-verbosity; saves artifacts under results/TIC{TIC}/ and figures/TIC{TIC}/\n",
    "\n",
    "import os, json, csv, warnings, math, datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lightkurve import search_lightcurvefile, LightCurveCollection\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "\n",
    "# ---------- USER KNOBS ----------\n",
    "TIC             = 119584412                # Target A\n",
    "SECTORS_HINT    = None                     # e.g., [22, 49]; set None to auto-use all available\n",
    "CATALOG_P       = 16.027187                # prior/catalog period (days)\n",
    "WINDOW_FRAC     = 0.01                     # ±1% TLS window around CATALOG_P\n",
    "DUR_HOURS       = 6.5                      # rough duration (hours)\n",
    "QUALITY_BITS    = 175                      # SPOC quality bitmask (keep good)\n",
    "FLAT_WIN_D      = 1.0                      # detrend window (days)\n",
    "POLY_ORDER      = 2\n",
    "PROTECT_IN_TRANSIT = True                  # re-flatten excluding in-transit points around TLS best\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "STAMP = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RDIR  = f\"results/TIC{TIC}\"\n",
    "FDIR  = f\"figures/TIC{TIC}\"\n",
    "os.makedirs(RDIR, exist_ok=True)\n",
    "os.makedirs(FDIR, exist_ok=True)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _tls_threads():\n",
    "    \"\"\"Use N-1 cores, but at least 1.\"\"\"\n",
    "    n = (os.cpu_count() or 1) - 1\n",
    "    return max(1, n)\n",
    "\n",
    "def _gentle_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    \"\"\"Lightkurve flatten with conservative window; avoids shaving ~hour dips.\"\"\"\n",
    "    # Estimate window length in cadences (min 7 to satisfy LK)\n",
    "    dt_med = np.nanmedian(np.diff(lc.time.value)) or 1e-3\n",
    "    wl = int(max(7, round(window_days / dt_med)))\n",
    "    try:\n",
    "        flat = lc.flatten(window_length=wl, polyorder=polyorder, return_trend=False)\n",
    "    except TypeError:\n",
    "        flat = lc.flatten(window_length=wl, polyorder=polyorder)\n",
    "    return flat.remove_nans().normalize()\n",
    "\n",
    "def _load_stitched_pdcsap(tic, sectors_hint=None, quality_bits=175):\n",
    "    sr = search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "    if len(sr) == 0:\n",
    "        raise RuntimeError(\"No SPOC LightCurveFiles found.\")\n",
    "    files = sr.download_all()\n",
    "\n",
    "    lcs = []\n",
    "    for f in files:\n",
    "        # optional sector filter\n",
    "        if sectors_hint is not None:\n",
    "            sec = getattr(f, \"sector\", None)\n",
    "            if (sec is None) or (sec not in sectors_hint):\n",
    "                continue\n",
    "\n",
    "        lc = f.PDCSAP_FLUX\n",
    "        # cleanup (version tolerant)\n",
    "        if hasattr(lc, \"remove_nans\"):\n",
    "            lc = lc.remove_nans()\n",
    "        if hasattr(lc, \"remove_outliers\"):\n",
    "            try:\n",
    "                lc = lc.remove_outliers(sigma=10)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # quality bitmask\n",
    "        if hasattr(lc, \"quality\"):\n",
    "            good = (lc.quality & quality_bits) == 0\n",
    "            lc = lc[good]\n",
    "        if len(lc.time) == 0:\n",
    "            continue\n",
    "        lcs.append(lc.normalize())\n",
    "\n",
    "    if not lcs:\n",
    "        raise RuntimeError(\"No usable PDCSAP cadences after quality mask.\")\n",
    "    return LightCurveCollection(lcs).stitch().remove_nans().normalize()\n",
    "\n",
    "def _median_bin(x, y, nbins=200):\n",
    "    edges = np.linspace(np.nanmin(x), np.nanmax(x), nbins+1)\n",
    "    idx   = np.digitize(x, edges) - 1\n",
    "    outx, outy = [], []\n",
    "    for i in range(nbins):\n",
    "        sel = idx == i\n",
    "        if np.any(sel):\n",
    "            outx.append(np.nanmedian(x[sel]))\n",
    "            outy.append(np.nanmedian(y[sel]))\n",
    "    return np.asarray(outx), np.asarray(outy)\n",
    "\n",
    "def _find_midtimes_quadratic(time_btjd, flux_norm, period, t0, dur_hours, k_half=1.0):\n",
    "    \"\"\"\n",
    "    Per-event Tmid finder:\n",
    "      - window around predicted T0+kP (±k_half * duration/2)\n",
    "      - local linear detrend\n",
    "      - quadratic fit to lowest ~30% points -> vertex = Tmid\n",
    "      - bootstrap residuals for σ_Tmid\n",
    "    \"\"\"\n",
    "    dur_days = dur_hours/24.0\n",
    "    kmin = math.floor((time_btjd.min() - t0)/period) - 1\n",
    "    kmax = math.ceil((time_btjd.max() - t0)/period) + 1\n",
    "    epochs, tmids, terrs = [], [], []\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    tb = np.asarray(time_btjd, float)\n",
    "    fb = np.asarray(flux_norm, float)\n",
    "\n",
    "    for k in range(kmin, kmax+1):\n",
    "        tc = t0 + k*period\n",
    "        w  = (np.abs(tb - tc) <= 0.5*k_half*dur_days)\n",
    "        if np.count_nonzero(w) < 8:\n",
    "            continue\n",
    "        t = tb[w].astype(float)\n",
    "        f = fb[w].astype(float)\n",
    "\n",
    "        # local linear detrend: f ~ a + b*(t - median)\n",
    "        A = np.vstack([np.ones_like(t), t - np.nanmedian(t)]).T\n",
    "        coeff, *_ = np.linalg.lstsq(A, f, rcond=None)\n",
    "        f_det = f - (A @ coeff)\n",
    "\n",
    "        # take lowest ~30% points\n",
    "        q = np.nanpercentile(f_det, 30.0)\n",
    "        sel = f_det <= q\n",
    "        if np.count_nonzero(sel) < 6:\n",
    "            continue\n",
    "        tt = t[sel] - np.nanmedian(t[sel])\n",
    "        ff = f_det[sel]\n",
    "\n",
    "        # quadratic fit: a*tt^2 + b*tt + c\n",
    "        X = np.vstack([tt**2, tt, np.ones_like(tt)]).T\n",
    "        p, *_ = np.linalg.lstsq(X, ff, rcond=None)\n",
    "        a, b, c = p\n",
    "        if a == 0:\n",
    "            continue\n",
    "        tmid = -b/(2*a) + np.nanmedian(t[sel])\n",
    "\n",
    "        # bootstrap uncertainty\n",
    "        model = a*tt**2 + b*tt + c\n",
    "        res   = ff - model\n",
    "        if len(res) < 6:\n",
    "            continue\n",
    "        draws = []\n",
    "        for _ in range(200):\n",
    "            res_s = rng.choice(res, size=res.size, replace=True)\n",
    "            ff_s  = model + res_s\n",
    "            p_s, *_ = np.linalg.lstsq(X, ff_s, rcond=None)\n",
    "            a_s, b_s, c_s = p_s\n",
    "            if a_s == 0:\n",
    "                continue\n",
    "            draws.append(-b_s/(2*a_s) + np.nanmedian(t[sel]))\n",
    "        if len(draws) < 10:\n",
    "            continue\n",
    "\n",
    "        epochs.append(k)\n",
    "        tmids.append(tmid)\n",
    "        terrs.append(np.nanstd(draws, ddof=1))\n",
    "\n",
    "    return np.asarray(epochs), np.asarray(tmids), np.asarray(terrs)\n",
    "\n",
    "def _fit_linear_ephemeris(epochs, tmids, terrs):\n",
    "    \"\"\"Weighted least squares fit of T(n)=T0+nP; returns P, T0, Cov 2x2, rchi2.\"\"\"\n",
    "    if len(tmids) < 2:\n",
    "        raise RuntimeError(\"Not enough mid-times to refit ephemeris.\")\n",
    "    w   = 1.0/np.clip(terrs, 1e-6, np.inf)**2\n",
    "    X   = np.vstack([epochs, np.ones_like(epochs)]).T  # slope=P, intercept=T0\n",
    "    XT  = (X.T * w)\n",
    "    beta= np.linalg.inv(XT @ X) @ (XT @ tmids)\n",
    "    P, T0 = beta[0], beta[1]\n",
    "    yhat  = X @ beta\n",
    "    resid = tmids - yhat\n",
    "    dof   = max(1, len(tmids)-2)\n",
    "    chi2  = np.sum((resid**2) * w)\n",
    "    rchi2 = chi2 / dof\n",
    "\n",
    "    # Only inflate when rchi2 > 1 (do not shrink when rchi2 < 1)\n",
    "    Cov = np.linalg.inv(XT @ X)\n",
    "    if rchi2 > 1:\n",
    "        Cov = Cov * rchi2\n",
    "    return P, T0, Cov, rchi2\n",
    "\n",
    "def _save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# ---------- 1) LOAD ----------\n",
    "print(f\"[load] TIC {TIC} — stitching PDCSAP (sectors: {'all' if not SECTORS_HINT else SECTORS_HINT})\")\n",
    "lc = _load_stitched_pdcsap(TIC, sectors_hint=SECTORS_HINT, quality_bits=QUALITY_BITS)\n",
    "\n",
    "# ---------- 2) FIRST FLATTEN & TLS (narrow window) ----------\n",
    "lc_flat = _gentle_flatten(lc, window_days=FLAT_WIN_D, polyorder=POLY_ORDER)\n",
    "pmin = CATALOG_P * (1.0 - WINDOW_FRAC)\n",
    "pmax = CATALOG_P * (1.0 + WINDOW_FRAC)\n",
    "time = lc_flat.time.value\n",
    "flux = lc_flat.flux.value\n",
    "print(f\"[tls] Narrow search around P={CATALOG_P:.6f} d (±{100*WINDOW_FRAC:.1f}%)\")\n",
    "\n",
    "tls = transitleastsquares(time, flux)\n",
    "tls_res = tls.power(\n",
    "    period_min=pmin,\n",
    "    period_max=pmax,\n",
    "    oversampling_factor=5,\n",
    "    use_threads=_tls_threads(),\n",
    "    show_progress_bar=False\n",
    ")\n",
    "bestP  = float(tls_res.period)\n",
    "bestT0 = float(tls_res.T0)\n",
    "bestSDE= float(getattr(tls_res, \"SDE\", getattr(tls_res, \"sde\", np.nan)))\n",
    "print(f\"[tls] best P={bestP:.5f} d, T0={bestT0:.5f} BTJD, SDE≈{bestSDE:.2f} (threads={_tls_threads()})\")\n",
    "\n",
    "# ---------- Optional: re-flatten protecting in-transit windows & re-run TLS ----------\n",
    "if PROTECT_IN_TRANSIT:\n",
    "    dur_days = DUR_HOURS/24.0\n",
    "    t_all = lc.time.value\n",
    "    mask_in = np.zeros_like(t_all, dtype=bool)\n",
    "    kmin = int(np.floor((t_all.min()-bestT0)/bestP))-1\n",
    "    kmax = int(np.ceil ((t_all.max()-bestT0)/bestP))+1\n",
    "    for k in range(kmin, kmax+1):\n",
    "        tc = bestT0 + k*bestP\n",
    "        mask_in |= np.abs(t_all - tc) < 0.6*dur_days  # protect ~60% of est. duration\n",
    "    lc_flat = _gentle_flatten(lc[~mask_in], window_days=FLAT_WIN_D, polyorder=POLY_ORDER)\n",
    "\n",
    "    # update arrays and re-run TLS in same narrow window\n",
    "    time = lc_flat.time.value\n",
    "    flux = lc_flat.flux.value\n",
    "    tls   = transitleastsquares(time, flux)\n",
    "    tls_res = tls.power(\n",
    "        period_min=pmin, period_max=pmax,\n",
    "        oversampling_factor=5,\n",
    "        use_threads=_tls_threads(),\n",
    "        show_progress_bar=False\n",
    "    )\n",
    "    bestP  = float(tls_res.period)\n",
    "    bestT0 = float(tls_res.T0)\n",
    "    bestSDE= float(getattr(tls_res, \"SDE\", getattr(tls_res, \"sde\", np.nan)))\n",
    "    print(f\"[tls] (protected) best P={bestP:.5f} d, T0={bestT0:.5f} BTJD, SDE≈{bestSDE:.2f}\")\n",
    "\n",
    "# ---------- Save TLS periodogram ----------\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.plot(tls_res.periods, tls_res.power, lw=1)\n",
    "plt.axvline(bestP, ls=\"--\")\n",
    "plt.xlabel(\"Period [days]\")\n",
    "plt.ylabel(\"TLS power (SDE proxy)\")\n",
    "plt.title(f\"TIC {TIC} — TLS around {CATALOG_P:.5f} d (best {bestP:.5f} d)\")\n",
    "plt.tight_layout()\n",
    "pgram_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_{STAMP}.png\")\n",
    "fig.savefig(pgram_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# ---------- Phase-fold plot (wrapped to one cycle) ----------\n",
    "folded = lc_flat.fold(period=bestP, epoch_time=bestT0)\n",
    "phase = folded.phase.value if hasattr(folded.phase, \"value\") else folded.phase\n",
    "phase = ((np.asarray(phase) + 0.5) % 1.0) - 0.5  # wrap to [-0.5, 0.5)\n",
    "px, py = _median_bin(phase, folded.flux.value, nbins=180)\n",
    "\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.scatter(phase, folded.flux.value, s=4, alpha=0.15)\n",
    "plt.plot(px, py, lw=1.8)\n",
    "dur_phase = (DUR_HOURS/24.0)/bestP\n",
    "plt.axvspan(-0.5*dur_phase, 0.5*dur_phase, alpha=0.15)\n",
    "plt.axvline(0.0, color=\"k\", lw=0.8, alpha=0.4)\n",
    "plt.xlabel(\"Phase [cycles]\"); plt.ylabel(\"Relative flux\")\n",
    "plt.title(f\"TIC {TIC} — phase-fold @ P={bestP:.5f} d, T0={bestT0:.5f}\")\n",
    "plt.tight_layout()\n",
    "fold_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_phase_{STAMP}.png\")\n",
    "fig.savefig(fold_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# ---------- 3) Per-event midtimes & linear ephemeris fit ----------\n",
    "print(\"[fit] estimating per-event mid-times and refitting linear ephemeris …\")\n",
    "epochs, tmids, terrs = _find_midtimes_quadratic(time, flux, bestP, bestT0, DUR_HOURS, k_half=1.7)\n",
    "if len(tmids) < 2:\n",
    "    raise RuntimeError(f\"Only {len(tmids)} mid-times found; need ≥2 to fit ephemeris.\")\n",
    "\n",
    "P_fit, T0_fit, Cov, rchi2 = _fit_linear_ephemeris(epochs, tmids, terrs)\n",
    "cov_dict = {\"C00_T0\": float(Cov[1,1]), \"C11_P\": float(Cov[0,0]), \"C01_T0P\": float(Cov[1,0])}\n",
    "\n",
    "print(f\"[fit] P = {P_fit:.8f} d, T0 = {T0_fit:.6f} BTJD, rχ² = {rchi2:.2f}\")\n",
    "print(f\"[fit] σ(P) = {math.sqrt(Cov[0,0]):.6e} d, σ(T0) = {math.sqrt(Cov[1,1]):.6e} d, ρ = {Cov[0,1]/math.sqrt(Cov[0,0]*Cov[1,1]):.3f}\")\n",
    "\n",
    "# ---------- Save midtimes CSV ----------\n",
    "mt_csv = os.path.join(RDIR, f\"TIC{TIC}_midtimes_{STAMP}.csv\")\n",
    "with open(mt_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"tmid_btjd\", \"tmid_err_d\"])\n",
    "    for n, tmid, terr in zip(epochs, tmids, terrs):\n",
    "        w.writerow([int(n), f\"{tmid:.8f}\", f\"{terr:.8e}\"])\n",
    "\n",
    "# ---------- Save ephemeris JSON ----------\n",
    "ephem_json = dict(\n",
    "    tic=TIC,\n",
    "    tls_narrow=dict(\n",
    "        catalog_P=CATALOG_P,\n",
    "        search_window_frac=WINDOW_FRAC,\n",
    "        best_P=bestP,\n",
    "        best_T0=bestT0,\n",
    "        SDE=bestSDE,\n",
    "        periodogram=pgram_path,\n",
    "        phase_plot=fold_path,\n",
    "        protected_in_transit=bool(PROTECT_IN_TRANSIT),\n",
    "    ),\n",
    "    fit=dict(\n",
    "        P=P_fit, T0=T0_fit, rchisq=rchi2, cov=cov_dict,\n",
    "        n_midtimes=int(len(tmids)),\n",
    "    ),\n",
    "    detrend=dict(kind=\"sliding_poly\", window_days=FLAT_WIN_D, polyorder=POLY_ORDER),\n",
    "    quality_bits=QUALITY_BITS,\n",
    "    run_utc=STAMP,\n",
    ")\n",
    "ephem_path = os.path.join(RDIR, f\"TIC{TIC}_refit_ephemeris_{STAMP}.json\")\n",
    "_save_json(ephem_path, ephem_json)\n",
    "\n",
    "# ---------- Simple O–C plot ----------\n",
    "oc = tmids - (T0_fit + epochs * P_fit)\n",
    "fig = plt.figure(figsize=(8.0, 4.8))\n",
    "plt.errorbar(epochs, oc * 24 * 60, yerr=terrs * 24 * 60, fmt=\"o\", ms=4, capsize=2)\n",
    "plt.axhline(0, color=\"k\", lw=1, ls=\"--\")\n",
    "plt.xlabel(\"Epoch (n)\"); plt.ylabel(\"O–C [minutes]\")\n",
    "plt.title(f\"TIC {TIC} — O–C after refit (P={P_fit:.6f} d)\")\n",
    "plt.tight_layout()\n",
    "oc_path = os.path.join(FDIR, f\"TIC{TIC}_OC_{STAMP}.png\")\n",
    "fig.savefig(oc_path, dpi=150); plt.close(fig)\n",
    "\n",
    "print(\"\\n[done]\")\n",
    "print(f\"  TLS periodogram : {pgram_path}\")\n",
    "print(f\"  Phase-fold plot : {fold_path}\")\n",
    "print(f\"  Midtimes CSV    : {mt_csv}\")\n",
    "print(f\"  Ephemeris JSON  : {ephem_path}\")\n",
    "print(f\"  O–C plot        : {oc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37c2c0a5-8788-4f70-a420-6947cb5aee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TIC 37749396 — stitching PDCSAP (sectors: all)\n",
      "[tls] Narrow search around P=13.475725 d (±1.0%) using 7 threads\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 125002 data points, 1027 periods from 13.341 to 13.61 days\n",
      "Using 7 of 8 CPU threads\n",
      "Searching for best T0 for period 13.47214 days\n",
      "[tls] best P=13.472137 d, T0=1392.79917 BTJD, SDE≈8.15\n",
      "[fit] estimating per-event mid-times and refitting linear ephemeris …\n",
      "[fit] P = 13.47232744 d, T0 = 1392.785303 BTJD, rχ² = 0.04\n",
      "[fit] σ(P) = 1.839523e-05 d, σ(T0) = 1.462941e-03 d, ρ = -0.986\n",
      "\n",
      "[done]\n",
      "  TLS periodogram : figures/TIC37749396/TIC37749396_TLS_narrow_20250927T213104Z.png\n",
      "  Phase-fold plot : figures/TIC37749396/TIC37749396_TLS_narrow_phase_20250927T213104Z.png\n",
      "  Midtimes CSV    : results/TIC37749396/TIC37749396_midtimes_20250927T213104Z.csv\n",
      "  Ephemeris JSON  : results/TIC37749396/TIC37749396_refit_ephemeris_20250927T213104Z.json\n",
      "  O–C plot        : figures/TIC37749396/TIC37749396_OC_20250927T213104Z.png\n"
     ]
    }
   ],
   "source": [
    "# === Target B (TOI 260.01 / TIC 37749396) • Combine sectors • TLS near catalog P • Refit (P, T0) with covariance ===\n",
    "# Minimal-verbosity; saves artifacts under results/TIC{TIC}/ and figures/TIC{TIC}/\n",
    "\n",
    "import os, json, csv, warnings, math, datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lightkurve import search_lightcurvefile, LightCurveCollection\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "\n",
    "# ---------- USER KNOBS ----------\n",
    "TIC          = 37749396                  # Target B\n",
    "SECTORS_HINT = None                      # e.g., [3, 42, 70]; set None to auto-use all available\n",
    "CATALOG_P    = 13.475725                 # narrow-center period (days); from your stitched TLS top-1\n",
    "WINDOW_FRAC  = 0.01                      # ±1% search window\n",
    "DUR_HOURS    = 3.0                       # rough duration for masks & display (hours)\n",
    "QUALITY_BITS = 175                       # SPOC quality bitmask (keep good)\n",
    "FLAT_WIN_D   = 1.0                       # detrend window (days), gentle so we don't erase dips\n",
    "POLY_ORDER   = 2\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "STAMP   = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RDIR    = f\"results/TIC{TIC}\"\n",
    "FDIR    = f\"figures/TIC{TIC}\"\n",
    "os.makedirs(RDIR, exist_ok=True)\n",
    "os.makedirs(FDIR, exist_ok=True)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _median_bin(x, y, nbins=180):\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    lo, hi = np.nanmin(x), np.nanmax(x)\n",
    "    edges = np.linspace(lo, hi, nbins+1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    px = 0.5*(edges[:-1] + edges[1:])\n",
    "    py = np.full(nbins, np.nan)\n",
    "    for i in range(nbins):\n",
    "        m = y[idx == i]\n",
    "        if m.size:\n",
    "            py[i] = np.nanmedian(m)\n",
    "    return px, py\n",
    "\n",
    "def _gentle_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    # Lightkurve flatten with conservative window; avoids shaving ~hour dips\n",
    "    try:\n",
    "        flat = lc.flatten(\n",
    "            window_length=int(max(7, round(window_days/(np.nanmedian(np.diff(lc.time.value)) or 1e-3)))),\n",
    "            polyorder=polyorder,\n",
    "            return_trend=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        flat = lc.flatten(\n",
    "            window_length=int(max(7, round(window_days/(np.nanmedian(np.diff(lc.time.value)) or 1e-3)))),\n",
    "            polyorder=polyorder\n",
    "        )\n",
    "    return flat.remove_nans().normalize()\n",
    "\n",
    "def _load_stitched_pdcsap(tic, sectors_hint=None, quality_bits=175):\n",
    "    sr = search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "    if len(sr) == 0:\n",
    "        raise RuntimeError(\"No SPOC LightCurveFiles found.\")\n",
    "    files = sr.download_all()\n",
    "\n",
    "    lcs = []\n",
    "    for f in files:\n",
    "        if sectors_hint is not None:\n",
    "            sec = getattr(f, \"sector\", None)\n",
    "            if (sec is None) or (sec not in sectors_hint):\n",
    "                continue\n",
    "\n",
    "        lc = f.PDCSAP_FLUX\n",
    "\n",
    "        if hasattr(lc, \"remove_nans\"):\n",
    "            lc = lc.remove_nans()\n",
    "        if hasattr(lc, \"remove_outliers\"):\n",
    "            try:\n",
    "                lc = lc.remove_outliers(sigma=10)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if hasattr(lc, \"quality\"):\n",
    "            good = (lc.quality & quality_bits) == 0\n",
    "            lc = lc[good]\n",
    "\n",
    "        if len(lc.time) == 0:\n",
    "            continue\n",
    "\n",
    "        lcs.append(lc.normalize())\n",
    "\n",
    "    if not lcs:\n",
    "        raise RuntimeError(\"No usable PDCSAP cadences after quality mask.\")\n",
    "    return LightCurveCollection(lcs).stitch().remove_nans().normalize()\n",
    "\n",
    "def _find_midtimes_quadratic(time_btjd, flux_norm, period, t0, dur_hours, k_half=1.0):\n",
    "    \"\"\"\n",
    "    Per-event Tmid finder:\n",
    "      - select windows around predicted midtimes (±k_half * duration/2)\n",
    "      - detrend locally (linear)\n",
    "      - fit quadratic to lowest ~30% of points -> vertex time = Tmid\n",
    "      - estimate per-event sigma_Tmid from bootstrap-of-residuals\n",
    "    Returns: epochs[], tmids[], tmid_errs[]\n",
    "    \"\"\"\n",
    "    tarr = np.asarray(time_btjd, float)\n",
    "    farr = np.asarray(flux_norm, float)\n",
    "    dur_days = dur_hours/24.0\n",
    "\n",
    "    kmin = math.floor((tarr.min() - t0)/period) - 1\n",
    "    kmax = math.ceil((tarr.max() - t0)/period) + 1\n",
    "    epochs, tmids, terrs = [], [], []\n",
    "\n",
    "    for k in range(kmin, kmax+1):\n",
    "        tc = t0 + k*period\n",
    "        w  = (np.abs(tarr - tc) <= 0.5*k_half*dur_days)\n",
    "        if np.count_nonzero(w) < 8:\n",
    "            continue\n",
    "        t  = tarr[w]\n",
    "        f  = farr[w]\n",
    "\n",
    "        # local linear detrend\n",
    "        A = np.vstack([np.ones_like(t), t - np.nanmedian(t)]).T\n",
    "        coeff, *_ = np.linalg.lstsq(A, f, rcond=None)\n",
    "        f_det = f - (A @ coeff)\n",
    "\n",
    "        # take lowest ~30% points for quadratic\n",
    "        q = np.nanpercentile(f_det, 30)\n",
    "        sel = f_det <= q\n",
    "        if np.count_nonzero(sel) < 6:\n",
    "            continue\n",
    "        tt = t[sel] - np.nanmedian(t[sel])\n",
    "        ff = f_det[sel]\n",
    "\n",
    "        X = np.vstack([tt**2, tt, np.ones_like(tt)]).T\n",
    "        p, *_ = np.linalg.lstsq(X, ff, rcond=None)\n",
    "        a, b, c = p\n",
    "        if a == 0:\n",
    "            continue\n",
    "        tmid = -b/(2*a) + np.nanmedian(t[sel])\n",
    "\n",
    "        # rough uncertainty via bootstrap of residuals\n",
    "        model = a*tt**2 + b*tt + c\n",
    "        res   = ff - model\n",
    "        if len(res) < 6:\n",
    "            continue\n",
    "        draws = []\n",
    "        rng = np.random.default_rng(42)\n",
    "        for _ in range(200):\n",
    "            res_s = rng.choice(res, size=res.size, replace=True)\n",
    "            ff_s  = model + res_s\n",
    "            p_s, *_ = np.linalg.lstsq(X, ff_s, rcond=None)\n",
    "            a_s, b_s, c_s = p_s\n",
    "            if a_s == 0:\n",
    "                continue\n",
    "            draws.append(-b_s/(2*a_s) + np.nanmedian(t[sel]))\n",
    "        if len(draws) < 10:\n",
    "            continue\n",
    "        epochs.append(k)\n",
    "        tmids.append(tmid)\n",
    "        terrs.append(np.nanstd(draws, ddof=1))\n",
    "    return np.asarray(epochs), np.asarray(tmids), np.asarray(terrs)\n",
    "\n",
    "def _fit_linear_ephemeris(epochs, tmids, terrs):\n",
    "    \"\"\"Weighted least squares fit of T(n)=T0+nP; returns P, T0, Cov 2x2, redchisq.\"\"\"\n",
    "    if len(tmids) < 2:\n",
    "        raise RuntimeError(\"Not enough mid-times to refit ephemeris.\")\n",
    "    w   = 1.0/np.clip(terrs, 1e-6, np.inf)**2\n",
    "    X   = np.vstack([epochs, np.ones_like(epochs)]).T\n",
    "    XT  = (X.T*w)\n",
    "    beta= np.linalg.inv(XT@X) @ (XT@tmids)\n",
    "    P, T0 = beta[0], beta[1]\n",
    "    yhat  = X@beta\n",
    "    resid = tmids - yhat\n",
    "    dof   = max(1, len(tmids)-2)\n",
    "    chi2  = np.sum((resid**2)*w)\n",
    "    rchi2 = chi2/dof\n",
    "    Cov   = np.linalg.inv(XT@X) * rchi2\n",
    "    return P, T0, Cov, rchi2\n",
    "\n",
    "def _save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# ---------- 1) LOAD & FLATTEN (gentle) ----------\n",
    "print(f\"[load] TIC {TIC} — stitching PDCSAP (sectors: {'all' if not SECTORS_HINT else SECTORS_HINT})\")\n",
    "lc = _load_stitched_pdcsap(TIC, sectors_hint=SECTORS_HINT, quality_bits=QUALITY_BITS)\n",
    "lc_flat = _gentle_flatten(lc, window_days=FLAT_WIN_D, polyorder=POLY_ORDER)\n",
    "\n",
    "# ---------- 2) TLS in a narrow window around catalog P ----------\n",
    "pmin = CATALOG_P * (1.0 - WINDOW_FRAC)\n",
    "pmax = CATALOG_P * (1.0 + WINDOW_FRAC)\n",
    "\n",
    "time = lc_flat.time.value\n",
    "flux = lc_flat.flux.value\n",
    "nthreads = max(1, (os.cpu_count() or 2) - 1)\n",
    "print(f\"[tls] Narrow search around P={CATALOG_P:.6f} d (±{100*WINDOW_FRAC:.1f}%) using {nthreads} threads\")\n",
    "\n",
    "tls = transitleastsquares(time, flux)\n",
    "tls_res = tls.power(\n",
    "    period_min=pmin,\n",
    "    period_max=pmax,\n",
    "    oversampling_factor=5,\n",
    "    use_threads=nthreads,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "bestP = float(tls_res.period)\n",
    "bestT0= float(tls_res.T0)\n",
    "bestSDE = float(getattr(tls_res, \"SDE\", getattr(tls_res, \"sde\", np.nan)))\n",
    "print(f\"[tls] best P={bestP:.6f} d, T0={bestT0:.5f} BTJD, SDE≈{bestSDE:.2f}\")\n",
    "\n",
    "# Save TLS periodogram\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.plot(tls_res.periods, tls_res.power, lw=1)\n",
    "plt.axvline(bestP, ls=\"--\")\n",
    "plt.xlabel(\"Period [days]\"); plt.ylabel(\"TLS power (SDE proxy)\")\n",
    "plt.title(f\"TIC {TIC} — TLS around {CATALOG_P:.5f} d (best {bestP:.5f} d)\")\n",
    "plt.tight_layout()\n",
    "pgram_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_{STAMP}.png\")\n",
    "fig.savefig(pgram_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# Save a phase-folded figure at TLS best\n",
    "folded = lc_flat.fold(period=bestP, epoch_time=bestT0)\n",
    "phase  = folded.phase.value if hasattr(folded.phase, \"value\") else folded.phase\n",
    "px, py = _median_bin(phase, folded.flux.value, nbins=180)\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.scatter(phase, folded.flux.value, s=4, alpha=0.15)\n",
    "plt.plot(px, py, lw=1.8)\n",
    "dur_phase = (DUR_HOURS/24.0)/bestP\n",
    "plt.axvspan(-0.5*dur_phase, 0.5*dur_phase, alpha=0.15)\n",
    "plt.xlabel(\"Phase [cycles]\"); plt.ylabel(\"Relative flux\")\n",
    "plt.title(f\"TIC {TIC} — phase-fold @ P={bestP:.5f} d, T0={bestT0:.5f}\")\n",
    "plt.tight_layout()\n",
    "fold_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_phase_{STAMP}.png\")\n",
    "fig.savefig(fold_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# ---------- 3) Refit P, T0 from per-event midtimes with full covariance ----------\n",
    "print(\"[fit] estimating per-event mid-times and refitting linear ephemeris …\")\n",
    "epochs, tmids, terrs = _find_midtimes_quadratic(time, flux, bestP, bestT0, DUR_HOURS, k_half=1.7)\n",
    "if len(tmids) < 2:\n",
    "    raise RuntimeError(f\"Only {len(tmids)} mid-times found; need ≥2 to fit ephemeris.\")\n",
    "\n",
    "P_fit, T0_fit, Cov, rchi2 = _fit_linear_ephemeris(epochs, tmids, terrs)\n",
    "cov_dict = {\"C00_T0\": float(Cov[1,1]), \"C11_P\": float(Cov[0,0]), \"C01_T0P\": float(Cov[1,0])}\n",
    "\n",
    "print(f\"[fit] P = {P_fit:.8f} d, T0 = {T0_fit:.6f} BTJD, rχ² = {rchi2:.2f}\")\n",
    "print(f\"[fit] σ(P) = {math.sqrt(Cov[0,0]):.6e} d, σ(T0) = {math.sqrt(Cov[1,1]):.6e} d, ρ = {Cov[0,1]/math.sqrt(Cov[0,0]*Cov[1,1]):.3f}\")\n",
    "\n",
    "# Save midtimes CSV\n",
    "mt_csv = os.path.join(RDIR, f\"TIC{TIC}_midtimes_{STAMP}.csv\")\n",
    "with open(mt_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"tmid_btjd\", \"tmid_err_d\"])\n",
    "    for n, tmid, terr in zip(epochs, tmids, terrs):\n",
    "        w.writerow([int(n), f\"{tmid:.8f}\", f\"{terr:.8e}\"])\n",
    "\n",
    "# ---------- Save ephemeris JSON ----------\n",
    "ephem_json = dict(\n",
    "    tic=TIC,\n",
    "    tls_narrow=dict(\n",
    "        catalog_P=CATALOG_P,\n",
    "        search_window_frac=WINDOW_FRAC,\n",
    "        best_P=bestP,\n",
    "        best_T0=bestT0,\n",
    "        SDE=bestSDE,\n",
    "        periodogram=pgram_path,\n",
    "        phase_plot=fold_path,\n",
    "    ),\n",
    "    fit=dict(\n",
    "        P=P_fit,\n",
    "        T0=T0_fit,\n",
    "        rchisq=rchi2,\n",
    "        cov=cov_dict,\n",
    "        n_midtimes=int(len(tmids)),\n",
    "    ),\n",
    "    detrend=dict(kind=\"sliding_poly\", window_days=FLAT_WIN_D, polyorder=POLY_ORDER),\n",
    "    quality_bits=QUALITY_BITS,\n",
    "    run_utc=STAMP,\n",
    ")\n",
    "ephem_path = os.path.join(RDIR, f\"TIC{TIC}_refit_ephemeris_{STAMP}.json\")\n",
    "_save_json(ephem_path, ephem_json)\n",
    "\n",
    "# ---------- Simple O–C plot ----------\n",
    "oc = tmids - (T0_fit + epochs * P_fit)\n",
    "fig = plt.figure(figsize=(8.0, 4.8))\n",
    "plt.errorbar(epochs, oc * 24 * 60, yerr=terrs * 24 * 60, fmt=\"o\", ms=4, capsize=2)\n",
    "plt.axhline(0, color=\"k\", lw=1, ls=\"--\")\n",
    "plt.xlabel(\"Epoch (n)\"); plt.ylabel(\"O–C [minutes]\")\n",
    "plt.title(f\"TIC {TIC} — O–C after refit (P={P_fit:.6f} d)\")\n",
    "plt.tight_layout()\n",
    "oc_path = os.path.join(FDIR, f\"TIC{TIC}_OC_{STAMP}.png\")\n",
    "fig.savefig(oc_path, dpi=150); plt.close(fig)\n",
    "\n",
    "print(\"\\n[done]\")\n",
    "print(f\"  TLS periodogram : {pgram_path}\")\n",
    "print(f\"  Phase-fold plot : {fold_path}\")\n",
    "print(f\"  Midtimes CSV    : {mt_csv}\")\n",
    "print(f\"  Ephemeris JSON  : {ephem_path}\")\n",
    "print(f\"  O–C plot        : {oc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "535a4d22-6995-479b-8488-f8e6c805fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] TIC 311183180 — stitching PDCSAP (sectors: all)\n",
      "[tls] Narrow search around P=9.348442 d (±1.0%) using 7 threads\n",
      "Transit Least Squares TLS 1.32 (5 Apr 2024)\n",
      "Creating model cache for 29 durations\n",
      "Searching 33536 data points, 459 periods from 9.255 to 9.442 days\n",
      "Using 7 of 8 CPU threads\n",
      "Searching for best T0 for period 9.34288 days\n",
      "[tls] best P=9.342883 d, T0=1439.58140 BTJD, SDE≈5.97\n",
      "[fit] estimating per-event mid-times and refitting linear ephemeris …\n",
      "[fit] P = 9.34282090 d, T0 = 1439.584770 BTJD, rχ² = 0.01\n",
      "[fit] σ(P) = 3.254905e-07 d, σ(T0) = 2.089374e-05 d, ρ = -0.831\n",
      "\n",
      "[done]\n",
      "  TLS periodogram : figures/TIC311183180/TIC311183180_TLS_narrow_20250927T214103Z.png\n",
      "  Phase-fold plot : figures/TIC311183180/TIC311183180_TLS_narrow_phase_20250927T214103Z.png\n",
      "  Midtimes CSV    : results/TIC311183180/TIC311183180_midtimes_20250927T214103Z.csv\n",
      "  Ephemeris JSON  : results/TIC311183180/TIC311183180_refit_ephemeris_20250927T214103Z.json\n",
      "  O–C plot        : figures/TIC311183180/TIC311183180_OC_20250927T214103Z.png\n"
     ]
    }
   ],
   "source": [
    "# === Target C (TOI 550.02 / TIC 311183180) • Combine sectors • TLS near catalog P • Refit (P, T0) with covariance ===\n",
    "# Minimal-verbosity; saves artifacts under results/TIC{TIC}/ and figures/TIC{TIC}/\n",
    "\n",
    "import os, json, csv, warnings, math, datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lightkurve import search_lightcurvefile, LightCurveCollection\n",
    "from transitleastsquares import transitleastsquares\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "plt.rcParams.update({\"figure.dpi\": 120})\n",
    "\n",
    "# ---------- USER KNOBS ----------\n",
    "TIC          = 311183180                 # Target C\n",
    "SECTORS_HINT = None                      # e.g., [5, 31]; set None to auto-use all available\n",
    "CATALOG_P    = 9.348442                  # narrow-center period (days); from your stitched TLS top-1\n",
    "WINDOW_FRAC  = 0.01                      # ±1% search window\n",
    "DUR_HOURS    = 2.8                       # rough duration for masks & display (hours)\n",
    "QUALITY_BITS = 175\n",
    "FLAT_WIN_D   = 1.0\n",
    "POLY_ORDER   = 2\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "STAMP   = dt.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RDIR    = f\"results/TIC{TIC}\"\n",
    "FDIR    = f\"figures/TIC{TIC}\"\n",
    "os.makedirs(RDIR, exist_ok=True)\n",
    "os.makedirs(FDIR, exist_ok=True)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _median_bin(x, y, nbins=180):\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    lo, hi = np.nanmin(x), np.nanmax(x)\n",
    "    edges = np.linspace(lo, hi, nbins+1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    px = 0.5*(edges[:-1] + edges[1:])\n",
    "    py = np.full(nbins, np.nan)\n",
    "    for i in range(nbins):\n",
    "        m = y[idx == i]\n",
    "        if m.size:\n",
    "            py[i] = np.nanmedian(m)\n",
    "    return px, py\n",
    "\n",
    "def _gentle_flatten(lc, window_days=1.0, polyorder=2):\n",
    "    try:\n",
    "        flat = lc.flatten(\n",
    "            window_length=int(max(7, round(window_days/(np.nanmedian(np.diff(lc.time.value)) or 1e-3)))),\n",
    "            polyorder=polyorder,\n",
    "            return_trend=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        flat = lc.flatten(\n",
    "            window_length=int(max(7, round(window_days/(np.nanmedian(np.diff(lc.time.value)) or 1e-3)))),\n",
    "            polyorder=polyorder\n",
    "        )\n",
    "    return flat.remove_nans().normalize()\n",
    "\n",
    "def _load_stitched_pdcsap(tic, sectors_hint=None, quality_bits=175):\n",
    "    sr = search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "    if len(sr) == 0:\n",
    "        raise RuntimeError(\"No SPOC LightCurveFiles found.\")\n",
    "    files = sr.download_all()\n",
    "\n",
    "    lcs = []\n",
    "    for f in files:\n",
    "        if sectors_hint is not None:\n",
    "            sec = getattr(f, \"sector\", None)\n",
    "            if (sec is None) or (sec not in sectors_hint):\n",
    "                continue\n",
    "\n",
    "        lc = f.PDCSAP_FLUX\n",
    "\n",
    "        if hasattr(lc, \"remove_nans\"):\n",
    "            lc = lc.remove_nans()\n",
    "        if hasattr(lc, \"remove_outliers\"):\n",
    "            try:\n",
    "                lc = lc.remove_outliers(sigma=10)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if hasattr(lc, \"quality\"):\n",
    "            good = (lc.quality & quality_bits) == 0\n",
    "            lc = lc[good]\n",
    "\n",
    "        if len(lc.time) == 0:\n",
    "            continue\n",
    "\n",
    "        lcs.append(lc.normalize())\n",
    "\n",
    "    if not lcs:\n",
    "        raise RuntimeError(\"No usable PDCSAP cadences after quality mask.\")\n",
    "    return LightCurveCollection(lcs).stitch().remove_nans().normalize()\n",
    "\n",
    "def _find_midtimes_quadratic(time_btjd, flux_norm, period, t0, dur_hours, k_half=1.0):\n",
    "    tarr = np.asarray(time_btjd, float)\n",
    "    farr = np.asarray(flux_norm, float)\n",
    "    dur_days = dur_hours/24.0\n",
    "\n",
    "    kmin = math.floor((tarr.min() - t0)/period) - 1\n",
    "    kmax = math.ceil((tarr.max() - t0)/period) + 1\n",
    "    epochs, tmids, terrs = [], [], []\n",
    "\n",
    "    for k in range(kmin, kmax+1):\n",
    "        tc = t0 + k*period\n",
    "        w  = (np.abs(tarr - tc) <= 0.5*k_half*dur_days)\n",
    "        if np.count_nonzero(w) < 8:\n",
    "            continue\n",
    "        t  = tarr[w]\n",
    "        f  = farr[w]\n",
    "\n",
    "        A = np.vstack([np.ones_like(t), t - np.nanmedian(t)]).T\n",
    "        coeff, *_ = np.linalg.lstsq(A, f, rcond=None)\n",
    "        f_det = f - (A @ coeff)\n",
    "\n",
    "        q = np.nanpercentile(f_det, 30)\n",
    "        sel = f_det <= q\n",
    "        if np.count_nonzero(sel) < 6:\n",
    "            continue\n",
    "        tt = t[sel] - np.nanmedian(t[sel])\n",
    "        ff = f_det[sel]\n",
    "\n",
    "        X = np.vstack([tt**2, tt, np.ones_like(tt)]).T\n",
    "        p, *_ = np.linalg.lstsq(X, ff, rcond=None)\n",
    "        a, b, c = p\n",
    "        if a == 0:\n",
    "            continue\n",
    "        tmid = -b/(2*a) + np.nanmedian(t[sel])\n",
    "\n",
    "        model = a*tt**2 + b*tt + c\n",
    "        res   = ff - model\n",
    "        if len(res) < 6:\n",
    "            continue\n",
    "        draws = []\n",
    "        rng = np.random.default_rng(42)\n",
    "        for _ in range(200):\n",
    "            res_s = rng.choice(res, size=res.size, replace=True)\n",
    "            ff_s  = model + res_s\n",
    "            p_s, *_ = np.linalg.lstsq(X, ff_s, rcond=None)\n",
    "            a_s, b_s, c_s = p_s\n",
    "            if a_s == 0:\n",
    "                continue\n",
    "            draws.append(-b_s/(2*a_s) + np.nanmedian(t[sel]))\n",
    "        if len(draws) < 10:\n",
    "            continue\n",
    "        epochs.append(k)\n",
    "        tmids.append(tmid)\n",
    "        terrs.append(np.nanstd(draws, ddof=1))\n",
    "    return np.asarray(epochs), np.asarray(tmids), np.asarray(terrs)\n",
    "\n",
    "def _fit_linear_ephemeris(epochs, tmids, terrs):\n",
    "    if len(tmids) < 2:\n",
    "        raise RuntimeError(\"Not enough mid-times to refit ephemeris.\")\n",
    "    w   = 1.0/np.clip(terrs, 1e-6, np.inf)**2\n",
    "    X   = np.vstack([epochs, np.ones_like(epochs)]).T\n",
    "    XT  = (X.T*w)\n",
    "    beta= np.linalg.inv(XT@X) @ (XT@tmids)\n",
    "    P, T0 = beta[0], beta[1]\n",
    "    yhat  = X@beta\n",
    "    resid = tmids - yhat\n",
    "    dof   = max(1, len(tmids)-2)\n",
    "    chi2  = np.sum((resid**2)*w)\n",
    "    rchi2 = chi2/dof\n",
    "    Cov   = np.linalg.inv(XT@X) * rchi2\n",
    "    return P, T0, Cov, rchi2\n",
    "\n",
    "def _save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# ---------- 1) LOAD & FLATTEN ----------\n",
    "print(f\"[load] TIC {TIC} — stitching PDCSAP (sectors: {'all' if not SECTORS_HINT else SECTORS_HINT})\")\n",
    "lc = _load_stitched_pdcsap(TIC, sectors_hint=SECTORS_HINT, quality_bits=QUALITY_BITS)\n",
    "lc_flat = _gentle_flatten(lc, window_days=FLAT_WIN_D, polyorder=POLY_ORDER)\n",
    "\n",
    "# ---------- 2) TLS narrow around catalog P ----------\n",
    "pmin = CATALOG_P * (1.0 - WINDOW_FRAC)\n",
    "pmax = CATALOG_P * (1.0 + WINDOW_FRAC)\n",
    "\n",
    "time = lc_flat.time.value\n",
    "flux = lc_flat.flux.value\n",
    "nthreads = max(1, (os.cpu_count() or 2) - 1)\n",
    "print(f\"[tls] Narrow search around P={CATALOG_P:.6f} d (±{100*WINDOW_FRAC:.1f}%) using {nthreads} threads\")\n",
    "\n",
    "tls = transitleastsquares(time, flux)\n",
    "tls_res = tls.power(\n",
    "    period_min=pmin,\n",
    "    period_max=pmax,\n",
    "    oversampling_factor=5,\n",
    "    use_threads=nthreads,\n",
    "    show_progress_bar=False\n",
    ")\n",
    "\n",
    "bestP = float(tls_res.period)\n",
    "bestT0= float(tls_res.T0)\n",
    "bestSDE = float(getattr(tls_res, \"SDE\", getattr(tls_res, \"sde\", np.nan)))\n",
    "print(f\"[tls] best P={bestP:.6f} d, T0={bestT0:.5f} BTJD, SDE≈{bestSDE:.2f}\")\n",
    "\n",
    "# Save TLS periodogram\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.plot(tls_res.periods, tls_res.power, lw=1)\n",
    "plt.axvline(bestP, ls=\"--\")\n",
    "plt.xlabel(\"Period [days]\"); plt.ylabel(\"TLS power (SDE proxy)\")\n",
    "plt.title(f\"TIC {TIC} — TLS around {CATALOG_P:.5f} d (best {bestP:.5f} d)\")\n",
    "plt.tight_layout()\n",
    "pgram_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_{STAMP}.png\")\n",
    "fig.savefig(pgram_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# Save a phase-folded figure at TLS best\n",
    "folded = lc_flat.fold(period=bestP, epoch_time=bestT0)\n",
    "phase  = folded.phase.value if hasattr(folded.phase, \"value\") else folded.phase\n",
    "px, py = _median_bin(phase, folded.flux.value, nbins=180)\n",
    "fig = plt.figure(figsize=(8.6,5.0))\n",
    "plt.scatter(phase, folded.flux.value, s=4, alpha=0.15)\n",
    "plt.plot(px, py, lw=1.8)\n",
    "dur_phase = (DUR_HOURS/24.0)/bestP\n",
    "plt.axvspan(-0.5*dur_phase, 0.5*dur_phase, alpha=0.15)\n",
    "plt.xlabel(\"Phase [cycles]\"); plt.ylabel(\"Relative flux\")\n",
    "plt.title(f\"TIC {TIC} — phase-fold @ P={bestP:.5f} d, T0={bestT0:.5f}\")\n",
    "plt.tight_layout()\n",
    "fold_path = os.path.join(FDIR, f\"TIC{TIC}_TLS_narrow_phase_{STAMP}.png\")\n",
    "fig.savefig(fold_path, dpi=150); plt.close(fig)\n",
    "\n",
    "# ---------- 3) Refit P, T0 with covariance ----------\n",
    "print(\"[fit] estimating per-event mid-times and refitting linear ephemeris …\")\n",
    "epochs, tmids, terrs = _find_midtimes_quadratic(time, flux, bestP, bestT0, DUR_HOURS, k_half=1.7)\n",
    "if len(tmids) < 2:\n",
    "    raise RuntimeError(f\"Only {len(tmids)} mid-times found; need ≥2 to fit ephemeris.\")\n",
    "\n",
    "P_fit, T0_fit, Cov, rchi2 = _fit_linear_ephemeris(epochs, tmids, terrs)\n",
    "cov_dict = {\"C00_T0\": float(Cov[1,1]), \"C11_P\": float(Cov[0,0]), \"C01_T0P\": float(Cov[1,0])}\n",
    "\n",
    "print(f\"[fit] P = {P_fit:.8f} d, T0 = {T0_fit:.6f} BTJD, rχ² = {rchi2:.2f}\")\n",
    "print(f\"[fit] σ(P) = {math.sqrt(Cov[0,0]):.6e} d, σ(T0) = {math.sqrt(Cov[1,1]):.6e} d, ρ = {Cov[0,1]/math.sqrt(Cov[0,0]*Cov[1,1]):.3f}\")\n",
    "\n",
    "# Save midtimes CSV\n",
    "mt_csv = os.path.join(RDIR, f\"TIC{TIC}_midtimes_{STAMP}.csv\")\n",
    "with open(mt_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"epoch\", \"tmid_btjd\", \"tmid_err_d\"])\n",
    "    for n, tmid, terr in zip(epochs, tmids, terrs):\n",
    "        w.writerow([int(n), f\"{tmid:.8f}\", f\"{terr:.8e}\"])\n",
    "\n",
    "# ---------- Save ephemeris JSON ----------\n",
    "ephem_json = dict(\n",
    "    tic=TIC,\n",
    "    tls_narrow=dict(\n",
    "        catalog_P=CATALOG_P,\n",
    "        search_window_frac=WINDOW_FRAC,\n",
    "        best_P=bestP,\n",
    "        best_T0=bestT0,\n",
    "        SDE=bestSDE,\n",
    "        periodogram=pgram_path,\n",
    "        phase_plot=fold_path,\n",
    "    ),\n",
    "    fit=dict(\n",
    "        P=P_fit,\n",
    "        T0=T0_fit,\n",
    "        rchisq=rchi2,\n",
    "        cov=cov_dict,\n",
    "        n_midtimes=int(len(tmids)),\n",
    "    ),\n",
    "    detrend=dict(kind=\"sliding_poly\", window_days=FLAT_WIN_D, polyorder=POLY_ORDER),\n",
    "    quality_bits=QUALITY_BITS,\n",
    "    run_utc=STAMP,\n",
    ")\n",
    "ephem_path = os.path.join(RDIR, f\"TIC{TIC}_refit_ephemeris_{STAMP}.json\")\n",
    "_save_json(ephem_path, ephem_json)\n",
    "\n",
    "# ---------- Simple O–C plot ----------\n",
    "oc = tmids - (T0_fit + epochs * P_fit)\n",
    "fig = plt.figure(figsize=(8.0, 4.8))\n",
    "plt.errorbar(epochs, oc * 24 * 60, yerr=terrs * 24 * 60, fmt=\"o\", ms=4, capsize=2)\n",
    "plt.axhline(0, color=\"k\", lw=1, ls=\"--\")\n",
    "plt.xlabel(\"Epoch (n)\"); plt.ylabel(\"O–C [minutes]\")\n",
    "plt.title(f\"TIC {TIC} — O–C after refit (P={P_fit:.6f} d)\")\n",
    "plt.tight_layout()\n",
    "oc_path = os.path.join(FDIR, f\"TIC{TIC}_OC_{STAMP}.png\")\n",
    "fig.savefig(oc_path, dpi=150); plt.close(fig)\n",
    "\n",
    "print(\"\\n[done]\")\n",
    "print(f\"  TLS periodogram : {pgram_path}\")\n",
    "print(f\"  Phase-fold plot : {fold_path}\")\n",
    "print(f\"  Midtimes CSV    : {mt_csv}\")\n",
    "print(f\"  Ephemeris JSON  : {ephem_path}\")\n",
    "print(f\"  O–C plot        : {oc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21dfcdb-7577-4367-9ceb-4ed9fcb61510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0v/nmv_q34n0hz650dnlq12lhl1jf_dp7/T/ipykernel_79405/1512842400.py:91: LightkurveDeprecationWarning: The search_lightcurvefile function is deprecated and may be removed in a future version.\n",
      "        Use search_lightcurve() instead.\n",
      "  sr = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
      "No data found for target \"TIC 123456789\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Target D] TIC 123456789 — sectors: None found\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "No usable PDCSAP sectors found. Provide SECTORS or check network.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m No usable PDCSAP sectors found. Provide SECTORS or check network.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Quick Scan — Target D (BLS wide → TLS narrow + fast vetting)\n",
    "# Edits: set TARGET_TIC (and optional TARGET_NAME). Optional: SECTORS to force a subset.\n",
    "\n",
    "# %%\n",
    "import os, time, warnings, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---- USER INPUT --------------------------------------------------------------\n",
    "TARGET_TIC   = 123456789          # <-- EDIT: TIC ID for Target D\n",
    "TARGET_NAME  = \"Target D\"         # <-- optional nickname for titles\n",
    "SECTORS      = None               # e.g., [14, 15] to force; or None to auto-discover\n",
    "\n",
    "# ---- RUNTIME SETTINGS (keep as-is) ------------------------------------------\n",
    "QUALITY_BITS = 175\n",
    "WINDOW_DAYS  = 0.75   # gentle high-pass\n",
    "MAD_SIGMA    = 5.0\n",
    "BLS_PMIN, BLS_PMAX = 0.5, 50.0\n",
    "BLS_NPER     = 5000\n",
    "DUR_GRID_HR  = np.linspace(0.5, 3.0, 18)  # BLS duration grid\n",
    "TLS_NARROW_FRAC = 0.01   # ±1% around each BLS peak\n",
    "TLS_MIN_TRANSITS_SECTOR = 3\n",
    "TLS_MIN_TRANSITS_STITCH = 2\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ---- Imports that may be optional -------------------------------------------\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares\n",
    "try:\n",
    "    from transitleastsquares import transitleastsquares as TLS\n",
    "    HAVE_TLS = True\n",
    "except Exception:\n",
    "    HAVE_TLS = False\n",
    "\n",
    "# ---- Helpers ----------------------------------------------------------------\n",
    "def _mad(x): \n",
    "    m = np.nanmedian(x); return np.nanmedian(np.abs(x - m)) * 1.4826\n",
    "\n",
    "def detrend_gentle(lc):\n",
    "    \"\"\"quality mask, sigma-clip, running-median high-pass, normalize.\"\"\"\n",
    "    lc2 = lc.remove_outliers(sigma=20)  # remove NaNs/inf\n",
    "    lc2 = lc2[np.bitwise_and(lc2.quality, 0) == 0] if hasattr(lc2, \"quality\") else lc2\n",
    "    lc2 = lc2.remove_nans()\n",
    "    # sigma-clip on flux\n",
    "    f = lc2.flux.value.copy()\n",
    "    s = _mad(f)\n",
    "    if np.isfinite(s) and s > 0:\n",
    "        m = np.nanmedian(f)\n",
    "        ok = (f > m - MAD_SIGMA*s) & (f < m + MAD_SIGMA*s)\n",
    "        lc2 = lc2[ok]\n",
    "    # high-pass via running median\n",
    "    dt = np.nanmedian(np.diff(lc2.time.value))\n",
    "    win = max(3, int(round((WINDOW_DAYS / dt))))\n",
    "    trend = pd.Series(lc2.flux.value).rolling(win, center=True, min_periods=max(3,win//3)).median()\n",
    "    trend = np.interp(np.arange(len(trend)), np.flatnonzero(np.isfinite(trend)), trend[np.isfinite(trend)])\n",
    "    flat = lc2.copy()\n",
    "    flat.flux = (lc2.flux.value / trend)\n",
    "    return flat.normalize().remove_nans()\n",
    "\n",
    "def load_pdcsap_sector(tic, sector=None, tries=3, sleep=2):\n",
    "    \"\"\"Prefer SPOC PDCSAP; robust to MAST flaps.\"\"\"\n",
    "    for k in range(tries):\n",
    "        try:\n",
    "            sr = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "            if len(sr) == 0:\n",
    "                return None\n",
    "            lcf = sr.download()\n",
    "            if lcf is None:\n",
    "                return None\n",
    "            lc = lcf.PDCSAP_FLUX\n",
    "            # Apply official LIGHTKURVE quality mask if present\n",
    "            lc = lc.remove_outliers(sigma=20)\n",
    "            if hasattr(lc, \"remove_quality\"):\n",
    "                lc = lc.remove_quality(QUALITY_BITS)\n",
    "            return lc.remove_nans()\n",
    "        except Exception as e:\n",
    "            if k == tries-1:\n",
    "                print(f\"[load] sector {sector} failed: {e}\")\n",
    "                return None\n",
    "            time.sleep(sleep*(2**k))\n",
    "\n",
    "def discover_sectors(tic):\n",
    "    try:\n",
    "        sr = lk.search_lightcurvefile(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "        sectors = sorted(list({r.mission_sectors[0] for r in sr if r.mission_sectors}))\n",
    "        return sectors\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def bls_scan(time, flux, dur_grid_hr=DUR_GRID_HR, pmin=BLS_PMIN, pmax=BLS_PMAX, nper=BLS_NPER):\n",
    "    y = flux - np.nanmedian(flux)\n",
    "    bls = BoxLeastSquares(time, y)\n",
    "    durations = dur_grid_hr/24.0\n",
    "    periods = np.geomspace(pmin, min(pmax, (time.max()-time.min())*0.95), nper)\n",
    "    res = bls.power(periods, durations, method='fast', normalization='standard')\n",
    "    return res, periods, durations\n",
    "\n",
    "def tls_scan_narrow(time, flux, per_center, frac=TLS_NARROW_FRAC, min_transits=2):\n",
    "    \"\"\"TLS around per_center ± frac; returns dict or None.\"\"\"\n",
    "    if not HAVE_TLS:\n",
    "        return None\n",
    "    y = flux/np.nanmedian(flux)\n",
    "    mask = np.isfinite(time) & np.isfinite(y)\n",
    "    time, y = time[mask], y[mask]\n",
    "    Pmin = per_center*(1.0 - frac)\n",
    "    Pmax = per_center*(1.0 + frac)\n",
    "    try:\n",
    "        model = TLS(time, y)\n",
    "        out = model.power(period_min=Pmin, period_max=Pmax, show_progress_bar=False, n_transits_min=min_transits)\n",
    "        return {\n",
    "            \"period\": float(out.period),\n",
    "            \"t0\": float(out.T0),\n",
    "            \"sde\": float(out.SDE),\n",
    "            \"duration_d\": float(out.duration),\n",
    "            \"depth\": float(out.depth)\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fold_plot(time, flux, period, t0=None, title=\"\", outpath=None):\n",
    "    if t0 is None:\n",
    "        t0 = time[0]\n",
    "    phase = ((time - t0 + 0.5*period) % period)/period - 0.5\n",
    "    order = np.argsort(phase); phase, f = phase[order], flux[order]\n",
    "    # mean-binned curve for visibility\n",
    "    nb = 100\n",
    "    bins = np.linspace(-0.5, 0.5, nb+1)\n",
    "    idx = np.digitize(phase, bins)-1\n",
    "    bmean = np.array([np.nanmean(f[idx==i]) for i in range(nb)])\n",
    "    bcent = 0.5*(bins[:-1]+bins[1:])\n",
    "    plt.figure(figsize=(7,3.2))\n",
    "    plt.scatter(phase, f, s=2, alpha=0.15, rasterized=True)\n",
    "    plt.plot(bcent, bmean, lw=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Phase (P)\")\n",
    "    plt.ylabel(\"Normalized flux\")\n",
    "    plt.tight_layout()\n",
    "    if outpath:\n",
    "        plt.savefig(outpath, dpi=180)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def odd_even_secondary_checks(time, flux, period, t0, dur_hr, tag, outdir=\"figures\"):\n",
    "    \"\"\"Quick QC panels & CSV row for odd/even + secondary@0.5P.\"\"\"\n",
    "    phase = ((time - t0 + 0.5*period) % period)/period - 0.5\n",
    "    dur = dur_hr/24.0\n",
    "    inodd  = (np.abs(phase) < 0.5*dur) & (np.floor(((time-t0)/period) % 2)==0)\n",
    "    ineven = (np.abs(phase) < 0.5*dur) & (np.floor(((time-t0)/period) % 2)==1)\n",
    "    insec  = (np.abs(phase-0.5) < 0.5*dur)\n",
    "    oot    = (np.abs(phase) > 1.5*dur) & (np.abs(phase-0.5) > 1.5*dur)\n",
    "\n",
    "    def depth_ppm(mask):\n",
    "        if mask.sum()<5 or oot.sum()<20: return np.nan\n",
    "        return (np.nanmedian(flux[oot]) - np.nanmedian(flux[mask]))*1e6\n",
    "\n",
    "    d_odd  = depth_ppm(inodd)\n",
    "    d_even = depth_ppm(ineven)\n",
    "    d_sec  = depth_ppm(insec)\n",
    "\n",
    "    # quick figure\n",
    "    plt.figure(figsize=(7.2,3.2))\n",
    "    plt.scatter(phase, flux, s=2, alpha=0.15, label=\"all\", rasterized=True)\n",
    "    plt.scatter(phase[inodd], flux[inodd], s=5, alpha=0.6, label=\"odd\")\n",
    "    plt.scatter(phase[ineven], flux[ineven], s=5, alpha=0.6, label=\"even\")\n",
    "    plt.scatter(phase[insec], flux[insec], s=5, alpha=0.6, label=\"secondary (0.5P)\")\n",
    "    plt.legend(frameon=False, ncol=3, fontsize=8)\n",
    "    plt.title(f\"{TARGET_NAME} TIC {TARGET_TIC} — Odd/Even/Secondary @ P={period:.5f} d\")\n",
    "    plt.xlabel(\"Phase\"); plt.ylabel(\"Normalized flux\"); plt.tight_layout()\n",
    "    outpng = f\"{outdir}/TIC{TARGET_TIC}_{tag}_odd_even_secondary.png\"\n",
    "    plt.savefig(outpng, dpi=180); plt.close()\n",
    "    return {\"odd_ppm\": d_odd, \"even_ppm\": d_even, \"secondary_ppm\": d_sec, \"png\": outpng}\n",
    "\n",
    "# ---- Load sectors ------------------------------------------------------------\n",
    "if SECTORS is None:\n",
    "    SECTORS = discover_sectors(TARGET_TIC)\n",
    "print(f\"[{TARGET_NAME}] TIC {TARGET_TIC} — sectors: {SECTORS if SECTORS else 'None found'}\")\n",
    "\n",
    "per_sector = []\n",
    "for s in (SECTORS or []):\n",
    "    lc = load_pdcsap_sector(TARGET_TIC, sector=s)\n",
    "    if lc is None or len(lc.flux.value)==0:\n",
    "        print(f\"  sector {s}: no PDCSAP\")\n",
    "        continue\n",
    "    flat = detrend_gentle(lc)\n",
    "    per_sector.append((s, flat))\n",
    "\n",
    "if not per_sector:\n",
    "    raise SystemExit(\"No usable PDCSAP sectors found. Provide SECTORS or check network.\")\n",
    "\n",
    "# ---- Per-sector scans --------------------------------------------------------\n",
    "stitched = None\n",
    "toprows = []\n",
    "for s, flat in per_sector:\n",
    "    print(f\"[scan] Sector {s}: N={len(flat.time)}\")\n",
    "    res, periods, durations = bls_scan(flat.time.value, flat.flux.value)\n",
    "    # top-3 by power\n",
    "    order = np.argsort(res.power)[::-1]\n",
    "    top_idx = order[:3]\n",
    "    tops = []\n",
    "    for i in top_idx:\n",
    "        tops.append({\"period\": float(res.period[i]), \"power\": float(res.power[i]),\n",
    "                     \"duration_d\": float(res.duration[i])})\n",
    "    # Save BLS periodogram\n",
    "    plt.figure(figsize=(7,3.0))\n",
    "    plt.plot(res.period, res.power, lw=1)\n",
    "    plt.xlabel(\"Period (days)\"); plt.ylabel(\"BLS power\")\n",
    "    plt.title(f\"{TARGET_NAME} TIC {TARGET_TIC} — Sector {s} BLS\")\n",
    "    plt.tight_layout()\n",
    "    bls_png = f\"figures/TIC{TARGET_TIC}_S{s}_BLS_periodogram.png\"\n",
    "    plt.savefig(bls_png, dpi=180); plt.close()\n",
    "\n",
    "    # TLS narrow around each top BLS peak\n",
    "    tls_rows = []\n",
    "    for j, trow in enumerate(tops):\n",
    "        tls = tls_scan_narrow(flat.time.value, flat.flux.value, trow[\"period\"],\n",
    "                              frac=TLS_NARROW_FRAC, min_transits=TLS_MIN_TRANSITS_SECTOR)\n",
    "        if tls is not None:\n",
    "            # quick fold plot\n",
    "            fold_plot(flat.time.value, flat.flux.value, tls[\"period\"], tls[\"t0\"],\n",
    "                      title=f\"{TARGET_NAME} TIC {TARGET_TIC} — S{s} TLS fold @ {tls['period']:.5f} d\",\n",
    "                      outpath=f\"figures/TIC{TARGET_TIC}_S{s}_TLS_fold_P{tls['period']:.5f}.png\")\n",
    "            tls_rows.append(tls)\n",
    "\n",
    "    # write CSVs\n",
    "    bls_df = pd.DataFrame(tops)\n",
    "    bls_df.to_csv(f\"results/TIC{TARGET_TIC}_S{s}_BLS_top3.csv\", index=False)\n",
    "    if tls_rows:\n",
    "        pd.DataFrame(tls_rows).to_csv(f\"results/TIC{TARGET_TIC}_S{s}_TLS_top3.csv\", index=False)\n",
    "\n",
    "    stitched = flat if stitched is None else stitched.append(flat)\n",
    "\n",
    "# ---- Stitched scan -----------------------------------------------------------\n",
    "print(f\"[scan] Stitched: N={len(stitched.time)}\")\n",
    "res, periods, durations = bls_scan(stitched.time.value, stitched.flux.value)\n",
    "\n",
    "# top-3 stitched\n",
    "order = np.argsort(res.power)[::-1]\n",
    "top_idx = order[:3]\n",
    "tops_st = [{\"period\": float(res.period[i]), \"power\": float(res.power[i]),\n",
    "            \"duration_d\": float(res.duration[i])} for i in top_idx]\n",
    "\n",
    "plt.figure(figsize=(7.5,3.0))\n",
    "plt.plot(res.period, res.power, lw=1)\n",
    "plt.xlabel(\"Period (days)\"); plt.ylabel(\"BLS power\")\n",
    "plt.title(f\"{TARGET_NAME} TIC {TARGET_TIC} — Stitched BLS\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/TIC{TARGET_TIC}_stitched_BLS_periodogram.png\", dpi=180); plt.close()\n",
    "\n",
    "tls_rows_st = []\n",
    "best_fold_path = None\n",
    "best_tls = None\n",
    "for trow in tops_st:\n",
    "    tls = tls_scan_narrow(stitched.time.value, stitched.flux.value, trow[\"period\"],\n",
    "                          frac=TLS_NARROW_FRAC, min_transits=TLS_MIN_TRANSITS_STITCH)\n",
    "    if tls is None: \n",
    "        continue\n",
    "    outpng = f\"figures/TIC{TARGET_TIC}_stitched_TLS_fold_P{tls['period']:.5f}.png\"\n",
    "    fold_plot(stitched.time.value, stitched.flux.value, tls[\"period\"], tls[\"t0\"],\n",
    "              title=f\"{TARGET_NAME} TIC {TARGET_TIC} — Stitched TLS fold @ {tls['period']:.5f} d\",\n",
    "              outpath=outpng)\n",
    "    tls_rows_st.append(tls)\n",
    "    if best_tls is None or tls[\"sde\"] > best_tls[\"sde\"]:\n",
    "        best_tls, best_fold_path = tls, outpng\n",
    "\n",
    "pd.DataFrame(tops_st).to_csv(f\"results/TIC{TARGET_TIC}_stitched_BLS_top3.csv\", index=False)\n",
    "if tls_rows_st:\n",
    "    pd.DataFrame(tls_rows_st).to_csv(f\"results/TIC{TARGET_TIC}_stitched_TLS_top3.csv\", index=False)\n",
    "\n",
    "# ---- Fast odd/even + secondary on best stitched TLS -------------------------\n",
    "qc_row = {}\n",
    "if best_tls is not None:\n",
    "    est_dur_hr = max(0.5, min(3.0, best_tls[\"duration_d\"]*24.0))\n",
    "    qc_row = odd_even_secondary_checks(stitched.time.value, stitched.flux.value,\n",
    "                                       best_tls[\"period\"], best_tls[\"t0\"], est_dur_hr,\n",
    "                                       tag=\"stitched\")\n",
    "    qc_row.update({\"period_best\": best_tls[\"period\"], \"t0_best\": best_tls[\"t0\"],\n",
    "                   \"sde_best\": best_tls[\"sde\"], \"duration_d_best\": best_tls[\"duration_d\"],\n",
    "                   \"fold_png\": best_fold_path})\n",
    "    pd.DataFrame([qc_row]).to_csv(f\"results/TIC{TARGET_TIC}_stitched_quick_qc.csv\", index=False)\n",
    "\n",
    "# ---- Console summary ---------------------------------------------------------\n",
    "print(\"\\n=== Quick Scan Summary ===\")\n",
    "print(f\"TIC {TARGET_TIC}  ({TARGET_NAME})\")\n",
    "print(f\" Sectors used: {[s for s,_ in per_sector]}\")\n",
    "if tls_rows_st:\n",
    "    print(\" Stitched TLS top (by SDE):\")\n",
    "    print(pd.DataFrame(tls_rows_st).sort_values(\"sde\", ascending=False).head(3))\n",
    "else:\n",
    "    print(\" TLS unavailable or no stitched TLS peaks found.\")\n",
    "if qc_row:\n",
    "    print(\"\\n Quick QC @ best stitched period:\")\n",
    "    print(qc_row)\n",
    "print(\"\\nSaved CSVs in results/ and plots in figures/. Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f944efb1-0720-4d99-b585-5adf052ec4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Target D (offline selection)\n",
      "-----------------------------------\n",
      "Source table : results/targets_with_sectors.csv\n",
      "Name         : 1487.01\n",
      "TIC          : 459978312\n",
      "Sectors(list): — (not in table)\n",
      "N sectors    : 0\n",
      "\n",
      "Next steps:\n",
      " - Run your loader in auto-discovery mode (SPOC first, else TESSCut) since the table lacks sector info.\n"
     ]
    }
   ],
   "source": [
    "# %% Offline Target D picker (no network): use local tables & stored sector lists\n",
    "import os, ast, pandas as pd\n",
    "\n",
    "# Skip these (A–C)\n",
    "USED_TICS = {119584412, 37749396, 311183180}\n",
    "\n",
    "# Order of preference for local tables\n",
    "LOCAL_TABLES = [\n",
    "    \"results/targets_with_sectors.csv\",  # best: has sector lists\n",
    "    \"results/priority_targets.csv\",\n",
    "    \"results/targets_ranked.csv\",\n",
    "    \"results/targets.csv\",\n",
    "]\n",
    "\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            df = pd.read_csv(p)\n",
    "            if len(df):\n",
    "                return p, df\n",
    "    return None, None\n",
    "\n",
    "src, df = _first_existing(LOCAL_TABLES)\n",
    "if df is None:\n",
    "    raise SystemExit(\"No local target tables found. Expected one of:\\n  - \" + \"\\n  - \".join(LOCAL_TABLES))\n",
    "\n",
    "# Normalize columns\n",
    "lower = {c.lower(): c for c in df.columns}\n",
    "tic_col   = lower.get(\"tic\") or lower.get(\"tic_id\") or lower.get(\"ticid\") or lower.get(\"tic_id_norm\")\n",
    "name_col  = lower.get(\"name\") or lower.get(\"toi\") or lower.get(\"target\") or lower.get(\"designation\")\n",
    "sectors_c = None\n",
    "for k in (\"sectors_now\",\"sectors\",\"observed_sectors\",\"sector_list\"):\n",
    "    if k in lower:\n",
    "        sectors_c = lower[k]; break\n",
    "if tic_col is None:\n",
    "    raise SystemExit(f\"No TIC-like column in {src}. Columns: {list(df.columns)}\")\n",
    "\n",
    "# Build pool: drop A–C, dedupe, keep valid TIC\n",
    "df[\"__tic\"] = pd.to_numeric(df[tic_col], errors=\"coerce\").astype(\"Int64\")\n",
    "pool = df.dropna(subset=[\"__tic\"]).drop_duplicates(subset=\"__tic\", keep=\"first\")\n",
    "pool = pool[~pool[\"__tic\"].isin(USED_TICS)].copy()\n",
    "\n",
    "def _parse_sectors(x):\n",
    "    # Accept \"[14, 15]\" or \"14,15\" or list; return list[int]\n",
    "    if pd.isna(x): return []\n",
    "    if isinstance(x, (list,tuple)): return [int(s) for s in x if str(s).strip()!=\"\"]\n",
    "    s = str(x).strip()\n",
    "    try:\n",
    "        # Try literal (e.g., \"[14, 15]\")\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, (list,tuple)):\n",
    "            return [int(v) for v in val if str(v).strip()!=\"\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: split by comma\n",
    "    return [int(t) for t in s.strip(\"[]\").split(\",\") if t.strip().isdigit()]\n",
    "\n",
    "if sectors_c is not None:\n",
    "    pool[\"__sectors\"] = pool[sectors_c].map(_parse_sectors)\n",
    "    pool[\"__nsec\"]    = pool[\"__sectors\"].map(len)\n",
    "    # Prefer more sectors, then keep current row order as tiebreak\n",
    "    pool = pool.sort_values([\"__nsec\"], ascending=False).reset_index(drop=True)\n",
    "else:\n",
    "    pool[\"__sectors\"] = [[] for _ in range(len(pool))]\n",
    "    pool[\"__nsec\"]    = 0\n",
    "\n",
    "if pool.empty:\n",
    "    raise SystemExit(\"After skipping A–C, no candidates remain in local table(s).\")\n",
    "\n",
    "# Pick first with ≥2 sectors if available; else first with ≥1; else first row\n",
    "picked = None\n",
    "for thresh in (2, 1, 0):\n",
    "    sub = pool[pool[\"__nsec\"] >= thresh]\n",
    "    if len(sub):\n",
    "        picked = sub.iloc[0]\n",
    "        break\n",
    "\n",
    "TARGET_NAME = \"Target D\"\n",
    "TARGET_TIC  = int(picked[\"__tic\"])\n",
    "SECTORS     = list(picked[\"__sectors\"]) if isinstance(picked[\"__sectors\"], list) else []\n",
    "CAND_NAME   = str(picked.get(name_col, f\"TIC {TARGET_TIC}\")) if name_col else f\"TIC {TARGET_TIC}\"\n",
    "\n",
    "print(\"Chosen Target D (offline selection)\")\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"Source table : {src}\")\n",
    "print(f\"Name         : {CAND_NAME}\")\n",
    "print(f\"TIC          : {TARGET_TIC}\")\n",
    "print(f\"Sectors(list): {SECTORS if SECTORS else '— (not in table)'}\")\n",
    "print(f\"N sectors    : {len(SECTORS)}\")\n",
    "print(\"\\nNext steps:\")\n",
    "if len(SECTORS) >= 1:\n",
    "    print(\" - Run your PDCSAP-first loader with SECTORS as hints; if a sector lacks PDCSAP, fall back to TESSCut.\")\n",
    "else:\n",
    "    print(\" - Run your loader in auto-discovery mode (SPOC first, else TESSCut) since the table lacks sector info.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca726e6-b499-4f1f-a3ac-303f0817ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Target D (offline, sector-aware)\n",
      "---------------------------------------\n",
      "Source table : results/targets_with_sectors.csv\n",
      "Name         : 1487.01\n",
      "TIC          : 459978312\n",
      "Sectors(list): — (none found in table)\n",
      "N sectors    : 0\n",
      "\n",
      "Next steps:\n",
      " - Use your PDCSAP-first loader on these sectors (or auto-discover if empty).\n"
     ]
    }
   ],
   "source": [
    "# %% Re-pick Target D, scanning ANY 'sector*' column in the local table (offline)\n",
    "import os, ast, pandas as pd\n",
    "\n",
    "USED_TICS = {119584412, 37749396, 311183180}\n",
    "src = \"results/targets_with_sectors.csv\"\n",
    "if not os.path.exists(src):\n",
    "    raise SystemExit(f\"Missing {src}\")\n",
    "\n",
    "df = pd.read_csv(src)\n",
    "lower = {c.lower(): c for c in df.columns}\n",
    "tic_col  = lower.get(\"tic\") or lower.get(\"tic_id\") or lower.get(\"ticid\") or lower.get(\"tic_id_norm\")\n",
    "name_col = lower.get(\"name\") or lower.get(\"toi\") or lower.get(\"target\") or lower.get(\"designation\")\n",
    "if tic_col is None:\n",
    "    raise SystemExit(f\"No TIC-like column in {src}. Columns: {list(df.columns)}\")\n",
    "\n",
    "# Build pool (skip A–C)\n",
    "df[\"__tic\"] = pd.to_numeric(df[tic_col], errors=\"coerce\").astype(\"Int64\")\n",
    "pool = (df.dropna(subset=[\"__tic\"])\n",
    "          .drop_duplicates(subset=\"__tic\", keep=\"first\")\n",
    "          [~df[\"__tic\"].isin(USED_TICS)].copy())\n",
    "\n",
    "# Find ANY sectorish column\n",
    "sectorish_cols = [c for c in df.columns if \"sector\" in c.lower()]\n",
    "def _parse_sectors(x):\n",
    "    if pd.isna(x): return []\n",
    "    if isinstance(x, (list, tuple)): return [int(s) for s in x if str(s).strip()!=\"\"]\n",
    "    s = str(x).strip()\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, (list,tuple)):\n",
    "            return [int(v) for v in val if str(v).strip()!=\"\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [int(t) for t in s.strip(\"[]\").split(\",\") if t.strip().isdigit()]\n",
    "\n",
    "pool[\"__sectors\"] = [[] for _ in range(len(pool))]\n",
    "for c in sectorish_cols:\n",
    "    # prefer the first sectorish column that yields non-empty lists\n",
    "    parsed = pool[c].map(_parse_sectors)\n",
    "    if parsed.map(len).sum() > pool[\"__sectors\"].map(len).sum():\n",
    "        pool[\"__sectors\"] = parsed\n",
    "\n",
    "pool[\"__nsec\"] = pool[\"__sectors\"].map(len)\n",
    "# Prefer lots of sectors; tie-break by table order\n",
    "pool = pool.sort_values(\"__nsec\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Pick with threshold 2→1→0\n",
    "picked = None\n",
    "for t in (2,1,0):\n",
    "    sub = pool[pool[\"__nsec\"]>=t]\n",
    "    if len(sub):\n",
    "        picked = sub.iloc[0]; break\n",
    "\n",
    "TARGET_TIC  = int(picked[\"__tic\"])\n",
    "TARGET_NAME = \"Target D\"\n",
    "CAND_NAME   = str(picked.get(name_col, f\"TIC {TARGET_TIC}\")) if name_col else f\"TIC {TARGET_TIC}\"\n",
    "SECTORS     = list(picked[\"__sectors\"]) if isinstance(picked[\"__sectors\"], list) else []\n",
    "\n",
    "print(\"Chosen Target D (offline, sector-aware)\")\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"Source table : {src}\")\n",
    "print(f\"Name         : {CAND_NAME}\")\n",
    "print(f\"TIC          : {TARGET_TIC}\")\n",
    "print(f\"Sectors(list): {SECTORS if SECTORS else '— (none found in table)'}\")\n",
    "print(f\"N sectors    : {len(SECTORS)}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\" - Use your PDCSAP-first loader on these sectors (or auto-discover if empty).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76beb82f-986a-4054-8645-003554425222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:18:17] Check 1487.01  TIC 459978312  (hint sectors: —)\n",
      "[20:18:18] Check 5789.01  TIC 87216634  (hint sectors: —)\n",
      "[20:18:18] Check 186.01  TIC 279741379  (hint sectors: —)\n",
      "[20:18:18] Check 2009.01  TIC 243187830  (hint sectors: —)\n",
      "[20:18:19] Check 197.01  TIC 441462736  (hint sectors: —)\n",
      "[20:18:19] Check 2194.01  TIC 271478281  (hint sectors: —)\n",
      "[20:18:19] Check 1611.01  TIC 264678534  (hint sectors: —)\n",
      "[20:18:19] Check 4328.01  TIC 77175217  (hint sectors: —)\n",
      "[20:18:19] Check 2134.02  TIC 75878355  (hint sectors: —)\n",
      "[20:18:20] Check 1793.01  TIC 304142124  (hint sectors: —)\n",
      "[20:18:20] Check 7032.01  TIC 22903436  (hint sectors: —)\n",
      "[20:18:20] Check 431.01  TIC 31374837  (hint sectors: —)\n",
      "[20:18:20] Check 2540.01  TIC 354518617  (hint sectors: —)\n",
      "[20:18:20] Check 2443.01  TIC 318753380  (hint sectors: —)\n",
      "[20:18:20] Check 402.02  TIC 120896927  (hint sectors: —)\n",
      "[20:18:20] Check 2076.01  TIC 27491137  (hint sectors: —)\n",
      "[20:18:20] Check 1405.01  TIC 387834907  (hint sectors: —)\n",
      "[20:18:21] Check 2023.01  TIC 16884216  (hint sectors: —)\n",
      "[20:18:21] Check 560.02  TIC 101011575  (hint sectors: —)\n",
      "[20:18:21] Check 1643.01  TIC 298647682  (hint sectors: —)\n",
      "[20:18:42] Check 174.03  TIC 425997655  (hint sectors: —)\n",
      "[20:21:26] Check 7062.01  TIC 23961340  (hint sectors: —)\n",
      "[20:22:04] Check 461.01  TIC 4646810  (hint sectors: —)\n",
      "[20:22:09] Check 1947.01  TIC 460140752  (hint sectors: —)\n",
      "[20:22:44] Check 1824.01  TIC 142387023  (hint sectors: —)\n",
      "[20:23:34] Check 6726.01  TIC 121490076  (hint sectors: —)\n",
      "[20:24:27] Check 866.01  TIC 381976956  (hint sectors: —)\n",
      "[20:26:22] Check 6249.02  TIC 456260074  (hint sectors: —)\n",
      "[20:26:53] Check 1255.01  TIC 237222864  (hint sectors: —)\n",
      "[21:27:45] Check 880.03  TIC 34077285  (hint sectors: —)\n",
      "[21:28:18] Check 5396.01  TIC 202428245  (hint sectors: —)\n",
      "[21:29:07] Check 815.01  TIC 102840239  (hint sectors: —)\n",
      "[21:29:25] Check 139.01  TIC 62483237  (hint sectors: —)\n",
      "[21:29:42] Check 2459.01  TIC 192790476  (hint sectors: —)\n",
      "[21:30:17] Check 6075.01  TIC 424388628  (hint sectors: —)\n",
      "[21:35:04] Check 1563.01  TIC 249945230  (hint sectors: —)\n",
      "[21:36:17] Check 454.01  TIC 153077621  (hint sectors: —)\n",
      "[21:36:33] Check 4310.01  TIC 303317324  (hint sectors: —)\n",
      "[21:36:53] Check 1030.01  TIC 464296022  (hint sectors: —)\n",
      "[21:37:21] Check 3485.01  TIC 394672497  (hint sectors: —)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "No suitable Target D found.\nTried 40 rows.\nTips:\n - Increase MAX_TRY_ROWS and/or TIME_LIMIT_S.\n - Keep REQUIRE_SECT=1 (we will use TESSCut if SPOC missing).\n - Or do a manual pick from results/targets_with_sectors.csv (choose a TIC with ≥2 sectors).",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m No suitable Target D found.\nTried 40 rows.\nTips:\n - Increase MAX_TRY_ROWS and/or TIME_LIMIT_S.\n - Keep REQUIRE_SECT=1 (we will use TESSCut if SPOC missing).\n - Or do a manual pick from results/targets_with_sectors.csv (choose a TIC with ≥2 sectors).\n"
     ]
    }
   ],
   "source": [
    "# %% Auto-pick & quick-scan Target D (robust + speed mode): find a candidate WITH data, then BLS→TLS\n",
    "import os, time, math, warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares as BLS\n",
    "\n",
    "# ---- OPTIONAL TLS ----\n",
    "try:\n",
    "    from transitleastsquares import transitleastsquares as TLS\n",
    "    HAVE_TLS = True\n",
    "except Exception:\n",
    "    HAVE_TLS = False\n",
    "\n",
    "# ===========================\n",
    "# SPEED MODE TOGGLES (edit me)\n",
    "# ===========================\n",
    "SPEED_MODE = True        # <- flip to False for a deeper scan\n",
    "TLS_ENABLE = False       # <- keep False for fastest; set True to run TLS in ±1% windows\n",
    "\n",
    "# ---- BASE CONFIG (defaults; may be overridden by SPEED MODE) ----\n",
    "USED_TICS     = {119584412, 37749396, 311183180}   # skip A–C\n",
    "TABLE_CANDIDATES = [\n",
    "    \"results/targets_with_sectors.csv\",\n",
    "    \"results/priority_targets.csv\",\n",
    "    \"results/targets_ranked.csv\",\n",
    "]\n",
    "REQUIRE_SECT  = 2            # prefer ≥2 sectors; SPEED_MODE may relax to 1\n",
    "MAX_TRY_ROWS  = 200          # how many candidates to test before giving up\n",
    "MAX_SPOC_SECT = 6            # cap SPOC sectors per candidate for speed\n",
    "MAX_TCUT_SECT = 3            # cap TESSCut sectors per candidate for speed\n",
    "WINDOW_DAYS   = 0.9          # gentle detrend window\n",
    "BLS_PMIN, BLS_PMAX = 0.5, 50.0\n",
    "BLS_NPER      = 5000         # number of BLS trial periods\n",
    "DURATIONS_H   = np.linspace(0.5, 3.0, 18)\n",
    "TLS_N_TOP_BLS = 3\n",
    "TLS_FRAC_WIN  = 0.01         # ±1%\n",
    "TLS_OVERSAMP  = 5            # TLS oversampling factor\n",
    "\n",
    "# === Runtime guards (add under CONFIG) ===\n",
    "TIME_LIMIT_S = 12000        # hard wall-clock cap (~20 minutes); tune as you like\n",
    "t_start = time.time()\n",
    "def _timed_out():\n",
    "    return (time.time() - t_start) > TIME_LIMIT_S\n",
    "\n",
    "# ---- SPEED MODE OVERRIDES ----\n",
    "if SPEED_MODE:\n",
    "    REQUIRE_SECT  = 1\n",
    "    MAX_TRY_ROWS  = 40\n",
    "    MAX_SPOC_SECT = 2\n",
    "    MAX_TCUT_SECT = 1\n",
    "    WINDOW_DAYS   = 0.8\n",
    "    BLS_NPER      = 2000\n",
    "    DURATIONS_H   = np.linspace(0.75, 2.0, 8)\n",
    "    TLS_N_TOP_BLS = 1\n",
    "    TLS_FRAC_WIN  = 0.01\n",
    "    TLS_OVERSAMP  = 3\n",
    "    if not HAVE_TLS:\n",
    "        TLS_ENABLE = False  # no TLS installed\n",
    "    else:\n",
    "        TLS_ENABLE = bool(TLS_ENABLE)  # respect your toggle above\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "def _print(msg): print(time.strftime(\"[%H:%M:%S]\"), msg)\n",
    "\n",
    "def _parse_sector_list(val):\n",
    "    if isinstance(val, (list, tuple)): return [int(x) for x in val]\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip().strip(\"[]\")\n",
    "        if not s: return []\n",
    "        out = []\n",
    "        for tok in s.split(\",\"):\n",
    "            tok = tok.strip()\n",
    "            if tok:\n",
    "                try: out.append(int(tok))\n",
    "                except: pass\n",
    "        return out\n",
    "    return []\n",
    "\n",
    "def _discover_spoc_sectors(tic):\n",
    "    if _timed_out(): return []\n",
    "    sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\")\n",
    "    secs = sorted({getattr(r, \"sector\", None) for r in sr if getattr(r, \"sector\", None) is not None})\n",
    "    return secs[:MAX_SPOC_SECT]\n",
    "\n",
    "def _discover_tesscut_sectors(tic):\n",
    "    if _timed_out(): return []\n",
    "    try:\n",
    "        sr = lk.search_tesscut(f\"TIC {tic}\")\n",
    "        secs = sorted({getattr(r, \"sector\", None) for r in sr if getattr(r, \"sector\", None) is not None})\n",
    "        return secs[-MAX_TCUT_SECT:]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _download_spoc_pdcsap(tic, sector):\n",
    "    if _timed_out(): return None\n",
    "    try:\n",
    "        sr = lk.search_lightcurve(f\"TIC {tic}\", mission=\"TESS\", author=\"SPOC\", sector=sector)\n",
    "        if len(sr) == 0: return None\n",
    "        lc = sr.download(flux_column=\"pdcsap_flux\")\n",
    "        if lc is None or len(lc.time.value) == 0: return None\n",
    "        return lc.remove_nans()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _download_tesscut_lc(tic, sector, cutout_size=15):\n",
    "    if _timed_out(): return None\n",
    "    try:\n",
    "        sr = lk.search_tesscut(f\"TIC {tic}\", sector=sector)\n",
    "        if len(sr) == 0: return None\n",
    "        tpf = sr.download(cutout_size=cutout_size)\n",
    "        if tpf is None: return None\n",
    "        try:\n",
    "            ap = tpf.create_threshold_mask(threshold=3)\n",
    "            if ap.sum() == 0: ap = None\n",
    "        except Exception:\n",
    "            ap = None\n",
    "        lc = tpf.to_lightcurve(aperture_mask=ap) if ap is not None else tpf.to_lightcurve()\n",
    "        return lc.remove_nans()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _gentle_flatten(lc, window_days=WINDOW_DAYS):\n",
    "    t = lc.time.value\n",
    "    if len(t) < 10: return lc.normalize()\n",
    "    dt = np.nanmedian(np.diff(np.sort(t)))\n",
    "    wl = int(max(51, window_days/dt))\n",
    "    if wl % 2 == 0: wl += 1\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            fl = lc.remove_outliers(sigma=6.0).flatten(window_length=wl, polyorder=2)\n",
    "        except Exception:\n",
    "            fl = lc.normalize()\n",
    "    return fl.remove_nans()\n",
    "\n",
    "def _run_bls(t, f):\n",
    "    m = np.isfinite(t) & np.isfinite(f)\n",
    "    t, f = t[m], f[m]\n",
    "    if len(t) < 300: return None\n",
    "    span = t.max() - t.min()\n",
    "    pmax_eff = max(1.01, min(BLS_PMAX, span/2.0))\n",
    "    periods = np.exp(np.linspace(np.log(BLS_PMIN), np.log(pmax_eff), BLS_NPER))\n",
    "    model = BLS(t, f)\n",
    "    res = model.power(periods, DURATIONS_H/24.0)\n",
    "    return dict(periods=periods, power=res.power, bls=res, model=model,\n",
    "                best_period=periods[np.argmax(res.power)])\n",
    "\n",
    "def _top_peaks(periods, power, n=3, sep=0.02):\n",
    "    idx = np.argsort(power)[::-1]\n",
    "    picks = []\n",
    "    for i in idx:\n",
    "        p = periods[i]\n",
    "        if all(abs(p - q)/q > sep for q,_ in picks):\n",
    "            picks.append((p, power[i]))\n",
    "        if len(picks) >= n: break\n",
    "    return picks\n",
    "\n",
    "def _run_tls_narrow(t, f, p0):\n",
    "    if not (HAVE_TLS and TLS_ENABLE): return None\n",
    "    m = np.isfinite(t) & np.isfinite(f); t, f = t[m], f[m]\n",
    "    if len(t) < 300: return None\n",
    "    tls = TLS(t, f)\n",
    "    return tls.power(period_min=p0*(1-TLS_FRAC_WIN), period_max=p0*(1+TLS_FRAC_WIN),\n",
    "                     use_threads='auto', oversampling_factor=TLS_OVERSAMP)\n",
    "\n",
    "def _plot_bls(periods, power, out_png, title):\n",
    "    plt.figure(figsize=(7.2,3.8))\n",
    "    plt.plot(periods, power, lw=1)\n",
    "    plt.xlabel(\"Period [days]\"); plt.ylabel(\"BLS power\")\n",
    "    plt.title(title); plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "def _plot_fold(t, f, period, t0, out_png, title):\n",
    "    m = np.isfinite(t) & np.isfinite(f); t, f = t[m], f[m]\n",
    "    phase = ((t - t0 + 0.5*period) % period) / period - 0.5\n",
    "    nb = 100\n",
    "    bins = np.linspace(-0.5, 0.5, nb+1)\n",
    "    idx = np.digitize(phase, bins) - 1\n",
    "    y = np.array([np.nanmean(f[idx==k]) for k in range(nb)])\n",
    "    x = 0.5*(bins[:-1]+bins[1:])\n",
    "    plt.figure(figsize=(6.6,3.6))\n",
    "    plt.scatter(phase, f, s=2, alpha=0.25, rasterized=True)\n",
    "    plt.plot(x, y, lw=2)\n",
    "    plt.xlabel(\"Phase\"); plt.ylabel(\"Normalized flux\"); plt.title(title)\n",
    "    plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "# ---- Load a candidates table ----\n",
    "cand = None\n",
    "src = None\n",
    "for p in TABLE_CANDIDATES:\n",
    "    if os.path.exists(p):\n",
    "        cand = pd.read_csv(p); src = p; break\n",
    "if cand is None or cand.empty:\n",
    "    raise SystemExit(\"No candidate table found. Expected one of:\\n  - \" + \"\\n  - \".join(TABLE_CANDIDATES))\n",
    "\n",
    "# Normalize columns\n",
    "cols = {c.lower(): c for c in cand.columns}\n",
    "tic_col  = cols.get(\"tic\") or cols.get(\"tic_id\") or cols.get(\"ticid\") or cols.get(\"tic_id_norm\")\n",
    "name_col = cols.get(\"name\") or cols.get(\"toi\") or cols.get(\"target\") or cols.get(\"designation\")\n",
    "sects_col = cols.get(\"sectors_now\") or cols.get(\"sectors\") or cols.get(\"observed_sectors\") or cols.get(\"sector_list\")\n",
    "if tic_col is None: raise SystemExit(f\"No TIC column in {src}\")\n",
    "\n",
    "cand[\"__tic\"] = pd.to_numeric(cand[tic_col], errors=\"coerce\").astype(\"Int64\")\n",
    "pool = cand.dropna(subset=[\"__tic\"])\n",
    "pool = pool[~pool[\"__tic\"].isin(USED_TICS)]\n",
    "\n",
    "# Prefer many sectors if a list exists\n",
    "if sects_col:\n",
    "    pool = pool.assign(__nsec=pool[sects_col].map(_parse_sector_list).map(len))\n",
    "    pool = pool.sort_values(\"__nsec\", ascending=False)\n",
    "\n",
    "# ---- Try candidates until one has data ----\n",
    "picked = None\n",
    "rows_tried = 0\n",
    "for _, r in pool.head(MAX_TRY_ROWS).iterrows():\n",
    "    if _timed_out():\n",
    "        _print(\"Time limit reached — stopping auto-pick loop.\")\n",
    "        break\n",
    "    rows_tried += 1\n",
    "    tic = int(r[\"__tic\"])\n",
    "    name = str(r.get(name_col, f\"TIC {tic}\"))\n",
    "    hint_secs = _parse_sector_list(r[sects_col]) if sects_col else []\n",
    "    _print(f\"Check {name}  TIC {tic}  (hint sectors: {hint_secs if hint_secs else '—'})\")\n",
    "\n",
    "    spoc_secs = _discover_spoc_sectors(tic)\n",
    "    cut_secs  = _discover_tesscut_sectors(tic)\n",
    "    all_secs  = list(dict.fromkeys(hint_secs + spoc_secs + cut_secs))\n",
    "    if REQUIRE_SECT and len(all_secs) < REQUIRE_SECT:\n",
    "        continue\n",
    "\n",
    "    lcs, used, src_kind = [], [], []\n",
    "    tried = all_secs if all_secs else spoc_secs or cut_secs\n",
    "    tried = tried[:max(MAX_SPOC_SECT, MAX_TCUT_SECT)]\n",
    "    for s in tried:\n",
    "        if _timed_out():\n",
    "            _print(\"Time limit reached during downloads — stopping.\")\n",
    "            break\n",
    "        lc = _download_spoc_pdcsap(tic, s)\n",
    "        if lc is not None and len(lc.time.value) > 300:\n",
    "            lcs.append(lc); used.append(s); src_kind.append(\"SPOC\"); continue\n",
    "        lc = _download_tesscut_lc(tic, s)\n",
    "        if lc is not None and len(lc.time.value) > 300:\n",
    "            lcs.append(lc); used.append(s); src_kind.append(\"TESSCut\")\n",
    "    if len(lcs) == 0:\n",
    "        continue\n",
    "\n",
    "    picked = dict(tic=tic, name=name, sectors=used, sources=sorted(set(src_kind)))\n",
    "    break\n",
    "\n",
    "if picked is None:\n",
    "    msg = []\n",
    "    if _timed_out():\n",
    "        msg.append(\"Stopped due to time cap.\")\n",
    "    msg.append(f\"No suitable Target D found.\\nTried {rows_tried} rows.\")\n",
    "    msg.append(\"Tips:\\n - Increase MAX_TRY_ROWS and/or TIME_LIMIT_S.\\n - Keep REQUIRE_SECT=1 (we will use TESSCut if SPOC missing).\\n - Or do a manual pick from results/targets_with_sectors.csv (choose a TIC with ≥2 sectors).\")\n",
    "    raise SystemExit(\"\\n\".join(msg))\n",
    "\n",
    "# ---- Detrend, stitch, search ----\n",
    "TARGET_TIC, TARGET_NAME = picked[\"tic\"], picked[\"name\"]\n",
    "_print(f\"\\nChosen Target D: {TARGET_NAME} — TIC {TARGET_TIC}\")\n",
    "_print(f\"Sectors used: {picked['sectors']}  (sources: {picked['sources']})\")\n",
    "\n",
    "flats = []\n",
    "for s, lc in zip(picked[\"sectors\"], lcs):\n",
    "    fl = _gentle_flatten(lc, WINDOW_DAYS)\n",
    "    try: fl = fl.normalize()\n",
    "    except Exception: pass\n",
    "    fl.meta[\"sector\"] = s\n",
    "    flats.append(fl)\n",
    "\n",
    "stitch = lk.LightCurveCollection(flats).stitch().remove_nans()\n",
    "t, f = stitch.time.value, stitch.flux.value\n",
    "\n",
    "_print(\"Running BLS (wide)…\")\n",
    "bls = _run_bls(t, f)\n",
    "if bls is None: raise SystemExit(\"BLS failed (too few points).\")\n",
    "\n",
    "top = _top_peaks(bls[\"periods\"], bls[\"power\"], n=TLS_N_TOP_BLS)\n",
    "bls_png = f\"figures/TIC{TARGET_TIC}_quickscan_BLS.png\"\n",
    "_plot_bls(bls[\"periods\"], bls[\"power\"], bls_png, f\"{TARGET_NAME} — BLS (wide)\")\n",
    "\n",
    "tls_rows, fold_pngs = [], []\n",
    "if HAVE_TLS and TLS_ENABLE:\n",
    "    _print(f\"Running TLS in ±{int(TLS_FRAC_WIN*100)}% windows around top BLS peaks…\")\n",
    "    for i, (p0, pow0) in enumerate(top, 1):\n",
    "        res = _run_tls_narrow(t, f, p0)\n",
    "        if res is None: continue\n",
    "        tls_rows.append({\n",
    "            \"rank\": i,\n",
    "            \"p0_bls_days\": float(p0),\n",
    "            \"tls_best_period_days\": float(res.period),\n",
    "            \"tls_sde\": float(res.SDE),\n",
    "            \"tls_depth_ppm\": float(1e6*res.depth),\n",
    "            \"tls_duration_hr\": float(24.0*res.duration),\n",
    "            \"tls_transit_count\": int(res.transit_count),\n",
    "        })\n",
    "        fold_png = f\"figures/TIC{TARGET_TIC}_quickscan_TLS_fold_rank{i}.png\"\n",
    "        _plot_fold(t, f, res.period, t0=np.nanmin(t), out_png=fold_png,\n",
    "                   title=f\"{TARGET_NAME} — TLS fold (rank {i}, P≈{res.period:.5f} d)\")\n",
    "        fold_pngs.append(fold_png)\n",
    "else:\n",
    "    _print(\"TLS disabled (speed mode) or not installed — skipping TLS step (BLS results still saved).\")\n",
    "\n",
    "# ---- Save tables & summary ----\n",
    "bls_top = pd.DataFrame(\n",
    "    [{\"rank\": i, \"bls_period_days\": float(p), \"bls_power\": float(pow_)} for i,(p,pow_) in enumerate(top, 1)]\n",
    ")\n",
    "tls_df = pd.DataFrame(tls_rows) if len(tls_rows) else pd.DataFrame(\n",
    "    columns=[\"rank\",\"p0_bls_days\",\"tls_best_period_days\",\"tls_sde\",\"tls_depth_ppm\",\"tls_duration_hr\",\"tls_transit_count\"]\n",
    ")\n",
    "summary = pd.DataFrame([{\n",
    "    \"tic\": TARGET_TIC,\n",
    "    \"name\": TARGET_NAME,\n",
    "    \"n_sectors\": len(picked[\"sectors\"]),\n",
    "    \"sectors\": \",\".join(str(s) for s in picked[\"sectors\"]),\n",
    "    \"sources\": \",\".join(picked[\"sources\"]),\n",
    "    \"bls_best_period_days\": float(bls[\"best_period\"]),\n",
    "    \"tls_best_period_days\": float(tls_df.iloc[0][\"tls_best_period_days\"]) if len(tls_df) else np.nan,\n",
    "    \"tls_best_sde\": float(tls_df.iloc[0][\"tls_sde\"]) if len(tls_df) else np.nan,\n",
    "    \"speed_mode\": SPEED_MODE,\n",
    "    \"tls_enabled\": bool(HAVE_TLS and TLS_ENABLE),\n",
    "}])\n",
    "\n",
    "bls_csv = f\"results/TIC{TARGET_TIC}_quickscan_BLS_top.csv\"\n",
    "tls_csv = f\"results/TIC{TARGET_TIC}_quickscan_TLS_top.csv\"\n",
    "sum_csv = f\"results/TIC{TARGET_TIC}_quickscan_summary.csv\"\n",
    "bls_top.to_csv(bls_csv, index=False)\n",
    "tls_df.to_csv(tls_csv, index=False)\n",
    "summary.to_csv(sum_csv, index=False)\n",
    "\n",
    "_print(\"\\nQuick scan complete.\")\n",
    "_print(f\"Saved:\\n - {bls_png}\\n - {bls_csv}\\n - {tls_csv}\\n - {sum_csv}\")\n",
    "for p in fold_pngs: _print(f\" - {p})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adb28a29-aec5-4256-85f3-855572fd4581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source table: results/targets_with_sectors.csv\n",
      "Top candidates by sector count (skipping A–C):\n",
      " - toi    1487.01\n",
      "toi    1487.01\n",
      "Name: 0, dtype: object  TIC 459978312   sectors:0   list:—\n",
      " - toi    880.03\n",
      "toi    880.03\n",
      "Name: 31, dtype: object  TIC 34077285    sectors:0   list:—\n",
      " - toi    815.01\n",
      "toi    815.01\n",
      "Name: 33, dtype: object  TIC 102840239   sectors:0   list:—\n",
      " - toi    139.01\n",
      "toi    139.01\n",
      "Name: 34, dtype: object  TIC 62483237    sectors:0   list:—\n",
      " - toi    2459.01\n",
      "toi    2459.01\n",
      "Name: 35, dtype: object  TIC 192790476   sectors:0   list:—\n",
      " - toi    6075.01\n",
      "toi    6075.01\n",
      "Name: 36, dtype: object  TIC 424388628   sectors:0   list:—\n",
      " - toi    1563.01\n",
      "toi    1563.01\n",
      "Name: 37, dtype: object  TIC 249945230   sectors:0   list:—\n",
      " - toi    454.01\n",
      "toi    454.01\n",
      "Name: 38, dtype: object  TIC 153077621   sectors:0   list:—\n",
      " - toi    4310.01\n",
      "toi    4310.01\n",
      "Name: 39, dtype: object  TIC 303317324   sectors:0   list:—\n",
      " - toi    1030.01\n",
      "toi    1030.01\n",
      "Name: 40, dtype: object  TIC 464296022   sectors:0   list:—\n"
     ]
    }
   ],
   "source": [
    "# %% Shortlist: top-by-sectors (skip A–C)\n",
    "import pandas as pd, os\n",
    "USED_TICS = {119584412, 37749396, 311183180}\n",
    "paths = [\"results/targets_with_sectors.csv\",\"results/priority_targets.csv\",\"results/targets_ranked.csv\"]\n",
    "df, src = None, None\n",
    "for p in paths:\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_csv(p); src = p; break\n",
    "assert df is not None and not df.empty, \"No candidates table found.\"\n",
    "\n",
    "def _parse_sectors(val):\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip().strip(\"[]\")\n",
    "        if not s: return []\n",
    "        return [int(x) for x in s.split(\",\") if x.strip().isdigit()]\n",
    "    return []\n",
    "\n",
    "cols = {c.lower(): c for c in df.columns}\n",
    "tic_col = cols.get(\"tic\") or cols.get(\"tic_id\") or cols.get(\"ticid\") or cols.get(\"tic_id_norm\")\n",
    "name_col = cols.get(\"name\") or cols.get(\"toi\") or cols.get(\"target\") or cols.get(\"designation\")\n",
    "sect_col = cols.get(\"sectors_now\") or cols.get(\"sectors\") or cols.get(\"observed_sectors\") or cols.get(\"sector_list\")\n",
    "\n",
    "df[\"__tic\"] = pd.to_numeric(df[tic_col], errors=\"coerce\").astype(\"Int64\")\n",
    "pool = df.dropna(subset=[\"__tic\"])\n",
    "if sect_col:\n",
    "    pool[\"__sectors\"] = pool[sect_col].map(_parse_sectors)\n",
    "    pool[\"__nsec\"] = pool[\"__sectors\"].map(len)\n",
    "else:\n",
    "    pool[\"__sectors\"] = [[]]*len(pool); pool[\"__nsec\"] = 0\n",
    "pool = pool[~pool[\"__tic\"].isin(USED_TICS)].sort_values(\"__nsec\", ascending=False)\n",
    "\n",
    "show = pool[[name_col, \"__tic\", \"__nsec\", sect_col if sect_col else name_col]].head(10).copy()\n",
    "print(f\"Source table: {src}\")\n",
    "print(\"Top candidates by sector count (skipping A–C):\")\n",
    "for _, r in show.iterrows():\n",
    "    nm = str(r[name_col]); tic = int(r[\"__tic\"]); nsec = int(r[\"__nsec\"])\n",
    "    secs = str(r.get(sect_col, \"—\"))\n",
    "    print(f\" - {nm:12s}  TIC {tic:<10d}  sectors:{nsec:<2d}  list:{secs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba8b111-8221-4e1a-9e48-0b16c72bb034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probing MAST endpoints with strict caps...\n",
      "[SPOC   ] OK in 2.8s\n",
      "[QLP    ] OK in 0.0s\n",
      "=> MAST reachable (at least one endpoint).\n"
     ]
    }
   ],
   "source": [
    "# %% Quick MAST connectivity check (hard per-call caps; non-blocking)\n",
    "import time, concurrent.futures as cf\n",
    "import lightkurve as lk\n",
    "\n",
    "# Tighten astroquery service timeouts\n",
    "try:\n",
    "    from astroquery.mast import Observations\n",
    "    Observations.TIMEOUT = 15  # seconds\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from astroquery.mast import Tesscut\n",
    "    Tesscut.TIMEOUT = 15\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "TARGET = \"TIC 307210830\"  # bright, well-known TIC to probe\n",
    "\n",
    "def probe(label, fn, timeout=15):\n",
    "    t0 = time.time()\n",
    "    with cf.ThreadPoolExecutor(max_workers=1) as ex:\n",
    "        fut = ex.submit(fn)\n",
    "        try:\n",
    "            r = fut.result(timeout=timeout)\n",
    "            ok = (len(r) > 0)\n",
    "            dt = time.time() - t0\n",
    "            print(f\"[{label:7s}] {'OK' if ok else 'EMPTY'} in {dt:.1f}s\")\n",
    "            return ok\n",
    "        except cf.TimeoutError:\n",
    "            dt = time.time() - t0\n",
    "            print(f\"[{label:7s}] TIMEOUT at {dt:.1f}s\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            dt = time.time() - t0\n",
    "            print(f\"[{label:7s}] ERROR in {dt:.1f}s: {type(e).__name__}\")\n",
    "            return False\n",
    "\n",
    "print(\"Probing MAST endpoints with strict caps...\")\n",
    "ok_spoc = probe(\"SPOC\", lambda: lk.search_lightcurve(TARGET, mission=\"TESS\", author=\"SPOC\"), timeout=15)\n",
    "ok_qlp  = probe(\"QLP\",  lambda: lk.search_lightcurve(TARGET, mission=\"TESS\", author=\"QLP\"),  timeout=15)\n",
    "\n",
    "# Optional: TESSCut is heavier; skip by default. Flip RUN_TESSCUT=True if you really want to test it.\n",
    "RUN_TESSCUT = False\n",
    "ok_tcut = False\n",
    "if RUN_TESSCUT:\n",
    "    ok_tcut = probe(\"TESSCut\", lambda: lk.search_tesscut(TARGET), timeout=12)\n",
    "\n",
    "if any([ok_spoc, ok_qlp, ok_tcut]):\n",
    "    print(\"=> MAST reachable (at least one endpoint).\")\n",
    "else:\n",
    "    print(\"=> All probes failed within the caps; treat MAST as unavailable right now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8feaf830-5951-44d4-b3c5-17ba61cf10ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAST product discovery: HEALTHY\n"
     ]
    }
   ],
   "source": [
    "# %% MAST health probe (3s). If this says \"UNHEALTHY\", skip online scans and run the local-scan cell below.\n",
    "import time, lightkurve as lk\n",
    "try: lk.log.setLevel('ERROR')\n",
    "except Exception: pass\n",
    "\n",
    "def mast_products_ok(timeout=3.0):\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        # Known evergreen target: LHS 3844 (TIC 307210830)\n",
    "        r = lk.search_lightcurve(\"TIC 307210830\", mission=\"TESS\")\n",
    "        # We only need *some* product list back, not a download\n",
    "        return len(r) > 0 and (time.time()-t0) <= max(timeout, 0.1)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "ok = mast_products_ok()\n",
    "print(\"MAST product discovery:\", \"HEALTHY\" if ok else \"UNHEALTHY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e3f98dd-4e43-4a69-b56a-e8156317b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:43:59] Scanning local caches for *lc.fits …\n",
      "[06:43:59] Found 121 candidate LCFs across caches.\n",
      "[06:44:07] Picked offline Target D: TIC 150428135 (using 2 file(s), found 41 total)\n",
      "[06:44:07] Stitched points: N=30658\n",
      "\n",
      "Quick scan complete (OFFLINE).\n",
      "Saved:\n",
      " - figures/TIC150428135_quickscan_BLS_offline.png \n",
      " - results/TIC150428135_quickscan_summary_offline.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Quick scan Target D (OFFLINE-ONLY, robust to columns): harvest local LCFs -> pick TIC != {A,B,C} -> BLS\n",
    "import os, sys, glob, time, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares as BLS\n",
    "from lightkurve import LightCurve\n",
    "\n",
    "# Quiet down warnings/logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    lk.log.setLevel(\"ERROR\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "USED_TICS         = {119584412, 37749396, 311183180}   # A, B, C (skip)\n",
    "SECTORS_PER_TIC   = 2                                   # up to N files per TIC\n",
    "WINDOW_DAYS       = 0.8\n",
    "BLS_PMIN, BLS_PMAX, BLS_NPER = 0.5, 50.0, 2000\n",
    "DURATIONS_H       = np.linspace(0.75, 2.0, 9)\n",
    "OUT_FIG_DIR       = \"figures\"\n",
    "OUT_RES_DIR       = \"results\"\n",
    "\n",
    "# Likely cache roots\n",
    "CAND_ROOTS = [\n",
    "    \"./mastDownload\",\n",
    "    os.path.expanduser(\"~/Downloads/mastDownload\"),\n",
    "    os.path.expanduser(\"~/.lightkurve\"),\n",
    "    os.path.expanduser(\"~/.astropy/cache\"),\n",
    "    os.path.expanduser(\"~/.astropy/cache/astroquery\"),\n",
    "    os.path.expanduser(\"~/.cache/astroquery\"),\n",
    "    os.path.expanduser(\"~/Library/Caches/astroquery\"),  # macOS\n",
    "]\n",
    "if os.environ.get(\"ASTROPY_CACHE_DIR\"):\n",
    "    CAND_ROOTS.append(os.path.expanduser(os.environ[\"ASTROPY_CACHE_DIR\"]))\n",
    "\n",
    "os.makedirs(OUT_FIG_DIR, exist_ok=True)\n",
    "os.makedirs(OUT_RES_DIR, exist_ok=True)\n",
    "def _print(*a): print(time.strftime(\"[%H:%M:%S]\"), *a)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _best_flux_columns(ts):\n",
    "    \"\"\"Return (flux_col, err_col) names to use, preferring PDCSAP where available.\"\"\"\n",
    "    cols = set(map(str.lower, ts.colnames))\n",
    "    # Map lowercase -> actual name (preserve original case)\n",
    "    name_map = {c.lower(): c for c in ts.colnames}\n",
    "    # Preferred flux columns in order\n",
    "    pref_flux = [\"pdcsap_flux\", \"flux\", \"sap_flux\", \"kspsap_flux\"]\n",
    "    flux_col = next((name_map[c] for c in pref_flux if c in cols), None)\n",
    "    if flux_col is None:\n",
    "        return None, None\n",
    "    # Try a good matching error column\n",
    "    candidates_err = [\n",
    "        flux_col.lower().replace(\"flux\", \"flux_err\"),\n",
    "        \"pdcsap_flux_err\", \"sap_flux_err\", \"flux_err\"\n",
    "    ]\n",
    "    err_col = next((name_map[c] for c in candidates_err if c in cols), None)\n",
    "    return flux_col, err_col\n",
    "\n",
    "def _to_clean_lightcurve(ts, time_col=\"time\"):\n",
    "    \"\"\"Create a LightCurve with .time, .flux, .flux_err using the best available columns.\"\"\"\n",
    "    if time_col not in ts.colnames:\n",
    "        return None\n",
    "    flux_col, err_col = _best_flux_columns(ts)\n",
    "    if flux_col is None:\n",
    "        return None\n",
    "    # Build a fresh LightCurve\n",
    "    lc = LightCurve(time=ts[time_col])\n",
    "    lc[\"flux\"] = ts[flux_col]\n",
    "    if err_col is not None:\n",
    "        lc[\"flux_err\"] = ts[err_col]\n",
    "    return lc.remove_nans()\n",
    "\n",
    "def _extract_tic_from_meta(meta):\n",
    "    \"\"\"Try to pull TIC from header metadata.\"\"\"\n",
    "    keys = [\"TICID\", \"TIC_ID\", \"TIC\", \"OBJECT\", \"TARGETID\", \"TARGET_ID\"]\n",
    "    for k in keys:\n",
    "        if k in meta and meta[k] not in (None, \"\", \"UNKNOWN\"):\n",
    "            s = str(meta[k]).strip()\n",
    "            s = s.replace(\"TIC\", \"\").replace(\"tic\", \"\").replace(\" \", \"\")\n",
    "            v = pd.to_numeric(s, errors=\"coerce\")\n",
    "            if pd.notnull(v):\n",
    "                try:\n",
    "                    return int(v)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "def _load_lc_from_file(path):\n",
    "    \"\"\"Open an *lc.fits(.gz) and return (LightCurve, TIC) or (None, None).\"\"\"\n",
    "    try:\n",
    "        obj = lk.read(path)  # robust, replaces deprecated open()\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    # obj can be LightCurve, TessLightCurve, or a collection. Normalize to a TimeSeries-ish view.\n",
    "    try:\n",
    "        ts = obj.to_timeseries()\n",
    "    except Exception:\n",
    "        # Some LK objects already behave like TimeSeries; try attributes directly\n",
    "        try:\n",
    "            ts = obj\n",
    "            ts.colnames  # probes TimeSeries-like\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    # Create clean LC with chosen flux\n",
    "    lc = _to_clean_lightcurve(ts)\n",
    "    if lc is None or len(lc.time.value) < 50:\n",
    "        return None, None\n",
    "    # TIC from meta if present; fallback to None\n",
    "    tic = _extract_tic_from_meta(getattr(obj, \"meta\", {}))\n",
    "    return lc, tic\n",
    "\n",
    "def _gentle_flatten(lc, window_days=WINDOW_DAYS):\n",
    "    t = lc.time.value\n",
    "    if len(t) < 10:\n",
    "        return lc.normalize()\n",
    "    dt = np.nanmedian(np.diff(np.sort(t)))\n",
    "    wl = int(max(51, window_days/dt))\n",
    "    if wl % 2 == 0:\n",
    "        wl += 1\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            fl = lc.remove_outliers(sigma=6.0).flatten(window_length=wl, polyorder=2)\n",
    "        except Exception:\n",
    "            fl = lc.normalize()\n",
    "    return fl.remove_nans()\n",
    "\n",
    "def _run_bls(t, f):\n",
    "    m = np.isfinite(t) & np.isfinite(f)\n",
    "    t, f = t[m], f[m]\n",
    "    if len(t) < 300:\n",
    "        return None\n",
    "    span = t.max() - t.min()\n",
    "    pmax_eff = max(1.01, min(BLS_PMAX, span/2.0))\n",
    "    periods = np.exp(np.linspace(np.log(BLS_PMIN), np.log(pmax_eff), BLS_NPER))\n",
    "    res = BLS(t, f).power(periods, DURATIONS_H/24.0)\n",
    "    return dict(best=periods[np.argmax(res.power)], periods=periods, power=res.power)\n",
    "\n",
    "def _plot_bls(periods, power, out_png, title):\n",
    "    plt.figure(figsize=(7.2, 3.8))\n",
    "    plt.plot(periods, power, lw=1)\n",
    "    plt.xlabel(\"Period [days]\"); plt.ylabel(\"BLS power\"); plt.title(title)\n",
    "    plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "# ---------- 1) harvest local LCFs ----------\n",
    "_print(\"Scanning local caches for *lc.fits …\")\n",
    "found_files = []\n",
    "for root in CAND_ROOTS:\n",
    "    if not root or not os.path.isdir(root):\n",
    "        continue\n",
    "    found_files += glob.glob(os.path.join(root, \"**\", \"*lc.fits\"), recursive=True)\n",
    "    found_files += glob.glob(os.path.join(root, \"**\", \"*lc.fits.gz\"), recursive=True)\n",
    "\n",
    "found_files = sorted(set(found_files))\n",
    "_print(f\"Found {len(found_files)} candidate LCFs across caches.\")\n",
    "\n",
    "# ---------- 2) group by TIC (skip A–C) ----------\n",
    "tic_to_lcs = {}\n",
    "for path in found_files:\n",
    "    lc, tic = _load_lc_from_file(path)\n",
    "    if lc is None or tic is None:\n",
    "        continue\n",
    "    if tic in USED_TICS:\n",
    "        continue\n",
    "    tic_to_lcs.setdefault(tic, []).append(lc)\n",
    "\n",
    "if not tic_to_lcs:\n",
    "    raise SystemExit(\n",
    "        \"No usable local light curves (with recognizable TIC) outside A–C.\\n\"\n",
    "        \"Options right now:\\n\"\n",
    "        \" - Drop any known SPOC/QLP *lc.fits into ./mastDownload and re-run.\\n\"\n",
    "        \" - Or temporarily re-run B/C and label that as a ‘Target D quick-scan (fallback)’ in today’s log.\"\n",
    "    )\n",
    "\n",
    "# ---------- 3) pick richest TIC and use up to SECTORS_PER_TIC ----------\n",
    "cands = sorted(tic_to_lcs.items(), key=lambda kv: len(kv[1]), reverse=True)\n",
    "picked_tic, lc_list = cands[0]\n",
    "use_lc = lc_list[:SECTORS_PER_TIC]\n",
    "_print(f\"Picked offline Target D: TIC {picked_tic} (using {len(use_lc)} file(s), found {len(lc_list)} total)\")\n",
    "\n",
    "# ---------- 4) detrend, stitch, BLS ----------\n",
    "flats = []\n",
    "for lc in use_lc:\n",
    "    fl = _gentle_flatten(lc, WINDOW_DAYS)\n",
    "    try:\n",
    "        fl = fl.normalize()\n",
    "    except Exception:\n",
    "        pass\n",
    "    flats.append(fl)\n",
    "\n",
    "stitch = lk.LightCurveCollection(flats).stitch().remove_nans()\n",
    "t, f = stitch.time.value, stitch.flux.value\n",
    "_print(f\"Stitched points: N={len(t)}\")\n",
    "\n",
    "bls = _run_bls(t, f)\n",
    "if bls is None:\n",
    "    raise SystemExit(\"BLS failed (too few points). Try increasing SECTORS_PER_TIC or placing more LCFs in cache.\")\n",
    "\n",
    "# ---------- 5) save artifacts ----------\n",
    "bls_png = os.path.join(OUT_FIG_DIR, f\"TIC{picked_tic}_quickscan_BLS_offline.png\")\n",
    "_plot_bls(bls[\"periods\"], bls[\"power\"], bls_png, f\"TIC {picked_tic} (offline) — BLS (wide)\")\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"tic\": int(picked_tic),\n",
    "    \"name\": f\"Target D (offline cache) — TIC {picked_tic}\",\n",
    "    \"n_lc_files_used\": int(len(use_lc)),\n",
    "    \"N_points\": int(len(t)),\n",
    "    \"bls_best_period_days\": float(bls[\"best\"]),\n",
    "}])\n",
    "sum_csv = os.path.join(OUT_RES_DIR, f\"TIC{picked_tic}_quickscan_summary_offline.csv\")\n",
    "summary.to_csv(sum_csv, index=False)\n",
    "\n",
    "print(\"\\nQuick scan complete (OFFLINE).\")\n",
    "print(\"Saved:\\n -\", bls_png, \"\\n -\", sum_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f68de5d9-78b5-4aed-b91b-2713d0e04094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:06:33] Searching roots:\n",
      " - /Users/kobi.weitzman/Documents/tess-ephem\n",
      " - /Users/kobi.weitzman/.astropy/cache\n",
      "\n",
      "Found 34 candidate light-curve files:\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2023209231226-s0068-0000000062483237-0262-a_fast/tess2023209231226-s0068-0000000062483237-0262-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2022057073128-s0049-0000000119584412-0221-s/tess2022057073128-s0049-0000000119584412-0221-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020212050318-s0028-0000000150428135-0190-a_fast/tess2020212050318-s0028-0000000150428135-0190-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2021065132309-s0036-0000000150428135-0207-a_fast/tess2021065132309-s0036-0000000150428135-0207-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2018319095959-s0005-0000000311183180-0125-s/tess2018319095959-s0005-0000000311183180-0125-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020294194027-s0031-0000000311183180-0198-s/tess2020294194027-s0031-0000000311183180-0198-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2018263035959-s0003-0000000150428135-0123-s/tess2018263035959-s0003-0000000150428135-0123-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020049080258-s0022-0000000119584412-0174-s/tess2020049080258-s0022-0000000119584412-0174-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020294194027-s0031-0000000150428135-0198-a_fast/tess2020294194027-s0031-0000000150428135-0198-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020186164531-s0027-0000000150428135-0189-a_fast/tess2020186164531-s0027-0000000150428135-0189-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020212050318-s0028-0000000062483237-0190-a_fast/tess2020212050318-s0028-0000000062483237-0190-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2019058134432-s0009-0000000150428135-0139-s/tess2019058134432-s0009-0000000150428135-0139-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2019032160000-s0008-0000000150428135-0136-s/tess2019032160000-s0008-0000000150428135-0136-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2020266004630-s0030-0000000150428135-0195-a_fast/tess2020266004630-s0030-0000000150428135-0195-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2018349182500-s0006-0000000150428135-0126-s/tess2018349182500-s0006-0000000150428135-0126-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2018263035959-s0003-0000000037749396-0123-s/tess2018263035959-s0003-0000000037749396-0123-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2023209231226-s0068-0000000062483237-0262-s/tess2023209231226-s0068-0000000062483237-0262-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2023263165758-s0070-0000000037749396-0265-s/tess2023263165758-s0070-0000000037749396-0265-s_lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2021118034608-s0038-0000000150428135-0209-a_fast/tess2021118034608-s0038-0000000150428135-0209-a_fast-lc.fits\n",
      " • /Users/kobi.weitzman/Documents/tess-ephem/data_raw_fresh/mastDownload/TESS/tess2019085135100-s0010-0000000150428135-0140-s/tess2019085135100-s0010-0000000150428135-0140-s_lc.fits\n",
      " … (+14 more)\n"
     ]
    }
   ],
   "source": [
    "# %% Locate local MAST/Lightkurve caches and list candidate *lc.fits files\n",
    "import os, glob, sys, platform, pathlib, time\n",
    "def _print(*a): print(time.strftime(\"[%H:%M:%S]\"), *a)\n",
    "\n",
    "# Common cache roots across macOS/Linux/conda\n",
    "home = os.path.expanduser(\"~\")\n",
    "roots = [\n",
    "    os.getcwd(),\n",
    "    os.path.join(os.getcwd(), \"mastDownload\"),\n",
    "    os.path.join(home, \"mastDownload\"),\n",
    "    os.path.join(home, \"Downloads\", \"mastDownload\"),\n",
    "    os.path.join(home, \".lightkurve-cache\", \"mastDownload\"),\n",
    "    os.path.join(home, \"Library\", \"Caches\", \"lightkurve\", \"mastDownload\"),  # macOS\n",
    "    os.path.join(home, \".astropy\", \"cache\"),  # sometimes nested under here\n",
    "]\n",
    "\n",
    "# Add any env-specified cache dirs if present\n",
    "for k in (\"XDG_CACHE_HOME\", \"ASTROPY_CACHE_DIR\", \"LIGHTKURVE_CACHE_DIR\"):\n",
    "    v = os.environ.get(k)\n",
    "    if v:\n",
    "        roots.append(v)\n",
    "        roots.append(os.path.join(v, \"mastDownload\"))\n",
    "\n",
    "# De-dup and keep only existing dirs\n",
    "roots = [str(pathlib.Path(r).expanduser()) for r in roots]\n",
    "roots = [r for r in dict.fromkeys(roots) if os.path.isdir(r)]\n",
    "\n",
    "_print(\"Searching roots:\")\n",
    "for r in roots: print(\" -\", r)\n",
    "\n",
    "patterns = [\"**/*lc.fits\", \"**/*lc.fits.gz\", \"**/*-lc.fits\", \"**/*-lc.fits.gz\"]\n",
    "found = []\n",
    "for r in roots:\n",
    "    for pat in patterns:\n",
    "        found += glob.glob(os.path.join(r, pat), recursive=True)\n",
    "\n",
    "# De-dup while preserving order\n",
    "seen=set(); files=[]\n",
    "for p in found:\n",
    "    q=os.path.abspath(p)\n",
    "    if q not in seen:\n",
    "        files.append(q); seen.add(q)\n",
    "\n",
    "print(f\"\\nFound {len(files)} candidate light-curve files:\")\n",
    "for p in files[:20]:\n",
    "    print(\" •\", p)\n",
    "if len(files) > 20:\n",
    "    print(f\" … (+{len(files)-20} more)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d6a4c0f-2c51-4e35-a941-8442bdb0352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:35:57] Using cached TIC: 2020212050318  (#files=3)\n",
      "[13:35:59] Stitched points: N=151909 (from 2 file(s))\n",
      "[13:36:06] \n",
      "Quick scan complete (OFFLINE).\n",
      "[13:36:06] Saved:\n",
      "[13:36:06]  - figures/TIC2020212050318_quickscan_BLS_offline.png\n",
      "[13:36:06]  - figures/TIC2020212050318_quickscan_fold_Pbest.png\n",
      "[13:36:06]  - figures/TIC2020212050318_quickscan_fold_halfP.png\n",
      "[13:36:06]  - figures/TIC2020212050318_quickscan_fold_doubleP.png\n",
      "[13:36:06]  - results/TIC2020212050318_quickscan_offline_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# %% Quick-scan FINISH (OFFLINE, robust header-based TIC parsing)\n",
    "import os, glob, warnings, re, numpy as np, pandas as pd, matplotlib.pyplot as plt, time, pathlib\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares as BLS\n",
    "from astropy.io import fits\n",
    "\n",
    "def _print(*a): print(time.strftime(\"[%H:%M:%S]\"), *a)\n",
    "\n",
    "# === prefer this TIC if present; else auto-pick ===\n",
    "PREFERRED_TIC = None   # e.g. 150428135; leave None to auto-pick first usable\n",
    "\n",
    "WINDOW_DAYS = 0.8\n",
    "BLS_PMIN, BLS_PMAX, BLS_NPER = 0.5, 50.0, 5000\n",
    "DURATIONS_H = np.linspace(0.75, 2.5, 12)\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True); os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ---------- locate cached LCFs (reuse common roots) ----------\n",
    "home = os.path.expanduser(\"~\")\n",
    "roots = [\n",
    "    os.getcwd(),\n",
    "    os.path.join(os.getcwd(), \"mastDownload\"),\n",
    "    os.path.join(home, \"mastDownload\"),\n",
    "    os.path.join(home, \"Downloads\", \"mastDownload\"),\n",
    "    os.path.join(home, \".lightkurve-cache\", \"mastDownload\"),\n",
    "    os.path.join(home, \"Library\", \"Caches\", \"lightkurve\", \"mastDownload\"),\n",
    "    os.path.join(home, \".astropy\", \"cache\"),\n",
    "]\n",
    "for k in (\"XDG_CACHE_HOME\", \"ASTROPY_CACHE_DIR\", \"LIGHTKURVE_CACHE_DIR\"):\n",
    "    v = os.environ.get(k); \n",
    "    if v: roots += [v, os.path.join(v, \"mastDownload\")]\n",
    "roots = [str(pathlib.Path(r).expanduser()) for r in roots]\n",
    "roots = [r for r in dict.fromkeys(roots) if os.path.isdir(r)]\n",
    "\n",
    "patterns = [\"**/*lc.fits\", \"**/*lc.fits.gz\", \"**/*-lc.fits\", \"**/*-lc.fits.gz\"]\n",
    "all_files=[]\n",
    "for r in roots:\n",
    "    for pat in patterns:\n",
    "        all_files += glob.glob(os.path.join(r, pat), recursive=True)\n",
    "# de-dup\n",
    "seen=set(); files=[]\n",
    "for p in all_files:\n",
    "    q=os.path.abspath(p)\n",
    "    if q not in seen:\n",
    "        files.append(q); seen.add(q)\n",
    "\n",
    "if not files:\n",
    "    raise SystemExit(\"No local *lc.fits files found. Copy any SPOC/QLP LCFs into ./mastDownload and re-run.\")\n",
    "\n",
    "# ---------- robust TIC parsing ----------\n",
    "# 1) Try to parse the long zero-padded number in SPOC filenames, e.g. ...-0000000123456789-... -> 123456789\n",
    "re_longnum = re.compile(r\"(\\d{12,18})\")  # padded TICs often 16-17 digits\n",
    "# 2) Try direct 'TIC 123456789' in filename/dirs\n",
    "re_ticword  = re.compile(r\"TIC[\\s\\-_]?(\\d{5,12})\", re.I)\n",
    "\n",
    "def _tic_from_basename_or_dirs(path):\n",
    "    base = os.path.basename(path)\n",
    "    m = re_ticword.search(base) or re_ticword.search(os.path.dirname(path))\n",
    "    if m: \n",
    "        return int(m.group(1))\n",
    "    m2 = re_longnum.search(base)\n",
    "    if m2:\n",
    "        return int(m2.group(1).lstrip(\"0\") or \"0\")  # unpad\n",
    "    return None\n",
    "\n",
    "def _tic_from_header(path):\n",
    "    # Look in FITS headers for TICID / OBJECT / TARGNAME / TARGETID\n",
    "    try:\n",
    "        with fits.open(path, memmap=False) as hdul:\n",
    "            # check primary then first extension\n",
    "            cards = {}\n",
    "            for h in (hdul[0].header, *(h.header for h in hdul[1:2] if len(hdul)>1)):\n",
    "                for key in (\"TICID\",\"OBJECT\",\"TARGNAME\",\"TARGETID\"):\n",
    "                    if key in h:\n",
    "                        cards[key] = str(h[key])\n",
    "            # Priority: explicit TICID\n",
    "            for key in (\"TICID\",\"TARGETID\"):\n",
    "                if key in cards:\n",
    "                    try:\n",
    "                        return int(str(cards[key]).strip())\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            # OBJECT/TARGNAME may be \"TIC 123...\" — extract digits\n",
    "            for key in (\"OBJECT\",\"TARGNAME\"):\n",
    "                if key in cards:\n",
    "                    m = re_ticword.search(cards[key])\n",
    "                    if m: \n",
    "                        return int(m.group(1))\n",
    "                    # as a last resort, grab the longest digit run\n",
    "                    m2 = re.search(r\"(\\d{5,12})\", cards[key])\n",
    "                    if m2:\n",
    "                        return int(m2.group(1))\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: ask Lightkurve for meta if possible\n",
    "    try:\n",
    "        obj = lk.open(path)\n",
    "        meta = getattr(obj, \"meta\", {}) or {}\n",
    "        for key in (\"TICID\",\"TARGETID\",\"OBJECT\"):\n",
    "            if key in meta:\n",
    "                try: return int(str(meta[key]).strip())\n",
    "                except: pass\n",
    "        # lightkurve sometimes exposes targetid on the object directly\n",
    "        for key in (\"targetid\",\"target_id\"):\n",
    "            if hasattr(obj, key):\n",
    "                try: return int(getattr(obj, key))\n",
    "                except: pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "tic_to_files = {}\n",
    "for path in files:\n",
    "    tic = _tic_from_basename_or_dirs(path)\n",
    "    if tic is None:\n",
    "        tic = _tic_from_header(path)\n",
    "    if tic:\n",
    "        tic_to_files.setdefault(tic, []).append(path)\n",
    "\n",
    "if not tic_to_files:\n",
    "    # Help the user debug by showing a few filenames/headers\n",
    "    sample = files[:3]\n",
    "    msg = \"Found LCFs but still could not parse TIC IDs—even from headers.\\n\"\n",
    "    msg += \"Examples:\\n\" + \"\\n\".join(\" - \"+os.path.basename(p) for p in sample)\n",
    "    raise SystemExit(msg)\n",
    "\n",
    "# ---------- choose TIC ----------\n",
    "if PREFERRED_TIC and (PREFERRED_TIC in tic_to_files):\n",
    "    target_tic = PREFERRED_TIC\n",
    "else:\n",
    "    # pick the TIC with the most files (likely best coverage)\n",
    "    target_tic = max(tic_to_files, key=lambda k: len(tic_to_files[k]))\n",
    "\n",
    "_print(f\"Using cached TIC: {target_tic}  (#files={len(tic_to_files[target_tic])})\")\n",
    "\n",
    "# ---------- light curve loading / quick scan ----------\n",
    "def _open_lc(path):\n",
    "    try:\n",
    "        obj = lk.open(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "    for attr in (\"PDCSAP_FLUX\", \"SAP_FLUX\"):\n",
    "        lc = getattr(obj, attr, None)\n",
    "        if lc is not None:\n",
    "            try:\n",
    "                lc = lc.remove_nans()\n",
    "                if len(lc.time.value) >= 300:\n",
    "                    return lc\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        lc = obj.to_lightcurve().remove_nans()\n",
    "        if len(lc.time.value) >= 300:\n",
    "            return lc\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _gentle_flatten(lc, window_days=WINDOW_DAYS):\n",
    "    t = lc.time.value\n",
    "    if len(t) < 10: return lc.normalize()\n",
    "    dt = np.nanmedian(np.diff(np.sort(t)))\n",
    "    wl = int(max(51, window_days/dt)); \n",
    "    if wl % 2 == 0: wl += 1\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            fl = lc.remove_outliers(sigma=6.0).flatten(window_length=wl, polyorder=2)\n",
    "        except Exception:\n",
    "            fl = lc.normalize()\n",
    "    return fl.remove_nans()\n",
    "\n",
    "def _run_bls(t, f):\n",
    "    m = np.isfinite(t) & np.isfinite(f); t, f = t[m], f[m]\n",
    "    if len(t) < 300: return None\n",
    "    span = t.max() - t.min()\n",
    "    pmax_eff = max(1.01, min(BLS_PMAX, span/2.0))\n",
    "    periods = np.exp(np.linspace(np.log(BLS_PMIN), np.log(pmax_eff), BLS_NPER))\n",
    "    model = BLS(t, f); res = model.power(periods, DURATIONS_H/24.0)\n",
    "    return dict(periods=periods, power=res.power, best=periods[np.argmax(res.power)])\n",
    "\n",
    "def _plot_bls(periods, power, out_png, title):\n",
    "    plt.figure(figsize=(7.6,4.1))\n",
    "    plt.plot(periods, power, lw=1)\n",
    "    plt.xlabel(\"Period [days]\"); plt.ylabel(\"BLS power\"); plt.title(title)\n",
    "    plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "def _plot_fold(t, f, period, t0, out_png, title):\n",
    "    m = np.isfinite(t) & np.isfinite(f); t, f = t[m], f[m]\n",
    "    phase = ((t - t0 + 0.5*period) % period) / period - 0.5\n",
    "    nb = 120; bins = np.linspace(-0.5, 0.5, nb+1)\n",
    "    idx = np.digitize(phase, bins) - 1\n",
    "    y = np.array([np.nanmean(f[idx==k]) for k in range(nb)])\n",
    "    x = 0.5*(bins[:-1]+bins[1:])\n",
    "    plt.figure(figsize=(6.8,3.8))\n",
    "    plt.scatter(phase, f, s=2, alpha=0.25, rasterized=True); plt.plot(x, y, lw=2)\n",
    "    plt.xlabel(\"Phase\"); plt.ylabel(\"Normalized flux\"); plt.title(title)\n",
    "    plt.grid(alpha=0.3); plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "# Use up to 2 files for speed\n",
    "picked_files = tic_to_files[target_tic][:2]\n",
    "lcs=[]\n",
    "for path in picked_files:\n",
    "    lc = _open_lc(path)\n",
    "    if lc is not None:\n",
    "        lcs.append(lc)\n",
    "if not lcs:\n",
    "    raise SystemExit(f\"No usable light curves found for TIC {target_tic} among {len(picked_files)} files.\")\n",
    "\n",
    "# Detrend + stitch\n",
    "flats=[_gentle_flatten(lc, WINDOW_DAYS).normalize() for lc in lcs]\n",
    "stitch = lk.LightCurveCollection(flats).stitch().remove_nans()\n",
    "t, f = stitch.time.value, stitch.flux.value\n",
    "_print(f\"Stitched points: N={len(t)} (from {len(lcs)} file(s))\")\n",
    "\n",
    "# BLS\n",
    "bls = _run_bls(t, f)\n",
    "if bls is None:\n",
    "    raise SystemExit(\"BLS failed (too few points).\")\n",
    "p_best = float(bls[\"best\"])\n",
    "\n",
    "# Save artifacts\n",
    "bls_png = f\"figures/TIC{target_tic}_quickscan_BLS_offline.png\"\n",
    "_plot_bls(bls[\"periods\"], bls[\"power\"], bls_png, f\"TIC {target_tic} (offline) — BLS (wide)\")\n",
    "\n",
    "t0 = float(np.nanmin(t))\n",
    "folds = []\n",
    "for tag, factor in [(\"Pbest\",1.0), (\"halfP\",0.5), (\"doubleP\",2.0)]:\n",
    "    P = p_best * factor\n",
    "    out = f\"figures/TIC{target_tic}_quickscan_fold_{tag}.png\"\n",
    "    _plot_fold(t, f, P, t0, out, f\"TIC {target_tic} — fold @ {tag} (P≈{P:.5f} d)\")\n",
    "    folds.append(out)\n",
    "\n",
    "out_csv = f\"results/TIC{target_tic}_quickscan_offline_summary.csv\"\n",
    "pd.DataFrame([{\n",
    "    \"tic\": target_tic,\n",
    "    \"n_input_files\": len(picked_files),\n",
    "    \"n_points\": len(t),\n",
    "    \"bls_best_period_days\": p_best,\n",
    "    \"bls_png\": os.path.basename(bls_png),\n",
    "    \"fold_png_p\": os.path.basename(folds[0]),\n",
    "    \"fold_png_halfp\": os.path.basename(folds[1]),\n",
    "    \"fold_png_doublep\": os.path.basename(folds[2]),\n",
    "}]).to_csv(out_csv, index=False)\n",
    "\n",
    "_print(\"\\nQuick scan complete (OFFLINE).\")\n",
    "_print(\"Saved:\")\n",
    "_print(f\" - {bls_png}\")\n",
    "for p in folds: _print(f\" - {p}\")\n",
    "_print(f\" - {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ade0b31-6ae4-4aa0-88ab-d86b11f35bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:07] Source table: results/targets_with_TIC_join.csv\n",
      "[19:07:07] Trying 1487.01 — TIC 459978312  @ RA=255.565616, Dec=64.600740\n",
      "[19:07:07]   SPOC   sectors: none\n",
      "[19:07:07]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 5789.01 — TIC 87216634  @ RA=302.775307, Dec=16.187998\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 186.01 — TIC 279741379  @ RA=51.746762, Dec=-63.499101\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2009.01 — TIC 243187830  @ RA=16.907810, Dec=22.954980\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 197.01 — TIC 441462736  @ RA=353.033635, Dec=-21.801421\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2194.01 — TIC 271478281  @ RA=299.154276, Dec=-31.335318\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 1611.01 — TIC 264678534  @ RA=325.186593, Dec=84.333490\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 4328.01 — TIC 77175217  @ RA=68.288928, Dec=-36.946046\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2134.02 — TIC 75878355  @ RA=271.935200, Dec=39.074146\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 1793.01 — TIC 304142124  @ RA=164.859599, Dec=-56.623041\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 7032.01 — TIC 22903436  @ RA=205.846147, Dec=39.249136\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 431.01 — TIC 31374837  @ RA=83.269169, Dec=-26.724519\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2540.01 — TIC 354518617  @ RA=82.558351, Dec=-42.697327\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2443.01 — TIC 318753380  @ RA=40.178638, Dec=1.198678\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 402.02 — TIC 120896927  @ RA=36.868242, Dec=-27.635206\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2076.01 — TIC 27491137  @ RA=217.392678, Dec=39.790429\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 1405.01 — TIC 387834907  @ RA=315.669066, Dec=70.080307\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 2023.01 — TIC 16884216  @ RA=232.863124, Dec=33.902033\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 560.02 — TIC 101011575  @ RA=129.688585, Dec=-13.256692\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:09]   TESSCutsectors: none\n",
      "[19:09:09] Trying 1643.01 — TIC 298647682  @ RA=272.264606, Dec=53.217634\n",
      "[19:09:09]   SPOC   sectors: none\n",
      "[19:09:09]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 174.03 — TIC 425997655  @ RA=55.459996, Dec=-62.767078\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 7062.01 — TIC 23961340  @ RA=313.291903, Dec=-9.043930\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 461.01 — TIC 4646810  @ RA=38.272011, Dec=-10.351781\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 1947.01 — TIC 460140752  @ RA=156.738445, Dec=-60.665262\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 1824.01 — TIC 142387023  @ RA=197.731187, Dec=61.744779\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 6726.01 — TIC 121490076  @ RA=343.572368, Dec=-43.010348\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 866.01 — TIC 381976956  @ RA=77.546815, Dec=-57.586669\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:10]   TESSCutsectors: none\n",
      "[19:09:10] Trying 6249.02 — TIC 456260074  @ RA=101.492328, Dec=66.890733\n",
      "[19:09:10]   SPOC   sectors: none\n",
      "[19:09:10]   QLP    sectors: none\n",
      "[19:09:20]   TESSCutsectors: none\n",
      "[19:09:20] Trying 1255.01 — TIC 237222864  @ RA=296.244328, Dec=74.062737\n",
      "[19:09:20]   SPOC   sectors: none\n",
      "[19:09:20]   QLP    sectors: none\n",
      "[19:11:34]   TESSCutsectors: none\n",
      "[19:11:34] Trying 880.03 — TIC 34077285  @ RA=94.164469, Dec=-13.987437\n",
      "[19:11:34]   SPOC   sectors: none\n",
      "[19:11:34]   QLP    sectors: none\n",
      "[19:11:34]   TESSCutsectors: none\n",
      "[19:11:34] Trying 5396.01 — TIC 202428245  @ RA=227.190884, Dec=62.959256\n",
      "[19:11:34]   SPOC   sectors: none\n",
      "[19:11:34]   QLP    sectors: none\n",
      "[19:11:34]   TESSCutsectors: none\n",
      "[19:11:34] Trying 815.01 — TIC 102840239  @ RA=155.871884, Dec=-43.834955\n",
      "[19:11:34]   SPOC   sectors: none\n",
      "[19:11:34]   QLP    sectors: none\n",
      "[19:11:34]   TESSCutsectors: none\n",
      "[19:11:34] Trying 139.01 — TIC 62483237  @ RA=336.402276, Dec=-34.909583\n",
      "[19:11:34]   SPOC   sectors: none\n",
      "[19:11:34]   QLP    sectors: none\n",
      "[19:11:34]   TESSCutsectors: none\n",
      "[19:11:34] Trying 2459.01 — TIC 192790476  @ RA=82.142839, Dec=-39.373118\n",
      "[19:11:34]   SPOC   sectors: none\n",
      "[19:11:34]   QLP    sectors: none\n",
      "[19:11:34]   TESSCutsectors: none\n",
      "[19:11:34] Trying 6075.01 — TIC 424388628  @ RA=281.039970, Dec=75.990935\n",
      "[19:11:34]   SPOC   sectors: none\n",
      "[19:11:34]   QLP    sectors: none\n",
      "[19:13:31]   TESSCutsectors: none\n",
      "[19:13:31] Trying 1563.01 — TIC 249945230  @ RA=38.809453, Dec=48.274186\n",
      "[19:13:31]   SPOC   sectors: none\n",
      "[19:13:31]   QLP    sectors: none\n",
      "[19:13:31]   TESSCutsectors: none\n",
      "[19:13:31] Trying 454.01 — TIC 153077621  @ RA=49.660234, Dec=-42.642356\n",
      "[19:13:31]   SPOC   sectors: none\n",
      "[19:13:31]   QLP    sectors: none\n",
      "[19:13:36]   TESSCutsectors: none\n",
      "[19:13:36] Trying 4310.01 — TIC 303317324  @ RA=351.903692, Dec=-25.508085\n",
      "[19:13:36]   SPOC   sectors: none\n",
      "[19:13:36]   QLP    sectors: none\n",
      "[19:13:44]   TESSCutsectors: none\n",
      "[19:13:44] Trying 1030.01 — TIC 464296022  @ RA=155.831688, Dec=-57.590345\n",
      "[19:13:44]   SPOC   sectors: none\n",
      "[19:13:44]   QLP    sectors: none\n",
      "[19:13:44]   TESSCutsectors: none\n",
      "[19:13:44] Trying 3485.01 — TIC 394672497  @ RA=201.749652, Dec=-73.594282\n",
      "[19:13:44]   SPOC   sectors: none\n",
      "[19:13:44]   QLP    sectors: none\n",
      "[19:13:44]   TESSCutsectors: none\n",
      "[19:13:44] Trying 913.01 — TIC 407126408  @ RA=234.573404, Dec=-80.803786\n",
      "[19:13:44]   SPOC   sectors: none\n",
      "[19:13:44]   QLP    sectors: none\n",
      "[19:13:55]   TESSCutsectors: none\n",
      "[19:13:55] Trying 2482.01 — TIC 117938087  @ RA=71.518433, Dec=-15.343361\n",
      "[19:13:55]   SPOC   sectors: none\n",
      "[19:13:55]   QLP    sectors: none\n",
      "[19:13:55]   TESSCutsectors: none\n",
      "[19:13:55] Trying 458.01 — TIC 64071894  @ RA=39.135784, Dec=-22.245114\n",
      "[19:13:55]   SPOC   sectors: none\n",
      "[19:13:55]   QLP    sectors: none\n",
      "[19:14:00]   TESSCutsectors: none\n",
      "[19:14:00] Trying 4590.01 — TIC 154923406  @ RA=203.200771, Dec=84.514872\n",
      "[19:14:00]   SPOC   sectors: none\n",
      "[19:14:00]   QLP    sectors: none\n",
      "[19:14:50]   TESSCutsectors: none\n",
      "[19:14:50] Trying 776.01 — TIC 306996324  @ RA=178.576633, Dec=-37.552732\n",
      "[19:14:50]   SPOC   sectors: none\n",
      "[19:14:50]   QLP    sectors: none\n",
      "[19:14:57]   TESSCutsectors: none\n",
      "[19:14:57] Trying 871.01 — TIC 219344917  @ RA=74.738165, Dec=-50.626891\n",
      "[19:14:57]   SPOC   sectors: none\n",
      "[19:14:57]   QLP    sectors: none\n",
      "[19:14:57]   TESSCutsectors: none\n",
      "[19:14:57] Trying 1823.01 — TIC 142381532  @ RA=196.220356, Dec=63.753791\n",
      "[19:14:57]   SPOC   sectors: none\n",
      "[19:14:57]   QLP    sectors: none\n",
      "[19:15:13]   TESSCutsectors: none\n",
      "[19:15:13] Trying 213.01 — TIC 234345288  @ RA=38.886597, Dec=-71.624162\n",
      "[19:15:13]   SPOC   sectors: none\n",
      "[19:15:13]   QLP    sectors: none\n",
      "[19:15:50]   TESSCutsectors: none\n",
      "[19:15:50] Trying 5554.01 — TIC 91287873  @ RA=104.744003, Dec=28.716266\n",
      "[19:15:50]   SPOC   sectors: none\n",
      "[19:15:50]   QLP    sectors: none\n",
      "[19:15:50]   TESSCutsectors: none\n",
      "[19:15:50] Trying 687.01 — TIC 74534430  @ RA=137.671344, Dec=-45.097516\n",
      "[19:15:50]   SPOC   sectors: none\n",
      "[19:15:50]   QLP    sectors: none\n",
      "[19:16:03]   TESSCutsectors: none\n",
      "[19:16:03] Trying 4424.01 — TIC 326386495  @ RA=268.244776, Dec=-48.562340\n",
      "[19:16:03]   SPOC   sectors: none\n",
      "[19:16:03]   QLP    sectors: none\n",
      "[19:16:17]   TESSCutsectors: none\n",
      "[19:16:17] Trying 2105.01 — TIC 156199502  @ RA=181.073879, Dec=83.437653\n",
      "[19:16:17]   SPOC   sectors: none\n",
      "[19:16:17]   QLP    sectors: none\n",
      "[19:16:17]   TESSCutsectors: none\n",
      "[19:16:17] Trying 1958.01 — TIC 356747847  @ RA=175.336020, Dec=-78.163020\n",
      "[19:16:17]   SPOC   sectors: none\n",
      "[19:16:17]   QLP    sectors: none\n",
      "[19:16:17]   TESSCutsectors: none\n",
      "[19:16:17] Trying 712.02 — TIC 150151262  @ RA=92.936137, Dec=-65.825972\n",
      "[19:16:17]   SPOC   sectors: none\n",
      "[19:16:17]   QLP    sectors: none\n",
      "[19:18:16]   TESSCutsectors: none\n",
      "[19:18:16] Trying 198.01 — TIC 12421862  @ RA=2.268160, Dec=-27.122133\n",
      "[19:18:16]   SPOC   sectors: none\n",
      "[19:18:16]   QLP    sectors: none\n",
      "[19:18:27]   TESSCutsectors: none\n",
      "[19:18:27] Trying 1759.01 — TIC 408636441  @ RA=326.853293, Dec=62.753861\n",
      "[19:18:27]   SPOC   sectors: none\n",
      "[19:18:27]   QLP    sectors: none\n",
      "[19:18:28]   TESSCutsectors: none\n",
      "[19:18:28] Trying 5076.01 — TIC 303432813  @ RA=50.509656, Dec=17.239967\n",
      "[19:18:28]   SPOC   sectors: none\n",
      "[19:18:28]   QLP    sectors: none\n",
      "[19:18:57]   TESSCutsectors: none\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Quick scan aborted: no usable data via 60″ cone for top rows.\nOptions: increase MAX_ROWS, widen CONE_RADIUS (e.g., 120 arcsec), or switch to a known-bright seed to verify connectivity.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Quick scan aborted: no usable data via 60″ cone for top rows.\nOptions: increase MAX_ROWS, widen CONE_RADIUS (e.g., 120 arcsec), or switch to a known-bright seed to verify connectivity.\n"
     ]
    }
   ],
   "source": [
    "# %% Target D quick-scan (coords-first cone search, non-blocking downloads)\n",
    "import os, time, warnings, concurrent.futures as cf\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import lightkurve as lk\n",
    "from astropy.timeseries import BoxLeastSquares as BLS\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "# Noise + network hygiene\n",
    "try: lk.log.setLevel(\"ERROR\")\n",
    "except: pass\n",
    "try:\n",
    "    from astroquery.mast import Observations\n",
    "    Observations.TIMEOUT = 8   # keep MAST calls snappy\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ======= CONFIG =======\n",
    "TABLE_PATH       = \"results/targets_with_TIC_join.csv\"  # needs RA/Dec columns\n",
    "USED_TICS        = {119584412, 37749396, 311183180}     # skip A–C\n",
    "MAX_ROWS         = 120\n",
    "CONE_RADIUS      = \"60 arcsec\"                          # wide to catch TIC renumbers\n",
    "AUTHORS          = (\"SPOC\", \"QLP\", \"TESSCut\")\n",
    "SECTS_PER_AUTH   = {\"SPOC\": 1, \"QLP\": 1, \"TESSCut\": 1}  # 1 sector only (fast probe)\n",
    "DL_TIMEOUT       = 25                                    # hard cap for single download\n",
    "DOWNLOAD_DIR     = \"net_probe\"                           # tiny cache\n",
    "# BLS knobs\n",
    "WINDOW_DAYS      = 0.8\n",
    "BLS_PMIN, BLS_PMAX, BLS_NPER = 0.5, 50.0, 2000\n",
    "DURATIONS_H      = np.linspace(0.75, 2.0, 9)\n",
    "# ======================\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def _print(*a): print(time.strftime(\"[%H:%M:%S]\"), *a)\n",
    "\n",
    "def _parse_sector_list(val):\n",
    "    if isinstance(val, (list, tuple)): return [int(x) for x in val]\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip().strip(\"[]\")\n",
    "        if not s: return []\n",
    "        out=[]\n",
    "        for tok in s.split(\",\"):\n",
    "            tok=tok.strip()\n",
    "            if tok:\n",
    "                try: out.append(int(tok))\n",
    "                except: pass\n",
    "        return out\n",
    "    return []\n",
    "\n",
    "def _row_to_coord(r):\n",
    "    for ra_key in [\"ra\",\"ra_deg\",\"tic_ra\",\"RA\",\"RA_deg\"]:\n",
    "        for de_key in [\"dec\",\"dec_deg\",\"tic_dec\",\"Dec\",\"DEC\"]:\n",
    "            if ra_key in r.index and de_key in r.index:\n",
    "                try:\n",
    "                    ra = float(r[ra_key]); de = float(r[de_key])\n",
    "                    if np.isfinite(ra) and np.isfinite(de):\n",
    "                        return SkyCoord(ra*u.deg, de*u.deg)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "def _discover_by_author(target, author, cap=4, radius=CONE_RADIUS):\n",
    "    try:\n",
    "        if author in (\"SPOC\",\"QLP\"):\n",
    "            sr = lk.search_lightcurve(target, mission=\"TESS\", author=author, radius=radius)\n",
    "        else:\n",
    "            sr = lk.search_tesscut(target)\n",
    "    except Exception:\n",
    "        return []\n",
    "    secs = sorted({getattr(r, \"sector\", None) for r in sr if getattr(r, \"sector\", None) is not None})\n",
    "    return secs[:cap]\n",
    "\n",
    "def _probe_download(search_result, author):\n",
    "    \"\"\"Download exactly 1 file with a hard timeout thread; return LightCurve-like object or None.\"\"\"\n",
    "    if len(search_result) == 0:\n",
    "        return None\n",
    "    def _do():\n",
    "        if author in (\"SPOC\",\"QLP\"):\n",
    "            obj = search_result[0].download(download_dir=DOWNLOAD_DIR)\n",
    "            return obj\n",
    "        else:\n",
    "            tpf = search_result[0].download(cutout_size=7, download_dir=DOWNLOAD_DIR)\n",
    "            # minimal LC from TPF\n",
    "            try:\n",
    "                ap = tpf.create_threshold_mask(threshold=3)\n",
    "                if ap.sum()==0: ap=None\n",
    "            except Exception:\n",
    "                ap=None\n",
    "            lc = tpf.to_lightcurve(aperture_mask=ap) if ap is not None else tpf.to_lightcurve()\n",
    "            return lc\n",
    "    with cf.ThreadPoolExecutor(max_workers=1) as ex:\n",
    "        fut = ex.submit(_do)\n",
    "        try:\n",
    "            return fut.result(timeout=DL_TIMEOUT)\n",
    "        except cf.TimeoutError:\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def _gentle_flatten(lc, window_days=WINDOW_DAYS):\n",
    "    try:\n",
    "        t = lc.time.value\n",
    "    except Exception:\n",
    "        return None\n",
    "    if len(t) < 10: return lc.normalize()\n",
    "    dt = np.nanmedian(np.diff(np.sort(t)))\n",
    "    wl = int(max(51, window_days/dt));  wl += (wl % 2 == 0)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        try:\n",
    "            fl = lc.remove_outliers(sigma=6.0).flatten(window_length=wl, polyorder=2)\n",
    "        except Exception:\n",
    "            fl = lc.normalize()\n",
    "    return fl.remove_nans()\n",
    "\n",
    "def _run_bls(t, f):\n",
    "    m = np.isfinite(t) & np.isfinite(f); t, f = t[m], f[m]\n",
    "    if len(t) < 300: return None\n",
    "    span = t.max() - t.min()\n",
    "    pmax_eff = max(1.01, min(BLS_PMAX, span/2.0))\n",
    "    periods = np.exp(np.linspace(np.log(BLS_PMIN), np.log(pmax_eff), BLS_NPER))\n",
    "    model = BLS(t, f); res = model.power(periods, DURATIONS_H/24.0)\n",
    "    return dict(periods=periods, power=res.power, best=periods[np.argmax(res.power)])\n",
    "\n",
    "def _plot_bls(periods, power, out_png, title):\n",
    "    plt.figure(figsize=(7.2,3.8)); plt.plot(periods, power, lw=1)\n",
    "    plt.xlabel(\"Period [days]\"); plt.ylabel(\"BLS power\"); plt.title(title); plt.grid(alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "\n",
    "# --- load table and rank rows: has coords first, then more hinted sectors (if any)\n",
    "if not os.path.exists(TABLE_PATH):\n",
    "    raise SystemExit(f\"Missing table: {TABLE_PATH} (needs RA/Dec columns).\")\n",
    "\n",
    "df = pd.read_csv(TABLE_PATH)\n",
    "if not len(df):\n",
    "    raise SystemExit(f\"Table is empty: {TABLE_PATH}\")\n",
    "\n",
    "cols = {c.lower(): c for c in df.columns}\n",
    "tic_col  = cols.get(\"tic\") or cols.get(\"tic_id\") or cols.get(\"ticid\") or cols.get(\"tic_id_norm\")\n",
    "name_col = cols.get(\"name\") or cols.get(\"toi\") or cols.get(\"target\") or cols.get(\"designation\")\n",
    "sect_col = cols.get(\"sectors_now\") or cols.get(\"sectors\") or cols.get(\"observed_sectors\") or cols.get(\"sector_list\")\n",
    "\n",
    "df[\"__tic\"] = pd.to_numeric(df[tic_col], errors=\"coerce\").astype(\"Int64\") if tic_col else pd.Series([pd.NA]*len(df))\n",
    "pool = df.dropna(subset=[\"__tic\"]) if tic_col else df.copy()\n",
    "pool = pool[~pool[\"__tic\"].isin(USED_TICS)] if tic_col else pool\n",
    "pool = pool.reset_index(drop=True)\n",
    "\n",
    "has_coord = []\n",
    "for _, r in pool.iterrows():\n",
    "    has_coord.append(_row_to_coord(r) is not None)\n",
    "pool[\"__has_coord\"] = has_coord\n",
    "if sect_col:\n",
    "    pool[\"__nsec_hint\"] = pool[sect_col].map(_parse_sector_list).map(len).fillna(0)\n",
    "else:\n",
    "    pool[\"__nsec_hint\"] = 0\n",
    "\n",
    "pool = pool.sort_values(by=[\"__has_coord\",\"__nsec_hint\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "# --- scan until one succeeds ---\n",
    "_print(f\"Source table: {TABLE_PATH}\")\n",
    "picked = None\n",
    "\n",
    "for i, r in pool.head(MAX_ROWS).iterrows():\n",
    "    coord = _row_to_coord(r)\n",
    "    if coord is None:\n",
    "        continue  # coords-only strategy to avoid name/TIC resolver pitfalls\n",
    "    name  = str(r[name_col]) if name_col in r.index else \"Target D\"\n",
    "    tic   = int(r[\"__tic\"]) if r[\"__tic\"] is not pd.NA else None\n",
    "\n",
    "    _print(f\"Trying {name} — TIC {tic if tic else '—'}  @ RA={coord.ra.deg:.6f}, Dec={coord.dec.deg:.6f}\")\n",
    "    lcs, used_auth, used_sect = None, None, None\n",
    "\n",
    "    for auth in AUTHORS:\n",
    "        secs = _discover_by_author(coord, author=auth, cap=4, radius=CONE_RADIUS)\n",
    "        _print(f\"  {auth:<7s}sectors: {secs if secs else 'none'}\")\n",
    "        if not secs:\n",
    "            continue\n",
    "        # fetch exactly one sector quickly\n",
    "        if auth in (\"SPOC\",\"QLP\"):\n",
    "            sr = lk.search_lightcurve(coord, mission=\"TESS\", author=auth, sector=secs[0], radius=CONE_RADIUS)\n",
    "        else:\n",
    "            sr = lk.search_tesscut(coord, sector=secs[0])\n",
    "        obj = _probe_download(sr, auth)\n",
    "        if obj is None:\n",
    "            continue\n",
    "        # Convert to LightCurve if needed\n",
    "        try:\n",
    "            lc = obj.remove_nans()\n",
    "        except Exception:\n",
    "            try:\n",
    "                lc = obj.to_lightcurve().remove_nans()\n",
    "            except Exception:\n",
    "                continue\n",
    "        if len(getattr(lc, \"time\", [])) < 300:\n",
    "            continue\n",
    "        lcs, used_auth, used_sect = lc, auth, secs[0]\n",
    "        break\n",
    "\n",
    "    if lcs is not None:\n",
    "        picked = dict(name=name, tic=tic, coord=coord, author=used_auth, sector=used_sect, lc=lcs)\n",
    "        break\n",
    "\n",
    "if picked is None:\n",
    "    raise SystemExit(\"Quick scan aborted: no usable data via 60″ cone for top rows.\\n\"\n",
    "                     \"Options: increase MAX_ROWS, widen CONE_RADIUS (e.g., 120 arcsec), \"\n",
    "                     \"or switch to a known-bright seed to verify connectivity.\")\n",
    "\n",
    "# --- detrend + BLS on the single sector we landed ---\n",
    "fl = _gentle_flatten(picked[\"lc\"], WINDOW_DAYS)\n",
    "if fl is None or len(fl.time.value) < 300:\n",
    "    raise SystemExit(\"Landed a file but failed to flatten/use it (too few points).\")\n",
    "\n",
    "t, f = fl.time.value, fl.flux.value\n",
    "_print(f\"Stitched points: N={len(t)} (author={picked['author']}, sector={picked['sector']})\")\n",
    "_print(\"Running BLS (wide)…\")\n",
    "bls = _run_bls(t, f)\n",
    "if bls is None:\n",
    "    raise SystemExit(\"BLS failed (not enough points after cleaning).\")\n",
    "\n",
    "p_best = float(bls[\"best\"])\n",
    "png = f\"figures/TargetD_BLS_online.png\"\n",
    "ttl = f\"{picked['name']} — {picked['author']} S{picked['sector']} — BLS (wide)\"\n",
    "_plot_bls(bls[\"periods\"], bls[\"power\"], png, ttl)\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    \"name\": picked[\"name\"],\n",
    "    \"tic\": picked[\"tic\"],\n",
    "    \"author\": picked[\"author\"],\n",
    "    \"sector\": picked[\"sector\"],\n",
    "    \"ra_deg\": picked[\"coord\"].ra.deg,\n",
    "    \"dec_deg\": picked[\"coord\"].dec.deg,\n",
    "    \"N_points\": len(t),\n",
    "    \"bls_best_period_days\": p_best,\n",
    "}])\n",
    "csv = \"results/TargetD_quickscan_online_summary.csv\"\n",
    "summary.to_csv(csv, index=False)\n",
    "\n",
    "_print(\"\\nQuick scan complete (ONLINE).\")\n",
    "_print(\"Saved:\\n -\", png, \"\\n -\", csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50a09a-0ce0-4e7b-8bdd-8bf71d66690f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tess-ephem)",
   "language": "python",
   "name": "tess-ephem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
